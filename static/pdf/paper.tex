% Define the switch here using "newif" and start its name with "if"
% Here, NAME_OF_SWITCH == "Arxiv"
\newif\ifJournal
% By default, a switch is "false". Use \NAME_OF_SWITCHtrue to set to true
% Uncomment the line below to set the switch to "true"
% \Journaltrue

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifJournal
% Journal version

\else
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arxiv version
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Minimal Template                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage{graphicx}
\RequirePackage{natbib}

\usepackage{authblk}
\usepackage[dvipsnames]{xcolor}
\usepackage[plainpages=false,pdfpagelabels,
colorlinks=true,linkcolor=RedOrange,
urlcolor=RedOrange,citecolor=MidnightBlue]{hyperref}
% Keywords command
\providecommand{\keywords}[1]{\small \textbf{\textit{Keywords---}} #1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Fonts, Margin, Macros 					%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage{kpfonts}
\usepackage{fullpage}
\usepackage{tikz-cd}

% Input my macros
\usepackage{my-macros}
\usepackage{marginnote} %for math mode margin notes.
\def\tl#1{{\color{BrickRed} \marginnote{\textbf{TL}}#1}}
\def\yh#1{{\color{RubineRed} \marginnote{\textbf{YH}}#1}}
\def\wg#1{{\color{Orange} \marginnote{\textbf{WG}}#1}}

% package for figures
\usepackage{subfig}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Front Matters                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Reversible Gromov-Monge Sampler for Simulation-Based Inference}

\author{YoonHaeng Hur}
\author{Wenxuan Guo}
\author{Tengyuan Liang\thanks{\tt 
% Email:\href{mailto:tengyuan.liang@chicagobooth.edu}{tengyuan.liang@chicagobooth.edu}.
Liang acknowledges the generous support from the NSF Career award (DMS-2042473), and the William S. Fishman Faculty Research Fund at the University of Chicago Booth School of Business. Liang wishes to thank Maxim Raginsky and Chris Hansen for discussions on simulation-based inference.}}
\affil{University of Chicago}


\maketitle

\begin{abstract}
	This paper introduces a new simulation-based inference procedure to model and sample from multi-dimensional probability distributions given access to i.i.d.\ samples, circumventing usual approaches of explicitly modeling the density function or designing Markov chain Monte Carlo. Motivated by the seminal work of \cite{memoli_2011} and \cite{sturm_2012} on distance and isomorphism between metric measure spaces, we propose a new notion called the Reversible Gromov-Monge (RGM) distance and study how RGM can be used to design new transform samplers in order to perform simulation-based inference. Our RGM sampler can also estimate optimal alignments between two heterogenous metric measure spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ from empirical data sets, with estimated maps that approximately push forward one measure $\mu$ to the other $\nu$, and vice versa. Analytic properties of RGM distance are derived; statistical rate of convergence, representation, and optimization questions regarding the induced sampler are studied. Synthetic and real-world examples showcasing the effectiveness of the RGM sampler are also demonstrated.
\end{abstract}

\keywords{Gromov-Wasserstein metric, transform sampling, simulation-based inference, generative models, isomorphism, likelihood-free inference}

% \tableofcontents
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Main Paper                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
% \paragraph{1}
One of the central tasks in statistics is to model and sample from a multi-dimensional probability distribution. Classic statistics approaches this problem by fitting a model to the target distribution and then sampling from a fitted model via Markov Chain Monte Carlo (MCMC) techniques. Although such model-based methods are widely used, MCMC sampling often entails several technicalities. Beyond diagnosing whether the chain mixes, obtaining i.i.d.\ samples from MCMC methods is difficult as one has to control correlations between successive samples, or to run parallel chains.

An alternative approach available in statistics, reserved for the one-dimensional case, is usually referred to as the (inverse) \textit{transform sampling}. Such an approach circumvents the calling for a parametric or nonparametric density and directly designs a sampler by transforming a simple uniform distribution. The idea is simple: one can transform a uniform measure $\mu = {\rm Unif}([0,1])$ to any one-dimensional target probability measure $\nu$ leveraging the following monotonic transform $T: [0, 1] \rightarrow \R$ called the inverse Cumulative Distribution Function (CDF),
\begin{align}
	\label{eqn:inv-cdf}
	T(x)= \inf \{ y \in \R ~:~ \nu((-\infty, y]) > x  \} \;.
\end{align}
Define the pushforward measure $T_\#\mu$ by $T_\#\mu(S) = \mu(\{ x : T(x) \in S \})$ for any Borel set $S\subseteq \R$, and one can easily check that $T_{\#} \mu = \nu$; namely, with a draw from the one-dimensional uniform distribution $x\sim \mu$, the transformed sample $T(x)$ has the target probability distribution $\nu$.

Recently, the \textit{transform sampling} idea has been extended to the multi-dimensional setting, as seen in both machine learning (generative modeling) and computational optimal transport. Again, given a target probability measure $\nu$ supported on $\cY$ and a user-specified probability measure $\mu$---that is easy to sample from such as a multivariate Gaussian---defined on $\cX$, we aim to find a measurable map $T \colon \cX \to \cY$ such that $T_{\#} \mu = \nu$, where $T_{\#} \mu$, the pushforward measure, is defined analogously to the one-dimensional case above. Such a map $T$, which is called a transport map from $\mu$ to $\nu$, transforms i.i.d.\ samples from $\mu$ into i.i.d.\ samples from $\nu$. Therefore,  with a good estimate of the transformation $T$, the transform sampler operates and scales more efficiently than classic MCMC approaches. 

Such transform sampling ideas have been leveraged in generative modeling by designing different criteria to learn a qualified transformation $T$; furthermore, remarkable empirical benchmarks have been documented. The essence of these methods can be summarized as follows. A transport map is obtained by minimizing $T \mapsto \cL(T_{\#} \widehat{\mu}, \widehat{\nu})$ over $\cF$, where $\cF$ is a map class that is rich enough to contain a transport map, $\widehat{\mu}$ and $\widehat{\nu}$ are empirical measures based on samples from $\mu$ and $\nu$, respectively, and $\cL$ measures certain discrepancy of two distributions. In summary, by properly designing a class of maps $\cF$ and collecting sufficiently many samples, we expect a minimizer $T$ that will satisfy $T_{\#} \mu \approx \nu$. In Generative Adversarial Networks (GAN) \citep{goodfellow_2014}, $\cF$ consists of neural networks and $\cL$ is Jensen-Shannon divergence. Moreover, different choices of $\cL$ have led to several variants: $f$-divergences for $f$-GAN \citep{nowozin_2016}, Wasserstein distances for Wasserstein-GAN \citep{arjovsky_2017}, and Maximum Mean Discrepancies (MMD) for MMD-GAN \citep{dziugaite_roy_ghahramani_2015, li_swersky_zemel_2015}.

The optimal transport theory aims to identify the optimal transformation $T$, quantified by the transportation cost of moving mass from $\mu$ to $\nu$. When $\mu$ and $\nu$ both lie in the same space, say $\R^d$, Brenier \citep{brenier1991} proved, under mild regularity conditions, the following remarkable result that backs up the \textit{transform sampling} in the multi-dimensional setting. Consider the Wasserstein-$p$ distance $W_p(\mu, \nu)$ defined as
\begin{align*}
	W_p(\mu, \nu) := \inf_{\gamma \in \Pi(\mu, \nu)} \left(  \int_{\R^d \times \R^d} \| x - y \|_2^p \dd{\gamma}(x, y) \right)^{1/p} \;,
\end{align*}
where $\Pi(\mu, \nu)$ denotes all couplings of $(\mu, \nu)$.
Brenier established that for $p = 2$, there exists a unique optimal transport map $T^{\star}$ given as the gradient of some convex function. Also, there exists a unique optimal coupling $\gamma^{\star}$ and $\gamma^{\star} = (\mathrm{Id}, T^{\star})_{\#} \mu$ holds. As a result,
\begin{align*}
 	W_2^2(\mu, \nu) = \int_{\R^d \times \R^d} \|x - y\|_2^2 \dd{\gamma^{\star}}(x, y) = \int_{\R^d} \|x - T^\star(x)\|_2^2 \dd{\mu}(x) \;.
\end{align*}
Now let's contrast this result with the one-dimensional (inverse) transform sampling: when $\mu = {\rm Unif}([0,1])$, it turns out the inverse CDF map $T: [0, 1] \rightarrow \R$ in \eqref{eqn:inv-cdf} minimizes the transportation cost $W_p(\mu, \nu), p\geq 1$. Brenier's result significantly enriches the one-dimensional insight to the multi-dimensional case: now the multi-dimensional map $T:\R^d \rightarrow \R^d$ is the gradient of a convex function, as opposed to a monotonic map $\R \rightarrow \R$.

Elegant as it is, one limitation of the above optimal transport theory is that both $\mu$ and $\nu$ are supported on a common Euclidean space. To see why such a limitation is a non-trifling issue in practice, consider a simple task where one aims to transform a multivariate Gaussian sample (say $\cX = \R^{10}$) to a sample of the MNIST image ($\cY \subset \R^{784}$). The probability distribution of MNIST images are supported on some image manifold $\cY$, which clearly differs from the usual Euclidean space $\cX$. It turns out, when $\cX$ and $\cY$ are two heterogenous spaces, Gromov-Wasserstein (GW) distance was proposed to attack the aforementioned limitation. Let $c_{\cX} \colon \cX \times \cX \rightarrow \R$ and $c_{\cY} \colon \cY \times \cY \rightarrow \R$ be two continuous cost functions on $\cX$ and $\cY$, respectively, \cite{memoli_2011} defined a notion of GW distance as
\begin{align}
	\mathrm{GW}(\mu, \nu) := \inf_{\gamma \in \Pi(\mu, \nu)} \left( \int_{\cX \times \cY} \int_{\cX \times \cY} \big( c_{\cX}(x, x') -  c_{\cY}(y, y') \big)^2 \dd{\gamma} (x, y) \dd{\gamma}(x', y') \right)^{1/2} \;.
\end{align}
A few remarks regarding the comparison between Wasserstein and Gromov-Wasserstein are warranted here: First, unlike Wasserstein which solves for an infinite-dimensional linear program in the coupling $\gamma$, GW formulates a Quadratic Assignment Program (QAP) in $\gamma$, which is known to be computationally hard; Second, GW aims to match the cost functions defined on two heterogenous spaces over a pair of couplings ($\gamma(x, y)$ and $\gamma(x', y')$), and thus holds promise to identify isomorphism between spaces. Despite being an elegant notion of distance between metric measure spaces (see \cite[Definition~5.1]{memoli_2011}), GW is hard to compute in practice due to its QAP nature; it is also unclear how to estimate ${\rm GW}(\mu, \nu)$ based on finite i.i.d.\ samples from $\mu$ and $\nu$, and how accurate such estimates are.

\paragraph{Main contributions}
This paper considers computational and statistical questions regarding Gromov-Wasserstein outlined above, and aims to design a new transform sampler as an approach to model and sample from multi-dimensional probability distributions given access to i.i.d.\ samples, circumventing the usual ways of modeling the density function or MCMC. Our transform sampler can also estimate good alignments between two heterogenous metric measure spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ from empirical data sets, with estimated maps that approximately pushforward one measure $\mu$ to the other $\nu$, and vice versa. Towards reaching these goals, we made the following specific contributions.
\begin{itemize}
	\item We introduce a new notion, Reversible Gromov-Monge (RGM) distance, on metric measure spaces that majorizes the usual Gromov-Wasserstein distance. Furthermore, we show several analytic properties possessed by GW carry naturally over to our RGM.
	
	\item Our RGM formulation naturally induces a transform sampler, as a relaxation of the usual GW formulation. Rather than solving a QAP which is quadratic in the coupling $\gamma \in \Pi(\mu, \nu)$, we decouple the pair as $(\mathrm{Id}, F)_{\#} \mu$ and $(B, \mathrm{Id})_{\#} \nu$ with $F:\cX \rightarrow \cY$ and $\cB:\cY \rightarrow \cX$, respectively, and then later bind them via the constraints $(\mathrm{Id}, F)_{\#} \mu \approx (B, \mathrm{Id})_{\#} \nu$. Such a decoupling and binding idea will prove suitable for the statistical estimation problem based on finite i.i.d.\ samples. We will also show, from an operator viewpoint, such a decoupling and binding idea ensures that our RGM is an infinite-dimensional convex program in $F, B$ that admits a simple representation theorem, as opposed to the otherwise intractable infinite-dimensional QAP in GW.
	
	\item We derive non-asymptotic rates of convergence for the proposed RGM sampler using tools from empirical processes, for generic classes modeling the measurable maps $F, B$. Based on our non-asymptotic results, concrete upper bounds can be easily spelled out in the cases when $F, B$ are parametrized by deep neural networks. As mentioned, the RGM sampler also promises to identify good alignments between metric measure spaces, and to learn approximate isomorphism when possible. We demonstrate such a point using numerical experiments on MNIST.
	
	% \item Last, we study a convex formulation of solving RGM by relaxing and lifting it to an infinite-dimensional space, from a dual operator viewpoint. There are two reasons behind our convex formulation: first, as a computational alternative to the possibly non-convex optimization; second, to bring out a connection to Nadaraya-Watson estimator in classic nonparametric statistics.
\end{itemize}


\paragraph{Organization}
The rest of the paper is organized as follows. First, we briefly review other related studies that are omitted in the discussion above. Then, in Section~\ref{sec:background}, preliminary background on optimal transport and Gromov-Wasserstein distance is outlined. Next, Section~\ref{sec:summary-of-results} summarizes the primary methodology and theory regarding our proposed Reversible Gromov-Monge (RGM) Sampler, with detailed derivations deferred later. To be specific, Section~\ref{sec:analytic-properties} collects analytical properties of the RGM metric, Section~\ref{sec:statistical-theory} derives the non-asymptotic rate of convergence analyzing the statistical properties of the RGM sampler, and Section~\ref{sec:representation} discusses a further relaxation of the RGM into an infinite-dimensional convex program which relies on a new representer theorem. Finally, synthetic and real-world examples showcasing the effectiveness of the RGM sampler are demonstrated in Section~\ref{sec:numerical}. Other proof details omitted in the main text are designated to Appendix~\ref{sec:appendix}.


\subsection{Related Literature}

Modern data sets are mostly in an unstructured form. Inferring the underlying probability distributions from data has been a central problem in statistics and unsupervised machine learning since the invention of histograms by Pearson a century ago. Classic mathematical statistics explicitly models the density function in a parametric or a nonparametric way \citep{silverman_1986, wasserman_2006}, and studies the minimax optimality of directly estimating such density functions \citep{stone_1982}. It is also unclear how to proceed to sample from a possibly improper\footnote{Here we mean that the estimated density is non-negative and integrates to $1$.} density estimator, even with an optimal estimator at hand. One may employ Markov Chain Monte Carlo (MCMC) techniques for sampling from certain models. However, on the computational front, it is highly non-trivial how to ensure the mixing properties of MCMC for a designed sampler \citep[Chapter~7]{robert_casella_2004}.

Recent work in unsupervised machine learning proposes to learn complex, high-dimensional distributions via (deep) generative models, either explicitly by parametrizing the sufficient statistics of the exponential families \citep{doersch2016TutorialVariationalAutoencoders,kingma2013auto}, or implicitly by parametrizing the pushforward map transporting distributions \citep{dziugaite_roy_ghahramani_2015,goodfellow_2014}, with a focus on tractability in computation. Surprisingly, though lacking theoretical underpinning and optimality, the generative modelsâ€™ approach performs well empirically in large-scale applications where classical statistical procedures are destined to fail. There has been a growing literature on understanding distribution estimation with the implicit framework, with more general metrics and target distribution classes, to name a few, \cite{muandet_fukumizu_sriperumbudur_scholkopf_2017, li_swersky_zemel_2015, dziugaite_roy_ghahramani_2015} on MMDs, \cite{sriperumbudur2012empirical, liang2019EstimatingCertain} on integral probability metrics, and \cite{mroueh2017sobolev, arora2017generalization,liang2018HowWell,singh2018minimax,bai2018approximability, weed2019estimation, lei2019sgd, chen2020statistical} on generative adversarial networks. Last but not the least, we emphasize that an alternative implicit distribution estimation approach using the simulated method of moments has been formulated in the econometrics literature since \cite{mcfadden1989MethodSimulated,pakes1989SimulationAsymptotics} and \cite{gourieroux1997SimulationbasedEconometric}. 


The Gromov-Wasserstein distance was introduced in \cite{memoli_2007} as a relaxation of the Gromov-Hausdorff distance widely used for comparing metric spaces. Analytic properties of the Gromov-Wasserstein distance have been studied extensively \citep{memoli_2011,sturm_2012}; the most important one is that it defines a distance between metric measure spaces, namely, metric spaces endowed with probability measures. Since it is possible to model a number of real-world data sets via metric measure spaces, the Gromov-Wasserstein distance has been utilized in various problems that aim to compare two data sets such as shape correspondence \citep{solomon_etal_2016}, graph matching \citep{xu_luo_zha_duke_2019}, and network matching \citep{chowdhury_memoli_2019}.

Being a quadratic assignment program, computation of the Gromov-Wasserstein distances is intractable in general. QAP, which dates back to \cite{koopmans1957AssignmentProblems}, is known to be NP-hard \citep{cela1998QuadraticAssignment} in the worst case. As a result, several approaches have been proposed for the approximate computation of Gromov-Wasserstein distance, its upper and lower bounds, and other variants. \cite{memoli_2011} studies lower bounds on the Gromov-Wasserstein distance that are easier to compute. \cite{peyre_etal_2016} adds an entropic regularization term to the Gromov-Wasserstein distance, which leads to a fast iterative algorithm. Recently, the Sliced Gromov-Wasserstein distance has been proposed by \cite{titouan_etal_2019}, which amounts to integrating the Gromov-Wasserstein distances over one-dimensional projections.







\section{Background}
\label{sec:background}
In this section, we provide background on the Optimal Transport (OT) theory and the Gromov-Wasserstein distance. First, we start with some notations. We denote the Frobenius norm of a matrix $A$ as $\|A\|$. For any $x \in \R^p$ with $p \in \N \cup \{\infty\}$, we denote its $\ell^2$ norm as $\|x\|$. Given a set $\cX$ and a function $f \colon \cX \to \R$, we define $\|f\|_\infty = \sup_{x \in \cX} |f(x)|$ with the essential supremum. For an integer $n \in \N$, we define $[n] = \{1, \ldots, n\}$. Given a Polish space $\cX$, that is, a complete and separable metric space, we denote its metric as $d_{\cX}$ and write $\cP(\cX)$ to denote the collection of all Borel probability measures on $\cX$. We call a pair $(\cX, \mu)$ a Polish probability space if $\cX$ is a Polish space and $\mu \in \cP(\cX)$. Given two Polish probability spaces $(\cX, \mu)$ and $(\cY, \nu)$, the collection of all transport maps from $\mu$ to $\nu$ is denoted as $\cT(\mu, \nu) := \{ T:\cX \rightarrow \cY ~|~ T_\# \mu = \nu \}$. $\gamma \in \cP(\cX \times \cY)$ is called a coupling if $\gamma(A \times \cY) = \mu(A)$ and $\gamma(\cX \times B) = \nu(B)$ for all Borel subsets $A \subset \cX$ and $B \subset \cY$. We denote the collection of all such couplings as $\Pi(\mu, \nu)$. For a sequence of numbers $a(n), b(n) \in \R$, we use $a(n)\precsim b(n)$ to denote the asymptotic relationship that $\lim\sup_{n \rightarrow \infty} a(n)/b(n) < \infty$.

\subsection{A Brief Overview of Optimal Transport Theory}
A major goal of OT is minimizing the cost associated with the transport map between two given Polish probability spaces. To be concrete, let $(\cX, \mu)$ and $(\cY, \nu)$ be Polish probability spaces. Consider a measurable function $c \colon \cX \times \cY \to \R_{+}$; we view $c(x, y)$ as the cost associated with $x \in \cX$ and $y \in \cY$. For each transport map $T \in \cT(\mu, \nu)$, we interpret $c(x, T(x))$ as a unit cost incurred by mapping each $x \in \cX$ to $T(x) \in \cY$. We define the average cost incurred by the transport map $T$ as the integration of all the unit costs with respect to $\mu$:
\begin{equation*}
	\int_{\cX} c(x, T(x)) \dd{\mu}(x) \;.
\end{equation*}
Minimizing the cost over $\cT(\mu, \nu)$ is referred to as the Monge problem named after Gaspard Monge. If there exists a minimizer $T^\star$ to this problem, that is,
\begin{equation*}
	T^\star \in \argmin_{T \in \cT(\mu, \nu)} \int_{\cX} c(x, T(x)) \dd{\mu}(x)\;,
\end{equation*}
we call $T^\star$ an optimal transport map. 

Another important OT problem is minimizing the cost given by couplings between $(\cX, \mu)$ and $(\cY, \nu)$. We define the average cost incurred by a coupling $\gamma \in \Pi(\mu, \nu)$ as the integration of the cost $c(x, y)$ with respect to $\gamma$:
\begin{equation*}
	\int_{\cX \times \cY} c(x, y) \dd{\gamma}(x, y) \;.
\end{equation*}
Minimizing this cost over $\Pi(\mu, \nu)$ is called the Kantorovich problem credited to Leonid Kantorovich. If
\begin{equation*}
	\gamma^\star \in \argmin_{\gamma \in \Pi(\mu, \nu)} \int_{\cX \times \cY} c(x, y) \dd{\gamma}(x, y) \;,
\end{equation*}
we call $\gamma^\star$ an optimal coupling.

Two OT problems are closely related: the Kantorovich problem is a relaxation of the Monge problem. To see this, for each $T \in \cT(\mu, \nu)$, define a map $(\mathrm{Id}, T) \colon \cX \to \cX \times \cY$ with $(\mathrm{Id}, T)(x) = (x, T(x))$. One can verify $(\mathrm{Id}, T)_{\#} \mu \in \Pi(\mu, \nu)$. Therefore, if we define $\Pi_{\cT} := \{(\mathrm{Id}, T)_{\#} \mu : T \in \cT(\mu, \nu)\}$, then $\Pi_{\cT} \subset \Pi(\mu, \nu)$ and thus
\begin{equation*}
	\inf_{T \in \cT(\mu, \nu)} \int_{\cX} c(x, T(x)) \dd{\mu}(x)
	= \inf_{\gamma \in \Pi_{\cT}} \int_{\cX \times \cY} c(x, y) \dd{\gamma}(x, y)
	\ge \inf_{\gamma \in \Pi(\mu, \nu)} \int_{\cX \times \cY} c(x, y) \dd{\gamma}(x, y)\;,
\end{equation*}
where the first equality follows from change-of-variables. In other words, two OT problems share the same objective function as a function of couplings; however, the Kantorovich problem has a larger constraint set.

Unlike the Monge problem, the Kantorovich problem has favorable properties. First, the cost function is linear in $\gamma$. Moreover, $\Pi(\mu, \nu)$ is compact in the weak topology of Borel probability measures defined on $\cX \times \cY$. This suggests that we can view Kantorovich problem as an infinite-dimensional linear program.

Besides seeking optimal transport maps or couplings, another interesting aspect of OT problems is that the least possible cost can endow a metric structure among Polish probability spaces. More precisely, if $\cX = \cY$ and $c = d_{\cX}^2$, then the minimum of Kantorovich's problem defines a distance between $\mu$ and $\nu$, known as the Wasserstein distance.

\begin{definition}
	Given a Polish space $\cX$, the Wasserstein-2 distance between $\mu, \nu \in \cP(\cX)$ is defined as
	\begin{equation*}
		W_2(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \left(\int_{\cX \times \cX} d_{\cX}^2(x, y) \dd{\gamma}(x, y)\right)^{1/2}\;.
	\end{equation*}
\end{definition}
\begin{remark}
	One can define the Wasserstein-$p$ distance by replacing the exponent $2$ above with $p \in [1, \infty]$. The Wasserstein-$p$ distance is known to satisfy the usual metric axioms.
\end{remark}


\subsection{Gromov-Wasserstein and Gromov-Monge Distances}
Although OT problems can be defined between arbitrary Polish probability spaces, in practice, it is unclear how to design a function $c \colon \cX \times \cY \to \R_+$ to represent meaningful cost associated with $x \in \cX$ and $y \in \cY$ in two heterogenous spaces.
For instance, if $\cX = \R^p$ and $\cY = \R^q$ with $p \neq q$, there is no simple choice for a cost function $c$ over $\R^p \times \R^q$. As a result, classic OT theory (including Brenier's result) is not suited for comparing heterogenous Polish probability spaces.

M{\'e}moli's pioneering work \cite{memoli_2011} resolved this issue by considering a quadratic objective function of $\gamma$:
\begin{equation*}
	\int_{\cX \times \cY} c(x, y) \dd{\gamma}(x, y) \Rightarrow \int_{\cX \times \cY}\int_{\cX \times \cY} (c_{\cX}(x, x') - c_{\cY}(y, y'))^2 \dd{\gamma}(x, y) \dd{\gamma}(x', y') \;,
\end{equation*}
where $c_{\cX}$ and $c_{\cY}$ are defined over $\cX \times \cX$ and $\cY \times \cY$, respectively. For instance, one can specify $c_{\cX} = d_{\cX}$ and $c_{\cY} = d_{\cY}$. Rather than considering a unit cost corresponding to each pair $(x, y) \in \cX \times \cY$, two pairs $(x, y)$ and $(x', y')$ in $\cX \times \cY$ are associated with the discrepancy of intra-space quantities $c_{\cX}(x, x')$ and $c_{\cY}(y, y')$. In summary, by switching from the integration $\dd{\gamma}$ to the integration $\dd{\gamma \otimes \gamma}$, we no longer need an otherwise inter-space quantity $c \colon \cX \times \cY \to \R_+$. Therefore, we can always define this objective function whenever we have proper $c_{\cX}$ and $c_{\cY}$ in each individual space, which leads to the following definition.

\begin{definition}
	\label{def:mms}
	A triple $(\cX, \mu, c_{\cX})$ is called a network space if $(\cX, \mu)$ is a Polish probability space such that $\mathrm{supp}(\mu) = \cX$ and $c_{\cX} \colon \cX \times \cX \to \R$ is continuous. The Gromov-Wasserstein distance between network spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ is defined as
	\begin{equation*}
		\mathrm{GW}(\mu, \nu) = \inf_{\gamma \in \Pi(\mu, \nu)} \left( \int_{\cX \times \cY} \int_{\cX \times \cY} (c_{\cX}(x, x') - c_{\cY}(y, y'))^2 \dd{\gamma}(x, y) \dd{\gamma}(x', y') \right)^{1/2}\;.
	\end{equation*}
\end{definition}

\begin{remark} \rm
	This definition is based on Definition 1 of \cite{chowdhury_memoli_2019}. A network space $(\cX, \mu, c_{\cX})$ is called a metric measure space if $c_{\cX} = d_{\cX}$ as introduced in \cite{memoli_2011} and \cite{sturm_2012}. In short, a network space is a generalization of a metric measure space.
\end{remark}

Like the Wasserstein distance, the Gromov-Wasserstein distance has metric properties; it satisfies symmetry and the triangle inequality, and $\mathrm{GW}(\mu, \nu) = 0$ if $(\cX, \mu, c_{\cX}) = (\cY, \nu, c_{\cY})$. However, the converse of this last statement does not hold in general: for its validity, a suitable equivalence relation needs to be defined on the collection of network spaces.

\begin{definition}
	\label{def:iso}
	Network spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ are strongly isomorphic if there exists $T \in \cT(\mu, \nu)$ such that $T \colon \cX \to \cY$ is bijective and $c_{\cX}(x, x') = c_{\cY}(T(x), T(x'))$ for all $x, x' \in \cX$. In this case, we write $(\cX, \mu, c_{\cX}) \cong (\cY, \nu, c_{\cY})$ and such a transport map $T$ is called a strong isomorphism.
\end{definition}

One can easily check that $\cong$ is indeed an equivalence relation on the collection of network spaces. The following theorem states that the Gromov-Wasserstein distance satisfies all metric axioms on the quotient space---under the equivalence relation $\cong$---of metric measure spaces.

\begin{theorem}[Lemma 1.10 of \cite{sturm_2012}]
	\label{thm:1}
	Let $\cM$ be the collection of all network spaces $(\cX, \mu, c_{\cX})$ such that $c_{\cX} = d_{\cX}$. Also, let $\cM/_{\cong}$ be the collection of all equivalence classes of $\cM$ induced by $\cong$. Then, GW satisfies the three metric axioms on $\cM/_{\cong}$.
\end{theorem}

Recall that the Monge problem is a restricted version of the Kantorovich problem with an additional constraint that couplings are given by a transport map; replacing $\Pi(\mu, \nu)$ in the Kantorovich problem with $\Pi_{\cT}$ yields the Monge problem. Imposing the same constraint on the definition of GW leads to the Gromov-Monge distance.
\begin{definition}
	The Gromov-Monge distance between network spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ is defined as
	\begin{equation*}
		\mathrm{GM}(\mu, \nu) = \inf_{T \in \cT(\mu, \nu)} \left( \int_{\cX} \int_{\cX} (c_{\cX}(x, x') - c_{\cY}(T(x), T(x')))^2 \dd{\mu}(x) \dd{\mu}(x') \right)^{1/2} \;.
	\end{equation*}
\end{definition}
Loosely speaking, computing GM amounts to finding a transport map $T$ such that $c_{\cX}(x, x')$ best matches $c_{\cY}(T(x), T(x'))$ on average; we can view such a map $T$ as a surrogate for an isomorphism.


\section{Summary of Results}
\label{sec:summary-of-results}
Inspired by the Gromov-Wasserstein and Gromov-Monge distances, we propose a new metric---the reversible Gromov-Monge distance---between network spaces in this paper. Our formulation seeks a pair of transport maps $F \in \cT(\mu, \nu)$ and $B \in \cT(\nu, \mu)$ best approximating isomorphic relations between network spaces. We propose a novel transform sampling method that uses $F$ as a push-forward map to obtain i.i.d.\ samples from a target distribution $\nu$.
We present two optimization formulations solving for such a pair $(F, B)$ in order: a potentially non-convex formulation that employs standard gradient descent method to optimize, and an infinite-dimensional convex formulation where global optima can be found efficiently. For the former, we analyze the statistical rate of convergence, for generic classes $\cF \times \cB$ parametrizing $(F, B)$. For the latter, we derive a new representer theorem on suitable reproducing kernel Hilbert spaces (RKHS).
% We discuss two optimization  methods for finding such a pair $(F, B)$: a gradient descent method and convex optimization. We will establish a convergence result for the former via statistical analysis. The latter is a consequence of the representer theorem on a suitable reproducing kernel Hilbert space (RKHS).

\subsection{Metric Properties of Reversible Gromov-Monge}
Our formulation is based on the following observation: for a coupling $\gamma$ such that $\gamma = (\mathrm{Id}, F)_{\#}\mu = (B, \mathrm{Id})_{\#} \nu$, which presents a binding constraint, we can simplify the objective function of GW as
\begin{equation*}
	\int_{\cX \times \cY} (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu} \;,
\end{equation*}
where $\dd{\mu \otimes \nu} := \dd{\mu}(x) \dd{\nu}(y)$ denotes the product measure of $\mu$ and $\nu$.
Imposing the binding constraint on the definition of GW leads to the following definition.
\begin{definition}
	For network spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$, we write $(F, B) \in \cI(\mu, \nu)$ if measurable maps $F \colon \cX \to \cY$ and $B \colon \cY \to \cX$ satisfy the binding constraint $(\mathrm{Id}, F)_{\#}\mu = (B, \mathrm{Id})_{\#} \nu$. We define the reversible Gromov-Monge (RGM) distance between $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ as
	\begin{equation*}
		\mathrm{RGM}(\mu, \nu) := \inf_{(F, B) \in \cI(\mu, \nu)} \left(\int_{\cX \times \cY} (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu}\right)^{1/2}\;.
	\end{equation*}
\end{definition}

\begin{remark}
	\rm
	A few remarks are in place for the binding constraint.
	If $(\mathrm{Id}, F)_{\#}\mu = (B, \mathrm{Id})_{\#} \nu$, then $F_{\#} \mu = \nu$ and $B_{\#} \nu = \mu$ follow due to marginal conditions. However, the converse is not true in general. To see this, let $\mu = \nu = \mathrm{Unif}([0, 1])$, then $F_{\#} \mu = \nu$ and $B_{\#} \nu = \mu$ hold for $F(x) = B(x) = |2 x - 1|$. However, $(\mathrm{Id}, F)_{\#}\mu \neq (B, \mathrm{Id})_{\#} \nu$ because $(\mathrm{Id}, F)_{\#}\mu$ is a uniform measure on $\{(x, |2x-1|): x \in [0, 1]\}$, whereas $(B, \mathrm{Id})_{\#} \nu$ is a uniform measure on $\{(|2y-1|, y): y \in [0, 1]\}$.
\end{remark}

Roughly speaking, computing RGM consists in finding a pair $(F, B) \in \cI(\mu, \nu)$ such that $c_{\cX}(x, B(y))$ best matches $c_{\cY}(F(x), y)$ on average. Like a strong isomorphism, we can view such a pair as jointly capturing an isomorphic relation of $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$. We will use this observation later to build a transform sampling method.

We will prove that RGM possesses metric properties similar to the Gromov-Wasserstein. Motivated by Theorem \ref{thm:1}, we derive the following result.
\begin{theorem}
	\label{thm:metric}
	Let $h \colon \R_+ \to \R$ be a continuous and strictly monotone function and $\cN^{h}$ be a collection of all network spaces $(\cX, \mu, c_{\cX})$ such that $c_{\cX} = h(d_{\cX})$. Then RGM satisfies the three metric axioms on $\cN^h/_{\cong}$ which is the collection of all equivalence classes of $\cN^h$ induced by $\cong$.
\end{theorem}
\begin{remark}
	\rm
	Suppose $\cX$ is a Euclidean space and $d_{\cX}$ is the standard Euclidean distance. If $h(x) = \exp(- \alpha x^2)$ with $\alpha > 0$, then $h(d_{\cX})$ is the radial basis function (RBF) kernel on $\cX$; we will use this in numerical experiments.
\end{remark}

We refer the proof of Theorem \ref{thm:metric} and details on analytic properties of RGM to Section \ref{sec:analytic-properties}.


\subsection{Transform Sampling via RGM}
With the proposed notion of RGM, we design a transform sampling method in this section. The transform sampler is based on finding a minimizing pair $(F, B)$ of RGM, which can capture isomorphic relations between network spaces. To implement this method, we need to estimate $(F, B)$ using only i.i.d.\ samples from $\mu$ and $\nu$. Leveraging the Lagrangian form, we derive a minimization problem that can be implemented based on finite samples.

First, we rewrite the population minimization problem with the binding constraint as follows,
\begin{align*}
	\min_{\substack{F \colon \cX \to \cY \\ B \colon \cY \to \cX}} \quad & \int_{\cX \times \cY} (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu} \\
	\mathrm{s.t.} \quad & \cL_{\cX \times \cY}((\mathrm{Id}, F)_{\#} \mu, (B, \mathrm{Id})_{\#} \nu) = 0\;.
\end{align*}
Here, $\cL_{\cX \times \cY}$ is a suitable discrepancy measure on $\cP(\cX \times \cY)$ so that $\cL_{\cX \times \cY}((\mathrm{Id}, F)_{\#} \mu, (B, \mathrm{Id})_{\#} \nu) = 0$ is a surrogate for the original constraint $(\mathrm{Id}, F)_{\#}\mu = (B, \mathrm{Id})_{\#} \nu$. In practice, we do not require $\cL_{\cX \times \cY} = 0$ implies $(\mathrm{Id}, F)_{\#}\mu = (B, \mathrm{Id})_{\#} \nu$; in fact, the former constraint can be a relaxation of the latter. The choice of $\cL_{\cX \times \cY}$ will be specified later. To solve this minimization problem, we propose utilizing the Lagrangian:
\begin{equation*}
	\min_{\substack{F \colon \cX \to \cY \\ B \colon \cY \to \cX}} \int_{\cX \times \cY} (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu} + \lambda \cdot \cL_{\cX \times \cY}((\mathrm{Id}, F)_{\#} \mu, (B, \mathrm{Id})_{\#} \nu)\;.
\end{equation*}

Given i.i.d.\ samples $\{x_i\}_{i=1}^{m}$ and $\{y_j\}_{j=1}^{n}$ from $\mu$ and $\nu$, respectively, we replace the population objective with its empirical estimates:
\begin{equation*}
	\min_{\substack{F \colon \cX \to \cY \\ B \colon \cY \to \cX}} \frac{1}{m n} \sum_{i = 1}^{m} \sum_{j = 1}^{n} (c_{\cX}(x_i, B(y_j)) - c_{\cY}(F(x_i), y_j))^2 + \lambda \cdot \cL_{\cX \times \cY}((\mathrm{Id}, F)_{\#} \widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n)\;,
\end{equation*}
where $\widehat{\mu}_m$ and $\widehat{\nu}_n$ are the empirical measures based on $\{x_i\}_{i=1}^{m}$ and $\{y_j\}_{j=1}^{n}$, respectively. Empirically, we find that adding the following extra terms often enhance empirical results:
\begin{equation*}
	\begin{split}
		\min_{\substack{F \colon \cX \to \cY \\ B \colon \cY \to \cX}} \quad & \frac{1}{m n} \sum_{i = 1}^{m} \sum_{j = 1}^{n} (c_{\cX}(x_i, B(y_j)) - c_{\cY}(F(x_i), y_j))^2 + \lambda_1 \cdot \cL_{\cX \times \cY}((\mathrm{Id}, F)_{\#} \widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) \\
		& + \lambda_2 \cdot \cL_{\cX}(\widehat{\mu}_m, B_{\#} \widehat{\nu}_n) + \lambda_3 \cdot \cL_{\cY}(F_{\#} \widehat{\mu}_m, \widehat{\nu}_n)\;.
	\end{split}
\end{equation*}
Like $\cL_{\cX}$, we utilize suitable discrepancy measures $\cL_{\cX}$ and $\cL_{\cY}$ so that these additional terms help matching the marginals of $(\mathrm{Id}, F)_{\#} \widehat{\mu}_m$ and $(B, \mathrm{Id})_{\#} \widehat{\nu}_n$.

Lastly, we discuss the choice of $\cL_{\cX}, \cL_{\cY}$, and $\cL_{\cX \times \cY}$. We use the square of MMD as the leading example for two reasons. First, MMD is a metric between Borel probability measures under mild conditions. Also, the square of MMD between discrete distributions admits a closed form. For instance, let $K_{\cX}$ be a kernel on $\cX$, then MMD between $\widehat{\mu}_m$ and $B_{\#} \widehat{\nu}_n$ is
\begin{equation*}
	\frac{1}{m^2} \sum_{i, i'} K_{\cX}(x_i, x_{i'})	+ \frac{1}{n^2} \sum_{j, j'} K_{\cX}(B(y_j), B(y_{j'}))	- \frac{2}{mn} \sum_{i, j} K_{\cX}(x_i, B(y_j))\;.
\end{equation*}
By choosing kernels $K_{\cX}, K_{\cY}, K_{\cX \times \cY}$ on $\cX, \cY, \cX \times \cY$, respectively, we can specify $\cL_{\cX}, \cL_{\cY}, \cL_{\cX \times \cY}$ as the square of corresponding MMDs. For the kernel $K_{\cX \times \cY}$ on the product space, we use the tensor product kernel $K_{\cX} \otimes K_{\cY}$ given as
\begin{equation*}
	K_{\cX} \otimes K_{\cY}((x, y), (x', y')) = K_{\cX}(x, x') K_{\cY}(y, y')\;.
\end{equation*}
The tensor product notation is employed since the kernel on the product space inherits the feature map as the tensor product of two individual feature maps w.r.t. $K_{\cX}$ and $K_{\cY}$.

Denoting the MMD associated with a kernel $K$ as $\mathrm{MMD}_{K}$, we obtain the following minimization problem:
\begin{equation}
	\label{eqn:1}
	\begin{split}
		\min_{\substack{F \colon \cX \to \cY \\ B \colon \cY \to \cX}} \quad & \frac{1}{m n} \sum_{i = 1}^{m} \sum_{j = 1}^{n} (c_{\cX}(x_i, B(y_j)) - c_{\cY}(F(x_i), y_j))^2  + \lambda_1 \cdot \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}}^2((\mathrm{Id}, F)_{\#} \widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n)\\
		& + \lambda_2 \cdot \mathrm{MMD}_{K_{\cX}}^2(\widehat{\mu}_m, B_{\#} \widehat{\nu}_n) + \lambda_3 \cdot \mathrm{MMD}_{K_{\cY}}^2(F_{\#} \widehat{\mu}_m, \widehat{\nu}_n) \;.
	\end{split}
\end{equation}

Once we solve the problem above, the solution $\widehat{F}:\cX\to\cY$ will serve as an approximate isomorphism and facilitate transform sampling of the target $\nu$ from a known distribution $\mu$. The map $\widehat{B}$ possesses similar properties as $\widehat{F}$, whereas the map $\widehat{F}$ is of our primary interest for sampling purposes. The reverse map $\widehat{B}:\cY \rightarrow \cX$ also embeds point clouds in $\cY$ into $\cX$, with approximate isomorphism properties in the sense of Gromov-Monge.

\subsection{Statistical Rate of Convergence}


Like other transform sampling approaches for generative models, we consider \eqref{eqn:1} using vector-valued function classes $\cF$ and $\cB$ parametrized by neural networks, and then optimize using a gradient descent algorithm. We emphasize this minimization problem is much simpler than adversarial formulations as in GANs: variational problems of GANs consist of minimization over a class of generators and maximization over a class of discriminators, which requires complex saddle-point dynamics \citep{daskalakis2017training, liang2018interaction}. In contrast, our RGM only solves a single minimization problem in network parameters. Although generally non-convex in nature, the parameter minimization problem in neural networks can often be efficiently optimized by stochastic gradient descent, and can even provably achieve the global optima if the loss satisfies certain Polyak-\L ojasiewicz conditions in the overparametrized regime \citep{bassily_belkin_ma_2018}.

We investigate the statistical rate of convergence for this minimization problem, assuming the empirical problem \eqref{eqn:1} can be solved accurately. First, define
\begin{equation}
	\label{eqn:cost-def}
	\begin{split}
		C(\mu, \nu, F, B) := & \int (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu} + \lambda_1 \cdot \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}}^2((\mathrm{Id}, F)_{\#} \mu, (B, \mathrm{Id})_{\#} \nu)\\
		& + \lambda_2 \cdot \mathrm{MMD}_{K_{\cX}}^2(\mu, B_{\#} \nu) + \lambda_3 \cdot \mathrm{MMD}_{K_{\cY}}^2(F_{\#} \mu, \nu) \;.
	\end{split}
\end{equation}
Then, the objective function of \eqref{eqn:1} is a plug-in estimator $C(\widehat{\mu}_m, \widehat{\nu}_n, F, B)$. Now, consider solving \eqref{eqn:1} over the transformation class $\cF \times \cB$ given as follows. Our non-asymptotic results work for generic classes $\cF \times \cB$.

\begin{definition}[Transformation Classes]
	\label{a:transform_class}
	Let $\cX$ and $\cY$ be subsets of Euclidean spaces of dimensions $\mathrm{dim}(\cX)$ and $\mathrm{dim}(\cY)$, respectively. $\cF$ (resp.\ $\cB$) is a collection of vector-valued measurable functions from $\cX$ to $\cY$ (resp.\ from $\cY$ to $\cX$). For each $F \in \cF$ and $k \in [\mathrm{dim}(\cY)]$, we write $F_k(x)$ to denote the $k$-th coordinate of $F(x)$. Accordingly, we define $\cF_k =\{F_k: \cX \rightarrow \R ~|~ F \in \cF\}$, namely, a collection of real-valued measurable functions defined on $\cX$ that are given as the $k$-th coordinate of $F \in \cF$. For $\ell \in [\mathrm{dim}(\cX)]$, we define $B_\ell$ and $\cB_{\ell} = \{B_\ell: \cY \rightarrow \R ~|~ B \in \cB\}$ analogously.
\end{definition}

For $\cF$ and $\cB$ given in Definition~\ref{a:transform_class}, solving \eqref{eqn:1} over $\cF \times \cB$ is written as
\begin{equation*}
	\min_{(F, B) \in \cF \times \cB} C(\widehat{\mu}_m, \widehat{\nu}_n, F, B)\;.
\end{equation*}
We prove that the empirical solution leads to an approximate infimum of $(F, B) \mapsto C(\mu, \nu, F, B)$ evaluated with the population measures $\mu, \nu$, with sufficiently large sample sizes $m$ and $n$.



\begin{theorem}
	\label{thm:stat}
	For $\cF$ and $\cB$ given in Definition~\ref{a:transform_class}, let $(\widehat{F}, \widehat{B})$ be a solution to the empirical RGM problem 
	\begin{align*}
		& (\widehat{F}, \widehat{B}) \in \argmin_{(F, B) \in \cF \times \cB} C(\widehat{\mu}_m, \widehat{\nu}_n, F, B)\;,
	\end{align*}
	with $C: \cP(\cX) \times \cP(\cY) \times \cF \times \cB \rightarrow \R$ defined in \eqref{eqn:cost-def}. Under Assumptions \ref{a:bounded1}-\ref{a:separation}, the following inequality holds with probability $1- \delta$ on $\{x_i\}_{i = 1}^m$ and $\{y_j\}_{j = 1}^n$
	\begin{align}
		C(\mu, \nu, \widehat{F}, \widehat{B}) - \inf_{(F, B) \in \cF \times \cB} C(\mu, \nu, F, B)  \precsim \cM(\cF, \cB, m, n, \delta).
	\end{align}
	Here, $\cM(\cF, \cB, m, n, \delta)$ denotes a complexity measure of $(\cF, \cB)$ given in terms of pseudo-dimensions (Pdim) of $\cF_k$ and $\cB_\ell$:
	\begin{equation*}
		\cM(\cF, \cB, m, n, \delta)
		:=
		\sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m\wedge n}} + \sqrt{\frac{\log(m \vee n)}{m\wedge n} \left( \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) + \sum_{\ell = 1}^{{\rm dim}(\cX)} {\rm Pdim}(\cB_{\ell}) \right)}.
	\end{equation*}
\end{theorem}
We will provide required assumptions and the proof of Theorem \ref{thm:stat} in Section \ref{sec:statistical-theory} along with the definition of the pseudo-dimension. When $\cF$ and $\cB$ are parametrized by neural network classes (the ones we will use for numerical demonstrations in Section~\ref{sec:numerical}), tight pseudo-dimension bounds established in \cite{anthony_bartlett_1999, harvey2017NearlytightVCdimension} can be plugged in Theorem 3 for concrete non-asymptotic rates. 




\subsection{Convex Formulation and Representer Theorem}
\label{subsec:cvx-representer}

As the last bit of our contributions, we study a convex formulation of solving \eqref{eqn:1} by relaxing and lifting it to an infinite-dimensional space. There are two reasons behind our convex formulation: first, as a computational alternative to the possibly non-convex optimization; second, to point out a connection with the Nadaraya-Watson estimator in classic nonparametric statistics. The crux lies in relaxing optimizing over the map $F:\cX \rightarrow \cY$ to optimizing over its induced (dual) linear operator $\bF: L^2_\cY \rightarrow L^2_\cX$ that maps functions on $\cY$ to functions on $\cX$. Let $\pi_\cX$ be a Borel measure on $\cX$ and $L^2_{\cX}$ be the collection of real-valued measurable functions $f$ defined on $\cX$ such that $\int_{\cX} f^2 \dd{\pi_{\cX}} < \infty$. Similarly, define $L^2_{\cY}$ given a Borel measure $\pi_{\cY}$ on $\cY$. Then, for a measurable map $F \colon \cX \to \cY$, we can define $\bF \colon L^2_{\cY} \to L^2_{\cX}$ by letting $\bF(g) = g \circ F$ for all $g \in L^2_{\cY}$. Similarly, we define $\bB \colon L^2_{\cX} \to L^2_{\cY}$ for each measurable map $B \colon \cY \to \cX$. We will see $\bF$ and $\bB$ are well-defined bounded linear operators in Section \ref{sec:representation} under a mild assumption. 

To state the representer theorem, consider \eqref{eqn:1} with $c_{\cX} = K_{\cX}$ and $c_{\cY} = K_{\cY}$, same as kernel functions specified in MMD terms. We show that this problem can be reduced to a finite-dimensional convex optimization by proving a representer theorem. Since finite-dimensional convex optimization can be optimized globally with provable guarantees, such a formulation can be solved numerically in an efficient way.

Let us lay out more details to state the result. Due to Mercer's theorem,  let $\{ \phi_k \in L^2_{\cX} \}_{k \in \N} $ and $\{ \psi_\ell \in L^2_{\cY} \}_{\ell \in \N}$ be countable orthonormal bases of $L^2_{\cX}$ and $L^2_{\cY}$ where the kernels admit the following spectral decompositions:
\begin{align}
	\label{eqn:spectral}
	K_{\cX}(x, x') = \sum_k \lambda_k \phi_k(x) \phi_k(x') \;, \quad   K_{\cY}(y, y') = \sum_{\ell} \gamma_\ell \psi_\ell(y) \psi_\ell(y') \;,
\end{align}
with positive eigenvalues $\lambda_k, \gamma_\ell >0$.
Since $\bF: L^2_{\cY} \rightarrow L^2_{\cX}$ defines a bounded linear operator, one can represent $\bF$ (correspondingly $\bB$) under the orthonormal bases
\begin{align}
	\bF[\psi_{\ell}] = \sum_{k = 1}^{\infty} \bF_{k \ell} \phi_k\;, \quad \bB[\phi_{k}] = \sum_{\ell = 1}^{\infty} \bB_{\ell k} \psi_\ell\;.
\end{align}
Here, $[\bF_{k \ell}]$ is a semi-infinite matrix with each column describing the $L^2_{\cX}$ representation of $\bF[\psi_{\ell}]$ under the basis $\{ \phi_k \in L^2_{\cX} \}_{k \in \N}$. With a slight abuse of notation, we will write $\bF$ and $\bB$ to denote these matrices $[\bF_{k \ell}]$ and $[\bB_{\ell k}]$. With these notations, \eqref{eqn:1} with $c_{\cX} = K_{\cX}$ and $c_{\cY} = K_{\cY}$ can be lifted to an infinite-dimensional optimization where the decision variables are matrices $\bF$ and $\bB$
\begin{equation}
	\label{eqn:op}
	\min_{(\bF, \bB) \in \cC} ~ \Omega(\bF, \bB)\;,
\end{equation}
where the exact form of $\Omega$ is deferred to Section \ref{sec:representation}. Here, $\cC$ denotes the constraint set implying that $\bF$ and $\bB$ are matrices corresponding to bounded linear operators induced by some maps $F \colon \cX \to \cY$ and $B \colon \cY \to \cX$.

We will relax this problem by removing the constraint set $\cC$, namely, by considering all matrices in $\R^{\infty \times \infty}$ as the decision variables, 
\begin{equation}
	\label{eqn:relaxed}
	\min_{\bF, \bB \in \R^{\infty \times \infty}} ~ \Omega(\bF, \bB)\;.
\end{equation}
In other words, this relaxed problem minimizes $\Omega$ over any pair of infinite-dimensional matrices. The next result, which we refer to as the representer theorem, shows that \eqref{eqn:relaxed} boils down to a finite-dimensional convex program. 

\begin{theorem}
	\label{thm:representer}
	Consider the optimization \eqref{eqn:op} under the assumptions in Proposition \ref{prop:representer}. Then, for any minimizer $(\bF^\star, \bB^\star)$ to the relaxed problem \eqref{eqn:relaxed}, we can find finite-dimensional matrices $\mathsf{F}_{m,n}^{\star} \in \R^{m \times n}$ and $\mathsf{B}_{n, m}^{\star} \in \R^{n \times m}$ such that
	\begin{align*}
		% \Lambda^{-1/2} \bF^{\star} \Gamma^{1/2} = \Lambda^{1/2} \Phi_m \mathsf{F}_{m,n}^{\star} \Psi_n^\top \Gamma^{1/2} \;, \\
% 		\Gamma^{-1/2} \bB^{\star} \Lambda^{1/2} = \Gamma^{1/2} \Psi_n \mathsf{B}_{n,m}^{\star} \Phi_m^\top \Lambda^{1/2} \;,
		\bF^{\star}  = \Lambda \Phi_m \mathsf{F}_{m,n}^{\star} \Psi_n^\top \;, \\
		\bB^{\star}  = \Gamma \Psi_n \mathsf{B}_{n,m}^{\star} \Phi_m^\top \;,
	\end{align*}
	where $\Lambda = \text{diag}(\lambda_1, \lambda_2, \dots), ~\Gamma = \text{diag}(\gamma_1, \gamma_2, \dots)$, and $\Phi_{m} \in \R^{\infty \times m}$ and $\Psi_{n} \in \R^{\infty \times n}$ are matrices whose elements are $\phi_k(x_i)$ and $\psi_{\ell}(y_j)$, as defined in \eqref{eqn:spectral}. In this case, $\Omega(\bF^\star, \bB^\star)$ can be rewritten as $\omega(\mathsf{F}_{m, n}^{\star}, \mathsf{B}_{n, m}^{\star})$ for some convex function $\omega$ defined over $\R^{m \times n} \times \R^{n \times m}$. In other words, by minimizing $\omega$ over $\R^{m \times n} \times \R^{n \times m}$, we obtain a relaxation of \eqref{eqn:relaxed}, that is, 
	\begin{equation*}
		\min_{\bF, \bB \in \R^{\infty \times \infty}} \Omega(\bF, \bB)
		\ge
		\min_{\substack{\mathsf{F}_{m, n} \in \R^{m \times n} \\ \mathsf{B}_{n, m} \in \R^{n \times m}}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \;.
	\end{equation*}
	In particular, the RHS is a finite-dimensional convex optimization. Lastly, this relaxation is tight, that is,  
	\begin{equation*}
		\min_{\bF, \bB \in \R^{\infty \times \infty}} \Omega(\bF, \bB)
		=
		\min_{\substack{\mathsf{F}_{m, n} \in \R^{m \times n} \\ \mathsf{B}_{n, m} \in \R^{n \times m}}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \;,
	\end{equation*}
	if kernel matrices $\bK_{\cX}$ and $\bK_{\cY}$ whose elements are $K_{\cX}(x_i, x_{i'})$ and $K_{\cY}(y_j, y_{j'})$, are positive definite. 
\end{theorem}

\begin{remark}
	\rm
	Looking inside the proof of Theorem \ref{thm:representer}, we know the solution to the infinite-dimensional optimization is an operator taking form of $\bF^\star = \Lambda \Phi_m \mathsf{F}_{m,n}^\star \Psi_n^\top$, with a finite-dimensional matrix $\mathsf{F}_{m,n}^\star\in \R^{m \times n}$. Therefore, for any $g \in L^2_\cY$, we can deduce
	\begin{align}
		\label{eqn:nw-connection}
		\bF^\star[g](x) =  \underbrace{K_\cX(x, X_m)}_{1\times m} \underbrace{\mathsf{F}_{m,n}^\star}_{m\times n} \underbrace{g(Y_n)}_{n\times 1} \;,
	\end{align}
	where $K_{\cX}(x, X_m)$ maps each $x \in \cX$ to a row vector whose $i$-th element is $K_{\cX}(x, x_i)$ and $g(Y_n)$ denotes a column vector whose $j$-th element is $g(y_j)$. 
	
	Now let's draw a connection between the classic Nadaraya-Watson estimator and \eqref{eqn:nw-connection}. For now consider a special case: $(x_i, y_i)$'s are paired with $m = n$. In such a case, Nadaraya-Watson estimator takes the form
	\begin{align}
		\sum_{i,j} K_{\cX}(x, x_i) \cdot \tfrac{1}{m} \delta_{i=j} \cdot g(y_j) \; ;
	\end{align}
	Namely, for a new point $x$, the corresponding function value $g(y)$ evaluated on its coupled $y = F(x)$ is a weighted average of $g(y_j)$'s according to the affinity $K_{\cX}(x, x_i)$. Our solution \eqref{eqn:nw-connection} extends the above nonparametric smoothing idea to the decoupled data case, where the coupling weights $\mathsf{F}_{m,n}^\star$ is based on a solution to a convex program, with
	\begin{align}
		\eqref{eqn:nw-connection} = \sum_{i,j} K_{\cX}(x, x_i) \cdot \mathsf{F}_{m,n}^\star[i,j] \cdot g(y_j) \;.
	\end{align}
	
	Lastly, we draw another connection to the Monte-Carlo integration. One downstream task after learning the distribution $\nu$ is to perform numerical integration of $g \in L^2_\cY$ under the measure $\nu \in \cP(\cY)$. In our transform sampling framework, this amounts to evaluate $\E_{y \sim F^\star_\# \mu}[ g(y)] = \E_{x \sim \mu}[ g\circ F^\star (x)]$. The integration, casted in the induced operator form, has the expression
	\begin{align}
		\label{eqn:weights-MC}
		\E_{x \sim \mu} \big[ \bF^\star[g](x) \big] = \E_{x \sim \mu}  \big[ \underbrace{K_\cX(x, X_m) \mathsf{F}_{m,n}^\star}_{=: W(x) \in \R^n } g(Y_n) \big] = \E_{x \sim \mu} \big[ \sum_{j=1}^n W_j(x) g(y_j) \big]
	\end{align}
	where $W(x)$ can be interpreted as the importance weights in the Monte-Carlo integration. We conclude with one more remark: if plug in instead $x \sim \widehat{\mu}_m$ in \eqref{eqn:weights-MC}, one can verify that under mild conditions, 
	\begin{align}
		\E_{x \sim \widehat{\mu}_m} \big[ \bF^\star[g](x) \big] = \frac{1}{n} \sum_{j=1}^n  g(y_j) \;.
	\end{align}
	In other words, with the empirical measure as input, \eqref{eqn:weights-MC} outputs the simple sample average. 
\end{remark}




\section{Analytic Properties}
\label{sec:analytic-properties}
In this section, we derive analytic properties of the proposed RGM distance. First, we discuss the relations among three distances: GW, GM, and RGM.
\begin{proposition}\label{prop:1}
	For network spaces $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ as in Definition~\ref{def:mms},
	\begin{equation*}
		\mathrm{GW}(\mu, \nu) \le \mathrm{GM}(\mu, \nu) \le \mathrm{RGM}(\mu, \nu)\;.
	\end{equation*}
\end{proposition}

The proof follows because our formulation can be obtained by further restricting the constraint set of couplings in GM. Note that our RGM is symmetric while the original GM is not. As a simple corollary, one can check
\begin{equation*}
		\max\{\mathrm{GM}(\mu, \nu), \mathrm{GM}(\nu, \mu)\}
		\le \mathrm{RGM}(\mu, \nu)\;.
\end{equation*}
Now, we establish further metric properties of RGM. Symmetry of RGM is already mentioned. Next, we prove a triangle inequality using a gluing technique as in OT.

\begin{proposition}
	\label{prop:2}
	RGM satisfies the triangle inequality, that is,
	\begin{equation*}
		\mathrm{RGM}(\mu_{\cX}, \mu_{\cZ}) \le \mathrm{RGM}(\mu_{\cX}, \mu_{\cY}) + \mathrm{RGM}(\mu_{\cY}, \mu_{\cZ})
	\end{equation*}
	holds for three network spaces $(\cX, \mu_{\cX}, c_{\cX})$, $(\cY, \mu_{\cY}, c_{\cY})$, and $(\cZ, \mu_{\cZ}, c_{\cZ})$.
\end{proposition}


Next, we study whether $\mathrm{RGM}(\mu, \nu) = 0$ holds if and only if $(\cX, \mu, c_{\cX}) \cong (\cY, \nu, c_{\cY})$. Here the equivalence relation induced by $\cong$ can be read from Definition~\ref{def:iso}. Like the Gromov-Wasserstein distance, in general, we can only assert the if part without further conditions. The following proposition states that $\mathrm{RGM}(\mu, \nu) = 0$ if and only if $(\cX, \mu, c_{\cX}) \cong (\cY, \nu, c_{\cY})$ under some additional conditions on $c_{\cX}$ and $c_{\cY}$, thereby implying Theorem \ref{thm:metric}.

\begin{proposition}
	\label{prop:rgm}
	Let $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ be two network spaces. If $(\cX, \mu, c_{\cX}) \cong (\cY, \nu, c_{\cY})$, then $\mathrm{RGM}(\mu, \nu) = 0$. The converse is true if there exists a continuous and strictly monotone function $h \colon \R_+ \to \R$ such that $c_{\cX} = h(d_{\cX})$ and $c_{\cY} = h(d_{\cY})$.
\end{proposition}


We conclude this section with a few more properties and examples. First, we give a sufficient condition for $(F, B) \in \cI(\mu, \nu)$ which can be useful in practice.
\begin{lemma}
	Let $(F, B) \in \cT(\mu, \nu) \times \cT(\nu, \mu)$. If $F \circ B = \mathrm{Id}$ or $B \circ F = \mathrm{Id}$ holds, then $(F, B) \in \cI(\mu, \nu)$.
\end{lemma}
\begin{proof}
	Without loss of generality, assume $B \circ F = \mathrm{Id}$. Then,
	\begin{equation*}
		(\mathrm{Id}, F)_{\#} \mu = (B \circ F, F)_{\#} \mu = (B, \mathrm{Id})_{\#} (F_{\#} \mu) = (B, \mathrm{Id})_{\#} \nu\;.
	\end{equation*}
	Hence, $(F, B) \in \cI(\mu, \nu)$.
\end{proof}

The following example illustrates that this condition can be used to find a pair $(F, B) \in \cI(\mu, \nu)$ when $\mu$ and $\nu$ are Gaussian distributions.
\begin{example}
	Given $p < q$, suppose $\mu = N(0, I_{p})$ and $\nu = N(0, \Sigma)$, where $I_{p} \in \R^{p \times p}$ is the identity matrix and $\Sigma \in \R^{q \times q}$ is of rank $p$. Then, we can find a rank-$p$ matrix $A \in \R^{q \times p}$ such that $\Sigma = A A^\top$. Let $F(x) = A x$ and $B(y) = A^\dagger y$, then one can easily check $F_{\#} \mu = \nu$, $B_{\#} \nu = \mu$, and $B \circ F = \mathrm{Id}$. Hence, $(F, B) \in \cI(\mu, \nu)$.
\end{example}

We conclude this section with a simple example that tells properly chosen cost functions give a strong isomorphism between two Gaussian distributions in general.

\begin{example}\label{ex:gauss}
	Consider two Gaussian distributions on $\R^d$, say $\mu = N(0, \Sigma_1)$ and $\nu = N(0, \Sigma_2)$. Assume $\Sigma_1$ and $\Sigma_2$ are invertible. Then two network spaces $(\R^d, \mu, c_{\cX})$ and $(\R^d, \nu, c_{\cY})$ are strongly isomorphic if $c_{\cX}$ and $c_{\cY}$ are Mahalanobis distances, that is, 
	\begin{equation*}
		c_{\cX}(x, x') = \sqrt{(x - x')^\top \Sigma_1^{- 1} (x - x')} \;, \quad c_{\cY}(y, y') = \sqrt{(y - y')^\top \Sigma_2^{- 1} (y - y')} \;.
	\end{equation*}
	To see this, let $T = \Sigma_2^{1 / 2} \Sigma_1^{- 1 / 2}$, where $\Sigma_1^{1 / 2}$ and $\Sigma_2^{1 / 2}$ are the square roots of $\Sigma_1$ and $\Sigma_2$, respectively. Obviously, a linear map $T$ satisfies $T \in \cT(\mu, \nu)$ and $c_{\cX}(x, x') = c_{\cY}(T x, T x')$ for all $x, x' \in \R^d$. According to Definition \ref{def:iso}, a linear map $T$ is a strong isomorphism. Proposition \ref{prop:rgm} implies $\mathrm{RGM}(\mu, \nu) = 0$. Notice that the same results hold for $c_{\cX}(x, x') = x^\top \Sigma_1^{-1} x'$ and $c_{\cY}(y, y') = y^\top \Sigma_2^{-1} y'$ as well.
\end{example}
	



\section{Statistical Theory}
\label{sec:statistical-theory}
This section serves to prove Theorem \ref{thm:stat}. Without loss of generality, we assume $\lambda_1 = \lambda_2 = \lambda_3 = 1$ in $C(\mu, \nu, F, B)$ since the proof is essentially identical with any constants $\lambda_i, 1\leq i \leq 3$. For convenience, we denote
\begin{align*}
	C_0(F, B) & = \int (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \dd{\mu \otimes \nu} \;, \\
	M(F, B) & = \mathrm{MMD}_{K_{\cY}}^2(F_{\#}\mu, \nu) + \mathrm{MMD}_{K_{\cX}}^2(\mu, B_{\#} \nu) + \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)
\end{align*}
and therefore $C(\mu, \nu, F, B) = C_0(F, B) + M(F, B)$. Similarly, define the empirical counterparts as
\begin{align*}
	\widehat{C}_0(F, B) & = \frac{1}{m n} \sum_{i = 1}^{m} \sum_{j = 1}^{n} (c_{\cX}(x_i, B(y_j)) - c_{\cY}(F(x_i), y_j))^2 \;, \\
	\widehat{M}(F, B) & = \mathrm{MMD}_{K_{\cY}}^2(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) + \mathrm{MMD}_{K_{\cX}}^2(\widehat{\mu}_m, B_{\#} \widehat{\nu}_n) + \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#} \widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n)
\end{align*}
and thus $C(\widehat{\mu}_m, \widehat{\nu}_n, F, B) = \widehat{C}_0(F, B) + \widehat{M}(F, B)$.

Our goal is to give an upper bound on $C(\mu, \nu, \widehat{F}, \widehat{B}) - \inf_{(F, B) \in \cF \times \cB} C(\mu, \nu, F, B) $. To this end, first recall that
\begin{equation*}
	C(\widehat{\mu}_m, \widehat{\nu}_n, \widehat{F}, \widehat{B}) \le C(\widehat{\mu}_m, \widehat{\nu}_n, F, B)
\end{equation*}
holds for any $F \in \cF$ and $B \in \cB$ by definition of $\widehat{F}$ and $\widehat{B}$ given in Theorem \ref{thm:stat}. Therefore,
\begin{equation*}
	C(\mu, \nu, \widehat{F}, \widehat{B}) - C(\mu, \nu, F, B)
	\le C(\mu, \nu, \widehat{F}, \widehat{B}) - C(\widehat{\mu}_m, \widehat{\nu}_n, \widehat{F}, \widehat{B}) + C(\widehat{\mu}_m, \widehat{\nu}_n, F, B) - C(\mu, \nu, F, B)\;.
\end{equation*}
The RHS can be decomposed as
\begin{equation*}
	C_0(\widehat{F}, \widehat{B}) - \widehat{C}_0(\widehat{F}, \widehat{B}) + M(\widehat{F}, \widehat{B}) - \widehat{M}(\widehat{F}, \widehat{B}) + \widehat{C}_0(F, B) - C_0(F, B) + \widehat{M}(F, B) - M(F, B)\;.
\end{equation*}
To further control the expression, we will first derive probabilistic bounds on $|\widehat{C}_0(F, B) - C_0(F, B)|$ and $|\widehat{M}(F, B) - M(F, B)|$ that hold for a fixed $(F, B) \in \cF \times \cB$ via standard concentration inequalities. Later, we will establish uniform probabilistic bounds on $\sup_{(F,B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)|$ and $\sup_{(F,B) \in \cF \times \cB} |\widehat{M}(F, B) - M(F, B)|$, using tools from empirical process theory.

\subsection{Concentration Inequalities}
We utilize the McDiarmid's inequality to derive bounds on $|\widehat{C}_0(F, B) - C_0(F, B)|$ and $|\widehat{M}(F, B) - M(F, B)|$. To give a bound on the former, we make the following boundedness assumption. 
% \begin{assumption}
% 	\label{a:bounded1}
% 	$(c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2$ is uniformly bounded over $\cF \times \cB$, that is, there exists a constant $H > 0$ such that
% 	\begin{equation*}
% 		\sup_{(F, B) \in \cF \times \cB} \sup_{(x, y) \in \cX \times \cY} (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2 \le H \;.
% 	\end{equation*}
% \end{assumption}

\begin{assumption}
	\label{a:bounded1}
	$c_{\cX}(\cdot, \cdot), c_{\cY}(\cdot, \cdot)$ is uniformly bounded, that is, there exists a constant $H > 0$ such that
	\begin{equation*}
		\sup_{(x, x') \in \cX \times \cX} c_{\cX}(x, x'), \sup_{(y, y') \in \cY \times \cY} c_{\cY}(y, y') \le \sqrt{\frac{H}{4}} \;.
	\end{equation*}
\end{assumption}


\begin{proposition}
	\label{prop:4}
	Under Assumption \ref{a:bounded1}, for any pair $(F, B) \in \cF \times \cB$ and $\delta > 0$, 
	\begin{equation*}
		|\widehat{C}_0(F, B) - C_0(F, B)|
		\precsim 
		\sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}}
	\end{equation*}
	holds with probability at least $1 - 4 \delta$.
\end{proposition}

To derive a similar bound on $|\widehat{M}(F, B) - M(F, B)|$, we assume that kernels are bounded.

\begin{assumption}
	\label{a:bounded_kernels}
	There exists $K > 0$ such that 
	\begin{equation*}
		\sup_{x \in \cX} |K_{\cX}(x, x)| \;, \; \sup_{y \in \cY} |K_{\cY}(y, y)| \le K \;.
	\end{equation*}
\end{assumption}

\begin{proposition}
	\label{prop:mmd_fixed}
	Under Assumption \ref{a:bounded_kernels}, for any pair $(F, B) \in \cF \times \cB$ and $\delta > 0$, 
	\begin{equation*}
		|\widehat{M}(F, B) - M(F, B)|
		\precsim \sqrt{\frac{\log(1 / \delta)}{m}} + \sqrt{\frac{\log(1 / \delta)}{n}}
	\end{equation*}
	holds with probability at least $1 - 6 \delta$.
\end{proposition}


\subsection{Uniform Deviations}
We now derive uniform deviation bounds for $\sup_{(F,B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)|$ and $\sup_{(F,B) \in \cF \times \cB} |\widehat{M}(F, B) - M(F, B)|$. For the former, we use the notion of uniform covering numbers defined below. 

\begin{definition}[Uniform Covering Number]
	Let $\cG$ be a collection of real-valued functions defined on a set $\cZ$. Given $m$ points $z_1, \ldots, z_m \in \cZ$ and any $\delta > 0$, we define $N_\infty(\delta, \cG, \{z_i\}_{i = 1}^{m})$ to be the $\delta$-covering number of $\cG$ under the pseudometric $d$ induced by points $z_1, \ldots, z_m$: 
	\begin{equation*}
		d(g, g') := \max_{i \in [m]} |g(z_i) - g'(z_i)| \;.
	\end{equation*}
	Also, we define the uniform $\delta$-covering number of $\cG$ as follows:
	\begin{equation*}
		N_\infty(\delta, \cG, m) := \sup\left\{N_\infty(\delta, \cG, \{z_i\}_{i = 1}^{m}) : z_1, \ldots, z_m \in \cZ\right\} \;.
	\end{equation*}
	Here, the supremum is taken over all possible combinations of $m$ points in $\cZ$.
\end{definition}

Also, we make the following assumptions.
\begin{assumption}
	\label{a:uniform_boundedness}
	$\cF_k$ and $\cB_\ell$ (see Definition \ref{a:transform_class}) consist of uniformly bounded functions, that is, there exists a constant $b > 0$ such that 
	\begin{equation*}
		\max_{k \in [\mathrm{dim}(\cY)]} \sup_{F_k \in \cF_k} \|F_k\|_\infty \;, \; \max_{\ell \in [\mathrm{dim}(\cX)]} \sup_{B_\ell \in \cB_\ell} \|B_\ell\|_\infty \le b \;.
	\end{equation*}
\end{assumption}

\begin{assumption}
	\label{a:lip_of_C}
	There exists a constant $L > 0$ such that 
	\begin{equation*}
		|c_{\cX}(x, x_1) - c_{\cX}(x, x_2)| \le L \|x_1 - x_2\| \;, \; |c_{\cY}(y_1, y) - c_{\cY}(y_2, y)| \le L \|y_1 - y_2\| \;.
	\end{equation*}
\end{assumption}
This Lipschitzness assumption ensures the smoothness of a map $(F, B) \mapsto |\widehat{C}_0(F, B) - C_0(F, B)|$ over $\cF \times \cB$, which allows us to utilize the uniform covering numbers.

\begin{proposition}
	\label{prop:union-bound-GW-term}
	Under Assumptions \ref{a:bounded1}, \ref{a:uniform_boundedness}, \ref{a:lip_of_C}, for any $\epsilon > 0$ and $\delta > 0$, 
	\begin{equation*}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)| \\
			\precsim
			& \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{m \wedge n}}
		\end{split}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$.
\end{proposition}

Now, the remaining task is to choose $\epsilon$ carefully in Proposition \ref{prop:union-bound-GW-term} for a concrete upper bound. To this end, we utilize the pseudo-dimension defined below.

\begin{definition}[Pseudo-Dimension]
	Let $\cG$ be a collection of real-valued functions defined on a set $\cZ$. Given a subset $S:=\{z_1, \ldots, z_m\} \subset \cZ$, we say $S$ is pseudo-shattered by $\cG$ if there are $r_1, \ldots, r_m \in \R$ such that for each $b \in \{0, 1\}^m$ we can find $g_b \in \cG$ satisfying $\mathrm{sign}(g_b(z_i) - r_i) = b_i$ for all $i \in [m]$. We define the pseudo-dimension of $\cG$, denoted as $\mathrm{Pdim}(\cG)$, as the maximum cardinality of a subset $S \subset \cZ$ that is pseudo-shattered by $\cG$.
\end{definition}

Using a well-established relation of the uniform covering number and the pseudo-dimension (Lemma \ref{lem:uniform_covering_to_pdim}), we can simplify Proposition \ref{prop:union-bound-GW-term} as follows.

\begin{corollary}
	\label{cor:union-bound-GW-term-pdim}
	Under Assumptions \ref{a:bounded1}, \ref{a:uniform_boundedness}, \ref{a:lip_of_C}, for any $\delta > 0$, 
	\begin{equation*}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)| \\
			\precsim
			& \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \sqrt{\frac{\log(m \vee n)}{m\wedge n} \left( \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) + \sum_{\ell = 1}^{{\rm dim}(\cX)} {\rm Pdim}(\cB_{\ell}) \right)}
		\end{split}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$.
\end{corollary}




To derive an upper bound on $\sup_{(F,B) \in \cF \times \cB} |\widehat{M}(F, B) - M(F, B)|$, we first introduce Rademacher complexities defined below.

\begin{definition}[Rademacher Complexity]
	Let $(\cZ, \rho)$ be a probability space and $\cG$ be a collection of measurable functions defined on $\cZ$. We define the Rademacher complexity of $\cG$ with respect to $m$ samples from $\rho$ as follows:
		\begin{equation*}
			R_m(\cG, \rho) =  \E_{z_i \overset{\mathrm{iid}}{\sim} \rho} \E_{\epsilon_i} \sup_{g \in \cG} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i g(z_i)\right|\;,
		\end{equation*}
		Here, $z_1, \ldots, z_m$ are i.i.d.\ samples from $\rho$ and $\epsilon_1, \ldots, \epsilon_m$ are i.i.d.\ Rademacher random variables such that $(z_1, \ldots, z_m)$ and $(\epsilon_1, \ldots, \epsilon_m)$ are independent.
\end{definition}

\begin{proposition}
	\label{prop:uniform-deviation-mmd-term}
	Denote a closed unit ball of any RKHS $\cH$ as $\cH(1)$. Also, let $(\mathrm{Id}, \cF) := \{(\mathrm{Id}, F) : F \in \cF\}$ and $(\cB, \mathrm{Id}) := \{(B, \mathrm{Id}) : B \in \cB\}$; hence, they are classes of maps from $\cX$ to $\cX \times \cY$ and from $\cY$ to $\cX \times \cY$, respectively. Under Assumption \ref{a:bounded_kernels}, for any $\delta > 0$,
	\begin{equation*}
		\begin{split}
			\sup_{(F,B) \in \cF \times \cB} |\widehat{M}(F, B) - M(F, B)|
			& \precsim \sqrt{\frac{\log(1 / \delta)}{m}} + \sqrt{\frac{\log(1 / \delta)}{n}} + R_m(\cH_{\cY}(1) \circ \cF, \mu) + R_n(\cH_{\cX}(1) \circ \cB, \nu) \\
			& \quad + R_m(\cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF), \mu) + R_n(\cH_{\cX \times \cY}(1) \circ (\cB, \mathrm{Id}), \nu)
		\end{split}
	\end{equation*}
	holds with probability at least $1 - 6 \delta$. Here, $\cF \circ \cG = \{f \circ g : f \in \cF, g \in \cG\}$ for any function classes $\cF$ and $\cG$ with matching input and output space.
\end{proposition}


Now, the only remaining task is to bound four Rademacher complexities. We will derive upper bounds using the chaining technique. To illustrate the main idea, let us consider $\cH_{\cY}(1) \circ \cF$. Recall that
\begin{equation*}
	R_m(\cH_{\cY}(1) \circ \cF, \mu) = \E_{x_i \overset{\mathrm{iid}}{\sim} \mu} R_m(\cH_{\cY}(1) \circ \cF, \{x_i\}_{i=1}^{m})\;,
\end{equation*}
where $R_m(\cH_{\cY}(1) \circ \cF, \{x_i\}_{i=1}^{m})$ is the empirical Rademacher complexity of $\cH_{\cY}(1) \circ F$ associated with $\{x_i\}_{i=1}^{m}$:
\begin{equation*}
	R_m(\cH_{\cY}(1) \circ \cF, \{x_i\}_{i=1}^{m}) = \E_{\epsilon_i} \sup_{h \in \cH_{\cY}(1), F \in \cF} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h(F(x_i))\right| = \E_{\epsilon_i} \sup_{h \in \cH_{\cY}(1), F \in \cF} \frac{1}{m} \sum_{i=1}^{m} \epsilon_i h(F(x_i))\;.
\end{equation*}
Notice that we may remove the absolute value since $\cH_{\cY}(1) = - \cH_{\cY}(1)$. Now, considering $\{x_i\}_{i=1}^{m}$ as fixed, we will first bound the empirical Rademacher complexity by replacing the Rademacher random variables with Gaussian random variables. Let $g_i$ be i.i.d.\ standard Gaussian random variables, then it is well known that
\begin{equation*}
	R_m(\cH_{\cY}(1)\circ \cF, \{x_i\}_{i=1}^m)
	\le \sqrt{\frac{\pi}{2}} \E_{g_i} \sup_{h \in \cH_{\cY}(1), F \in \cF} \frac{1}{m} \sum_{i=1}^m  g_i h(F(x_i)) =: \sqrt{\frac{\pi}{2}} \cG_m(\cH_{\cY}(1)\circ \cF, \{x_i\}_{i=1}^m)\;.
\end{equation*}
Also, under the assumption that $K_{\cY}$ is bounded by $K$, the reproducing property and the Cauchy-Schwarz inequality imply
\begin{equation*}
	\begin{split}
		\sup_{h \in \cH_{\cY}(1), F \in \cF} \sum_{i=1}^{m} g_i h(F(x_i))
		& = \sup_{h \in \cH_{\cY}(1), F \in \cF} \left\langle h, \sum_{i=1}^{m} g_i K_{\cY}(\cdot, F(x_i)) \right\rangle_{\cH_{\cY}} \\
		& \le \sup_{h \in \cH_{\cY}(1), F \in \cF} \|h\|_{\cH_{\cY}} \left[\sum_{i = 1}^{m} g_i^2 K_{\cY}( F(x_i), F(x_i)) + \sum_{i\neq j} g_i g_j K_{\cY}(F(x_i), F(x_j)) \right]^{1/2} \\
		& \le \sup_{F \in \cF} \left[\sum_{i = 1}^{m} g_i^2 K + \sum_{i\neq j} g_i g_j K_{\cY}(F(x_i), F(x_j)) \right]^{1/2} \\
		& \le \left[\sum_{i = 1}^{m} g_i^2 K + \sup_{F \in \cF} \sum_{i\neq j} g_i g_j K_{\cY}(F(x_i), F(x_j)) \right]^{1/2} \;.
	\end{split}
\end{equation*}
Here, $\langle \cdot, \cdot \rangle_{\cH_{\cY}}$ denotes the inner product on $\cH_{\cY}$. Hence, 
\begin{align*}
	\cG_m(\cH_{\cY}(1)\circ \cF, \{x_i\}_{i=1}^m)
	& \le \frac{1}{m} \E_{g_i} \left[\sum_{i = 1}^{m} g_i^2 K + \sup_{F \in \cF} \sum_{i\neq j} g_i g_j K_{\cY}(F(x_i), F(x_j)) \right]^{1/2} \\
	& \le \frac{1}{m} \left[ m K + \E_{g_i} \sup_{F \in \cF} \sum_{i\neq j} g_i g_j K_{\cY}(F(x_i), F(x_j)) \right]^{1/2} \;,
\end{align*}
where the second inequality follows from the Jensen's inequality and $\E g_i^2 = 1$.

For any $F \colon \cX \to \cY$, let $A_F \in \R^{m \times m}$ be a matrix whose diagonal elements are zero and $(i, j)$-th element is $K_{\cY}(F(x_i), F(x_j))$ for $i \neq j$. Then, the last term amounts to the supremum of a quadratic process
\begin{align*}
	\E_{g} \sup_{F \in \cF} g^\top A_F g \;,
\end{align*}
where $g := [g_1, \ldots, g_m]^\top \sim N(0, I_m)$.

We rely on the following chaining bound for the quadratic processes, derived in the appendix.
\begin{lemma}[Chaining Bound]
	\label{lem:chaining}
	% Let $\cT$ be a pseudometric space with the metric $d(t, t')$ and $G_t$ be random variables indexed by $t \in \cT$ such that for some constant $v=2, c=2$
	% \begin{align}
	% 	\log \E e^{\lambda(G_t - G_{t'})} \leq \frac{ \lambda^2 d^2(t, t')}{1 - 2 \lambda d(t, t')}
	% \end{align}
	% for all $t, t' \in \cT$ and $0<\lambda < \big(2d(t, t') \big)^{-1}$.
	Let $\mathbb{S}_0^{m \times m}$ be the collection of all symmetric matrices $A$ whose diagonal elements are zero. Endow $\mathbb{S}_0^{m\times m}$ with a metric $d$ given by $d(A, A') := \| A - A'\|$. Given $\cT \subset \mathbb{S}_0^{m \times m}$ and a fixed $A_0 \in \cT$, define $\Delta = \sup_{A \in \cT} d(A, A_0)$. Let $N(\delta, \cT)$ be the covering number of $\cT$ under the metric $d(\cdot, \cdot)$, then
	\begin{equation}
		\label{eqn:chain}
		\E_{g} \sup_{A \in \cT} g^\top A g 
		\leq \inf_{J \in \N} \left\{m \delta_J +  12 \int_{\delta_J / 2}^{\Delta/2} \sqrt{2 \log N(\delta, \cT)} \dd{\delta} + 24 \int_{\delta_J / 2}^{\Delta/2} \log N(\delta, \cT) \dd{\delta} \right\} \;,
	\end{equation}
	where for any integer $J \geq 0$, we define $\delta_J = 2^{-J} \Delta$.	
\end{lemma}

With the above chaining bound, we can directly upper bound the Rademacher complexities of the compositional classes such as $R_m(\cH_{\cY}(1)\circ \cF, \mu)$ and $R_m(\cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF), \mu)$. More specifically, for the former class, we will apply this chaining bound to $\cT := \{ A_F : F \in \cF \}$. Then, to further bound the RHS of \eqref{eqn:chain}, we make the following assumptions.

\begin{assumption}
	\label{a:lip_kernels}
	Suppose $K_{\cX}$ and $K_{\cY}$ are Lipschitz: there exists $L > 0$ such that
	\begin{align*}
		|K_{\cX}(x_1, x') - K_{\cX}(x_2, x')| \le L \|x_1 - x_2\| \;, 
		\quad |K_{\cY}(y_1, y') - K_{\cY}(y_2, y')| \le L \|y_1 - y_2\| \;.
	\end{align*}
\end{assumption}
This plays a similar role as Assumption \ref{a:lip_of_C}: we can derive an upper bound on $d(A_F, A_{F'})$ via closeness of $F$ and $F'$ in $\cF$. As a result, we will see that the covering number $N(\delta, \cT)$ can be bounded by the complexity of $\cF$. 


\begin{assumption}
	\label{a:separation}
	There exist $y_0$ and $y_0'$ in $\cY$ with $K_{\cY}(y_0, y_0') \neq K_{\cY}(y_0, y_0)$ such that 
	\begin{itemize}
		\item $\cF$ contains a constant map $F$ satisfying $F(x) = y_0$ for all $x \in \cX$, 
		\item whenever we have $x \neq x' \in \cX$, we can find a non-constant map $F \in \cF$ such that $F(x) = y_0$ and $F(x') = y_0'$. 
	\end{itemize}
	Similarly, there exist $x_0$ and $x_0'$ in $\cX$ with $K_{\cX}(x_0, x_0') \neq K_{\cX}(x_0, x_0)$ such that 
	\begin{itemize}
		\item $\cB$ contains a constant map $B$ such that $B(y) = x_0$ for all $y \in \cY$, 
		\item whenever we have $y \neq y' \in \cY$, we can find a non-constant map $B \in \cB$ such that $B(y) = x_0$ and $B(y') = x_0'$. 
	\end{itemize}
\end{assumption}
The main purpose of this assumption is to exclude overly restrictive $\cF$ and $\cB$, and is minimal: $\cF$ and $\cB$ should contain constant maps, as well as non-constant maps. With these assumptions, we can derive the following result.

\begin{proposition}\label{prop:Rademacher-upper-bound-by-chaining}
	Under Assumptions \ref{a:bounded_kernels}, \ref{a:uniform_boundedness}, \ref{a:lip_kernels}, \ref{a:separation},
	\begin{align*}
		R_m(\cH_{\cY}(1)\circ \cF, \mu)\;, \; R_m(\cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF), \mu) \precsim \sqrt{\frac{\log m}{m} \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k})}\;, \\
		R_n(\cH_{\cX}(1)\circ \cB, \mu)\;, \; R_n(\cH_{\cX \times \cY}(1) \circ (\cB, \mathrm{Id}), \nu) \precsim \sqrt{\frac{\log n}{n} \sum_{k =1}^{{\rm dim}(\cX)} {\rm Pdim}(\cB_{k})}\;.
	\end{align*}
\end{proposition}

In summary, Propositions \ref{prop:4}, \ref{prop:mmd_fixed}, \ref{prop:uniform-deviation-mmd-term}, \ref{prop:Rademacher-upper-bound-by-chaining} and Corollary \ref{cor:union-bound-GW-term-pdim} directly imply Theorem \ref{thm:stat}.



\section{Representer Theorem and Convex Formulation}
\label{sec:representation}

This section provides details of the results presented in Section \ref{subsec:cvx-representer}. Again, without loss of generality, we only consider $\lambda_1 = \lambda_2 = \lambda_3 = 1$ in \eqref{eqn:1}.

First, we clarify how measurable maps correspond to bounded linear operators between $L^2$ spaces.
\begin{proposition}
	Let $F \colon \cX \to \cY$ be a measurable map such that $\|\dd{F_\# \pi_\cX} / \dd{\pi_\cY}\|_\infty < \infty$. If we define
	\begin{equation*}
		\bF(g) = g \circ F
	\end{equation*}
	for all $g \in L^2_{\cY}$, then $\bF \colon L^2_{\cY} \to L^2_{\cX}$ is a bounded linear operator. Similarly, a measurable map $B \colon \cY \to \cX$ satisfying $\|\dd{B_\# \pi_\cY} / \dd{\pi_\cX}\|_\infty < \infty$ induces a bounded linear operator $\bB \colon L^2_{\cX} \to L^2_{\cY}$ such that $\bB(g) = g \circ B$ for all $g \in L^2_{\cX}$.
\end{proposition}
\begin{proof}[Proof]
	Linearity of $\bF$ is obvious. Since
	\begin{equation*}
		\int_{\cX} g(F(x))^2 \dd{\pi_{\cX}} = \int_{\cY} g(y)^2 \dd{F_{\#} \pi_{\cX}}(y) = \int_{\cY} g(y)^2 \frac{\dd{F_{\#} \pi_\cX}}{\dd{\pi_\cY}}(y) \dd{\pi_{\cY}}(y) \le \|g\|_{L^2(\pi_{\cY})}^2 \left\|\frac{\dd{F_{\#} \pi_\cX}}{\dd{\pi_\cY}} \right\|_\infty\;,
	\end{equation*}
	we can see $\bF(g) = g \circ F \in L^2_{\cX}$ and thus $\bF \colon L^2_{\cY} \to L^2_{\cX}$. From this inequality, the operator norm of $\bF$ is bounded by $\|\dd{F_\# \pi_\cX} / \dd{\pi_\cY}\|_\infty^{1 / 2}$; hence boundedness of $\bF$ follows. The same argument applies to $\bB$.
\end{proof}

Next, we prove that \eqref{eqn:1} can be written in terms of $\bF$ and $\bB$ if $K_{\cX}$ and $K_{\cY}$ are given by the Mercer's representation:
\begin{align}
	\label{eqn:k_x}
	K_{\cX}(x, x') = \sum_{k = 1}^{\infty} \lambda_k \phi_k(x) \phi_k(x')\;, \\
	\label{eqn:k_y}
	K_{\cY}(y, y') = \sum_{\ell = 1}^{\infty} \gamma_\ell \psi_\ell(y) \psi_\ell(y')\;.
\end{align}
Let $\Phi_x = [\cdots, \phi_k(x), \cdots ]^\top \in \R^\infty$ and $\Psi_y = [\cdots, \psi_\ell(y), \cdots]^\top \in \R^\infty$. Then, $K_{\cX}(x, x') = \Phi_{x}^\top \Lambda \Phi_{x'}$ and $K_{\cY}(y, y') = \Psi_{y}^\top \Gamma \Psi_{y'}$. Also, 
\begin{align*}
	K_{\cX}(x, B(y)) &= \sum_{k} \lambda_k \phi_k(x) [\phi_k \circ B] (y) \\
	& =\sum_{k} \lambda_k \phi_k(x) \bB[\phi_k] (y) \\
	& = \sum_{k, \ell} \lambda_k \phi_k(x) \bB_{\ell k} \psi_\ell(y) \\
	& = \Psi_{y}^\top \bB \Lambda \Phi_{x} \;.
\end{align*}
Analogously, we can obtain 
\begin{align*}
	& K_{\cY}(F(x), y) = \Phi_{x}^\top \bF \Gamma \Psi_{y} \;, \\
	& K_{\cX}(B(y), B(y')) = \Psi_{y}^\top \bB \Lambda \bB^\top \Psi_{y'} \;, \\
	& K_{\cY}(F(x), F(x')) = \Phi_{x}^\top \bF \Gamma \bF^\top \Phi_{x'} \;. \\
\end{align*}

Using this, we have
\begin{equation}
	\label{eqn:rgm}
	\frac{1}{m n} \sum_{i = 1}^{m} \sum_{j = 1}^{n} (K_{\cX}(x_i, B(y_j)) - K_{\cY}(F(x_i), y_j))^2 = \frac{1}{mn} \sum_{i, j} (\Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} - \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j})^2.
\end{equation}
Also, 
\begin{equation} 
	\label{eqn:mmd-x}
	\begin{split}
		\mathrm{MMD}_{K_{\cX}}^2(\widehat{\mu}_m, B_{\#} \widehat{\nu}_n) 
		& = 
		\frac{1}{m^2} \sum_{i, i'} K_{\cX}(x_i, x_{i'})	+ \frac{1}{n^2} \sum_{j, j'} K_{\cX}(B(y_j), B(y_{j'}))	- \frac{2}{mn} \sum_{i, j} K_{\cX}(x_i, B(y_j)) \\
		& = \frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \Lambda \Phi_{x_i'} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \bB \Lambda \bB^\top \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} \;.
	\end{split}
\end{equation}
Similarly, we have
\begin{equation}
	\label{eqn:mmd-y}
	\mathrm{MMD}_{K_{\cY}}^2(F_{\#} \widehat{\mu}_m, \widehat{\nu}_n) 
	= 
	\frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \bF \Gamma \bF^\top \Phi_{x_{i'}} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \Gamma \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j} \;
\end{equation}
and 
\begin{equation}
	\label{eqn:mmd-xy}
	\begin{split}
		& \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}}^2((\mathrm{Id}, F)_{\#} \widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) \\
		& = \frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \Lambda \Phi_{x_i'} \Phi_{x_i}^\top \bF \Gamma \bF^\top \Phi_{x_{i'}} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \Gamma \Psi_{y_{j'}} \Psi_{y_j}^\top \bB \Lambda \bB^\top \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j} \;.
	\end{split}
\end{equation}

The following proposition summarizes the discussion so far. 
\begin{proposition}
	\label{prop:representer}
	Given Borel measures $\pi_{\cX}$ and $\pi_{\cY}$ over $\cX$ and $\cY$, respectively, suppose their corresponding $L^2$ spaces $L^2_{\cX}$ and $L^2_{\cY}$ have countable orthonormal bases: $\{\phi_k\}_{k \in \N}$ and $\{\psi_\ell\}_{\ell \in \N}$. Also, assume $K_{\cX}$ and $K_{\cY}$ are given by the Mercer's representation \eqref{eqn:k_x} and \eqref{eqn:k_y}. Let $\cF_o$ and $\cB_o$ be collections of all $F \colon \cX \to \cY$ and $B \colon \cY \to \cX$ such that $\|\dd{F_\# \pi_\cX} / \dd{\pi_\cY}\|_\infty < \infty$ and $\|\dd{B_\# \pi_\cY} / \dd{\pi_\cX}\|_\infty < \infty$, respectively. Then, solving \eqref{eqn:1} over $\cF_o \times \cB_o$ is equivalent to \eqref{eqn:op}, where $\cC$ denotes the collection of all pairs of matrices $(\bF, \bB)$ that correspond to a pair of bounded linear operators induced by $(F, B) \in \cF_o \times \cB_o$. Also, $\Omega$ is defined as 	
	\begin{equation*}
		\begin{split}
			\Omega(\bF, \bB)
			&:=
			\frac{1}{mn} \sum_{i, j} (\Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} - \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j})^2 \\
			&+ 
			\frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \Lambda \Phi_{x_i'} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \bB \Lambda \bB^\top \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} \\
			&+ 
			\frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \bF \Gamma \bF^\top \Phi_{x_{i'}} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \Gamma \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j} \\
			&+ 
			\frac{1}{m^2} \sum_{i, i'} \Phi_{x_i}^\top \Lambda \Phi_{x_i'} \Phi_{x_i}^\top \bF \Gamma \bF^\top \Phi_{x_{i'}} + \frac{1}{n^2} \sum_{j, j'} \Psi_{y_j}^\top \Gamma \Psi_{y_{j'}} \Psi_{y_j}^\top \bB \Lambda \bB^\top \Psi_{y_{j'}} - \frac{2}{m n} \sum_{i, j} \Psi_{y_j}^\top \bB \Lambda \Phi_{x_i} \Phi_{x_i}^\top \bF \Gamma \Psi_{y_j} \;.
		\end{split}
	\end{equation*}
\end{proposition}
Based on this, we now prove Theorem \ref{thm:representer}.

\begin{proof}[Proof of Theorem \ref{thm:representer}]
	Let $\bx_i = \Lambda^{1 / 2} \Phi_{x_i}$ and $\by_j = \Gamma^{1 / 2} \Psi_{y_j}$. Notice that we can view them as elements of a Hilbert space $\ell^2_{\N}$, that is, the space of square-summable sequences: 
	\begin{equation*}
		\ell^2_{\N} = \{ (a_k)_{k \in \N} : \sum_{k} a_k^2 < \infty \} \;.
	\end{equation*}
	Also, define $\bar{\bF} = \Lambda^{- 1 / 2} \bF \Gamma^{1 / 2} $ and $\bar{\bB} = \Gamma^{- 1 / 2} \bB \Lambda^{1 / 2}$ where $\Lambda, \Gamma \succ 0$. By rewriting \eqref{eqn:rgm}-\eqref{eqn:mmd-xy} using $\bx_i$, $\by_j$, $\bar{\bF}$, and $\bar{\bB}$, we have
	\begin{align*}
		\Omega(\bF, \bB)
		& = \frac{1}{mn} \sum_{i, j} (\by_j^\top \bar{\bB} \bx_i - \bx_i^\top \bar{\bF} \by_j)^2 \tag{i} \\
		& + \frac{1}{m^2} \sum_{i, i'} \bx_i^\top \bx_{i'} + \frac{1}{n^2} \sum_{j, j'} \by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'} - \frac{2}{m n} \sum_{i, j} \by_j^\top \bar{\bB} \bx_i \tag{ii} \\
		& + \frac{1}{m^2} \sum_{i, i'} \bx_i^\top \bar{\bF} \bar{\bF}^\top \bx_{i'} + \frac{1}{n^2} \sum_{j, j'} \by_j^\top \by_{j'} - \frac{2}{m n} \sum_{i, j} \bx_i^\top \bar{\bF} \by_j \tag{iii} \\
		& + \frac{1}{m^2} \sum_{i, i'} (\bx_i^\top \bx_{i'}) (\bx_i^\top \bar{\bF} \bar{\bF}^\top \bx_{i'}) + \frac{1}{n^2} \sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'}) - \frac{2}{m n} \sum_{i, j} (\by_j^\top \bar{\bB} \bx_i) (\bx_i^\top \bar{\bF} \by_j) \tag{iv} \\
		& =: \bar{\Omega}(\bar{\bF}, \bar{\bB}) \;.
	\end{align*}
	As a result, \eqref{eqn:relaxed} reduces to $\min_{\bar{\bF}, \bar{\bB} \in \R^{\infty \times \infty}} \bar{\Omega}(\bar{\bF}, \bar{\bB})$.	Now, we define two finite-dimensional subspaces of $\ell^2_{\N}$ spanned by $(\bx_1, \ldots, \bx_m)$ and $(\by_1, \ldots, \by_n)$, respectively: 
	\begin{equation*}
		U_m := \mathrm{span}\{\bx_1, \ldots, \bx_m\} \;, \quad 
		V_n := \mathrm{span}\{\by_1, \ldots, \by_n\} \;.
	\end{equation*}
	Also, we define $P_{U_m}$ and $P_{V_n}$ to be matrices that correspond to the orthogonal projection operators from $\ell^2_{\N}$ to $U_m$ and to $V_n$, respectively. Recall that $P_{U_m}$ and $P_{V_n}$ are symmetric and idempotent by definition.

	Our goal is to prove
	\begin{equation*}
		\bar{\Omega}(\bar{\bF}, \bar{\bB})
		\ge
		\bar{\Omega}(P_{U_m} \bar{\bF} P_{V_n}, P_{V_n} \bar{\bB} P_{U_m}).
	\end{equation*}
	More precisely, we show that four terms (i)-(iv) decrease if we replace $\bar{\bF}$ and $\bar{\bB}$ with $P_{U_m} \bar{\bF} P_{V_n}$ and $P_{V_n} \bar{\bB} P_{U_m}$, respectively. First, observe that (i) remains the same. By definition, $P_{U_m} \bx_i = \bx_i$ and $P_{V_n} \by_j = \by_j$, thus $\by_j^\top \bar{\bB} \bx_i = \by_j^\top P_{V_n} \bar{\bB} P_{U_m} \bx_i$ and $\bx_i^\top \bar{\bF} \by_j = \bx_i^\top P_{U_m} \bar{\bF} P_{V_n} \by_j$. Hence, (i) does not change.

	To prove that (ii) decrease, it suffices to prove
	\begin{equation*}
		\sum_{j, j'} \by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'} 
		\ge
		\sum_{j, j'} \by_{j}^\top (P_{V_n} \bar{\bB} P_{U_m}) (P_{V_n} \bar{\bB} P_{U_m})^\top \by_{j'}
		=
		\sum_{j, j'} \by_{j}^\top \bar{\bB} P_{U_m} \bar{\bB}^\top \by_{j'} \;.
	\end{equation*}
	To this end, define $P_{U_m}^\perp$ to be a matrix that corresponds to the orthogonal projection from $\ell^2_{\N}$ to $U_m^\perp$, the orthogonal complement of $U_m$. By definition, $P_{U_m} + P_{U_m}^\perp$ is the identity matrix and $P_{U_m} P_{U_m}^\perp = 0$. Hence,
	\begin{equation*}
		\sum_{j, j'} \by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'} 
		=
		\left\|\bar{\bB}^\top \sum_{j} \by_{j}\right\|^2
		=
		\left\|P_{U_m} \bar{\bB}^\top \sum_{j} \by_{j}\right\|^2 + \left\|P_{U_m}^\perp \bar{\bB}^\top \sum_{j} \by_{j}\right\|^2
		\ge
		\sum_{j, j'} \by_{j}^\top \bar{\bB} P_{U_m} \bar{\bB}^\top \by_{j'}
	\end{equation*}
	Here, the second equality is the Pythagorean theorem. Therefore, we can see (ii) decreases if we replace $\bar{\bB}$ with $P_{V_n} \bar{\bB} P_{U_m}$. Similarly, (iii) decreases.

	For (iv), it suffices to prove
	\begin{equation*}
		\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'}) 
		\ge
		\sum_{j, j'} (\by_j^\top \by_{j'}) \left(\by_{j}^\top (P_{V_n} \bar{\bB} P_{U_m}) (P_{V_n} \bar{\bB} P_{U_m})^\top \by_{j'}\right)
		=
		\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} P_{U_m} \bar{\bB}^\top \by_{j'}) \;.
	\end{equation*}
	To see this, 
	\begin{equation*}
		\begin{split}
			\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} \bar{\bB}^\top \by_{j'}) 
			& =
			\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} P_{U_m} \bar{\bB}^\top \by_{j'}) + 
			\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} P_{U_m}^\perp \bar{\bB}^\top \by_{j'}) \\
			& \ge \sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} P_{U_m} \bar{\bB}^\top \by_{j'}) \;,
		\end{split}
	\end{equation*}
	where the inequality holds since 
	\begin{equation*}
		\sum_{j, j'} (\by_j^\top \by_{j'}) (\by_{j}^\top \bar{\bB} P_{U_m}^\perp \bar{\bB}^\top \by_{j'}) = \mathrm{Tr} \left[\left(\sum_{j} P_{U_m}^\perp \bar{\bB}^\top \by_j \by_j^\top\right) \left(\sum_{j} P_{U_m}^\perp \bar{\bB}^\top \by_j \by_j^\top\right)^\top \right] \ge 0 \;.
	\end{equation*}
	Similarly, we can obtain
	\begin{equation*}
		\sum_{i, i'} (\bx_i^\top \bx_{i'}) (\bx_i^\top \bar{\bF} \bar{\bF}^\top \bx_{i'})
		\ge 
		\sum_{i, i'} (\bx_i^\top \bx_{i'}) (\bx_i^\top \bar{\bF} P_{V_n} \bar{\bF}^\top \bx_{i'}).
	\end{equation*}
	Hence, (iv) decreases.	
	
	Consequently, we have
	\begin{equation*}
		\eqref{eqn:relaxed}
		=
		\min_{\bar{\bF}, \bar{\bB} \in \R^{\infty \times \infty}} \bar{\Omega}(\bar{\bF}, \bar{\bB})
		=
		\min_{\bar{\bF}, \bar{\bB} \in \R^{\infty \times \infty}} \bar{\Omega}(P_{U_m} \bar{\bF} P_{V_n}, P_{V_n} \bar{\bB} P_{U_m}) \;.
	\end{equation*}
	By definition of a projection operator, we can find $\mathsf{U}_m \in \R^{m \times \infty}$ and $\mathsf{V}_n \in \R^{n \times \infty}$ such that 
	\begin{equation*}
		P_{U_m} = \Lambda^{1 / 2} \Phi_m \mathsf{U}_m \;, \quad 
		P_{V_n} = \Gamma^{1 / 2} \Psi_n \mathsf{V}_n.
	\end{equation*}
	By letting $\mathsf{U}_m \bar{\bF} \mathsf{V}_n^\top = \mathsf{F}_{m, n} \in \R^{m \times n}$ and $\mathsf{V}_n \bar{\bB} \mathsf{U}_m^\top = \mathsf{B}_{n, m} \in \R^{n \times m}$, we have
	\begin{align*}
		P_{U_m} \bar{\bF} P_{V_n} = \Lambda^{1/2} \Phi_m \mathsf{F}_{m,n} \Psi_n^\top \Gamma^{1/2} \;, \\
		P_{V_n} \bar{\bB} P_{U_m} = \Gamma^{1/2} \Psi_n \mathsf{B}_{n,m} \Phi_m^\top \Lambda^{1/2} \;.
	\end{align*}
	Hence, 
	\begin{equation*}
		\min_{\bar{\bF}, \bar{\bB} \in \R^{\infty \times \infty}} \bar{\Omega}(P_{U_m} \bar{\bF} P_{V_n}, P_{V_n} \bar{\bB} P_{U_m}) 
		=
		\min_{(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \in \mathsf{C}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \;,
	\end{equation*}
	where	
	\begin{equation*}
		\omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m})
		:= \bar{\Omega}(\Lambda^{1 / 2} \Phi_m \mathsf{F}_{m, n} \Psi_n^\top \Gamma^{1 / 2}, \Gamma^{1 / 2} \Psi_n \mathsf{B}_{n, m} \Phi_m^\top \Lambda^{1 / 2}) \;.
	\end{equation*} 
	Here, $\mathsf{C}$ is a constraint set implying that $\mathsf{F}_{m, n}$ and $\mathsf{B}_{n, m}$ are associated with $\bar{\bF}$ and $\bar{\bB}$, respectively, namely,
	\begin{equation*}
		\mathsf{C} = \{(\mathsf{U}_m \bar{\bF} \mathsf{V}_n^\top,  \mathsf{V}_n \bar{\bB} \mathsf{U}_m^\top) : \bar{\bF}, \bar{\bB} \in \R^{\infty \times \infty}\} \subset \R^{m \times n} \times \R^{n \times m} \;.
	\end{equation*}
	Therefore,
	\begin{equation*}
		\eqref{eqn:relaxed}
		=
		\min_{(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \in \mathsf{C}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m})
		\ge
		\min_{\substack{\mathsf{F}_{m, n} \in \R^{m \times n} \\ \mathsf{B}_{n, m} \in \R^{n \times m}}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \;.
	\end{equation*}
	Finally, note that $\mathsf{C} = \R^{m \times n} \times \R^{n \times m}$ if $\mathsf{U}_m$ and $\mathsf{V}_n$ are full rank, that is, row spaces of $\mathsf{U}_m$ and $\mathsf{V}_n$ are rank-$m$ and rank-$n$, respectively. This is true if kernel matrices 
	\begin{equation*}
		\bK_{\cX} = (\Lambda^{1 / 2} \Phi_m)^\top (\Lambda^{1 / 2} \Phi_m) \;, \; \bK_{\cY} = (\Gamma^{1 / 2} \Psi_n)^\top (\Gamma^{1 / 2} \Psi_n)
	\end{equation*}
	are invertible. This is equivalent to say that they are positive definite. In this case, 
	\begin{equation*}
		\mathsf{U}_m = \bK_{\cX}^{- 1} (\Lambda^{1 / 2} \Phi_m)^\top \;, \;
		\mathsf{V}_n = \bK_{\cY}^{- 1} (\Gamma^{1 / 2} \Psi_n)^\top,
	\end{equation*}
	which are indeed full rank. Accordingly, we have 
	\begin{equation*}
		\eqref{eqn:relaxed}
		=
		\min_{(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \in \mathsf{C}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m})
		=
		\min_{\substack{\mathsf{F}_{m, n} \in \R^{m \times n} \\ \mathsf{B}_{n, m} \in \R^{n \times m}}} \omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m}) \;.
	\end{equation*}
	Finally, we prove $\omega$ is convex. To see this, verify
	\begin{align*}
		\omega(\mathsf{F}_{m, n}, \mathsf{B}_{n, m})
		& = \frac{1}{mn} \| \bK_{\cY} \mathsf{B}_{n, m} \bK_{\cX} - \bK_{\cY} \mathsf{F}_{m, n}^\top \bK_{\cX} \|^2 \\
		& + \left\|\bK_{\cX}^{1/2} \cdot \left(\frac{1}{m} \mathbf{1}_m - \mathsf{B}_{n, m}^\top \bK_{\cY} \frac{1}{n} \mathbf{1}_n \right) \right\|^2
		+ \left\|\bK_{\cY}^{1/2} \cdot \left(\frac{1}{n} \mathbf{1}_n - \mathsf{F}_{m, n}^\top \bK_{\cX} \frac{1}{m} \mathbf{1}_m \right) \right\|^2 \\
		& + \left\| \frac{1}{m} \bK_{\cX}^{3/2} \mathsf{F}_{m,n} \bK_{\cY}^{1/2} - \frac{1}{n} \bK_{\cX}^{1/2} \mathsf{B}_{n,m}^\top \bK_{\cY}^{3/2} \right\|^2 \;,
	\end{align*}
	where $\bK_{\cX}^{1 / 2}$ and $\bK_{\cY}^{1 / 2}$ are the square root matrices of $\bK_{\cX}$ and $\bK_{\cY}$, respectively, and $\mathbf{1}_m \in \R^m$ and $\mathbf{1}_n \in \R^n$ are all-ones vectors. 
\end{proof}



\section{Numerical Examples}
\label{sec:numerical}


This section investigates numerical examples, one synthetic and one real-world,  to showcase the effectiveness of our reversible Gromov-Monge sampler. The synthetic example is of a sanity check nature to see that RGM can effectively learn simple parametric distributions, whereas the real-world example is to generate high fidelity images that are drawn from the underlying probability distribution supported on the MNIST image manifold.

To implement our method, one needs to specify $c_\cX$, $c_\cY$, $K_\cX$, $K_\cY$ according to the nature of the data set.
In practice, scaling $c_\cX, c_\cY, K_\cX, K_\cY$ similarly leads to lower empirical loss and more accurate samplers; hence proper tuning for cost functions and kernels is crucial.
% To see this, suppose $(\cX, \mu, c_\cX) = (\cY, \nu, c_\cY)$ and two data sets are identical, i.e., $n = m$ and $y_i = x_i$, $i = 1, \dots, m$. Then GM term in \eqref{eqn:1} is clearly zero by choosing $F$ and $B$ as identity maps.
% On the contrary, if we choose $c_\cY = c\cdot c_\cX$ for some positive constant $c$, GM term becomes
% \[
% \frac{1}{m^2} \sum_{i,j = 1}^{m} (1 - c)^2 c_{\cX}(x_i, x_j)^2
% \]
% for $F$, $B$ identity maps; its value is not necessarily zero and identity maps are not necessarily a pair of minimizers. \wg{explain the importance of scaling by this extreme case}
%
Here we offer some concrete suggestions on tuning cost functions and kernels: for cost functions $c_\cX$ and $c_\cY$,
if we know how to guarantee strong isomorphisms as in Gaussian example, we may simply choose the functions that ensure the existence of isomorphisms; otherwise we may choose first-stage cost functions as $c_{\cX, 0} = d_\cX^2/p$ and $c_{\cY, 0} = d_\cY^2/q$ ($p$, $q$ are extrinsic dimensions of $\cX$, $\cY$) or some scaled kernels as in MNIST example, and then standardize them by matching medians and standard errors. In other words, in the second case one can choose $c_\cX = (c_{\cX, 0}-m_\cX)/\text{sd}_{\cX}$, 
$c_\cY = (c_{\cY, 0}-m_\cY)/\text{sd}_{\cY}$, where $m_\cX$, $\text{sd}_\cX$ are median and standard error of $\{c_{\cX, 0}(x_i, x_j)\}_{i, j = 1}^{m}$, and $m_\cY$, $\text{sd}_\cY$ are defined analogously on $\cY$.
For $K_\cX$ and $K_\cY$, we suggest
kernels that are characteristic, so to better enforce the equality between $(\mathrm{Id}, F)_{\#}\mu$ and $(B, \mathrm{Id})_{\#} \nu$.



Throughout the experiments, we highlight two main features of our formulation: first, an exemption from complicated tuning that is usually unavoidable for deep generative models; second, an approximate isomorphism  $\widehat{F}$ that facilitates transform sampling. Let's clarify the term \textit{approximate isomorphism}, which originates from Definition~\ref{def:iso}.
We say ${F}: (\cX, \mu, c_\cX) \to (\cY, \nu, c_\cY)$ is an approximate isomorphism if it
satisfies ${F}_\#\mu\approx \nu$ and
$c_\cX(x, x^\prime)\approx c_\cY({F}(x), {F}(x^\prime))$ for $x$, $x^\prime\in\cX$. 
Note that how close the approximation in metrics is depends on the numerical value of the GM term in our examples.
In both two examples below, we demonstrate that $\widehat{F}$, $\widehat{B}$ are approximate isomorphisms. 



\subsection{2D Gaussian}

We first check our method on a synthetic Gaussian data set, for which a strong isomorphism can be guaranteed by our specification. Suppose
the target distribution is $\nu = N(0, \Sigma)$ on $\cY = \mathbb{R}^2$, where $\Sigma$ is a full-rank covariance matrix ($\Sigma = [1.0, 0.7;0.7, 1.0]$).
We select
$\cX = \mathbb{R}^2$, $\mu = N(0, I_2)$,
$c_\cX(x, x^\prime) = \langle x, x^\prime\rangle$,
$c_\cY(y, y^\prime) =  \langle y, \Sigma^{-1} y^\prime\rangle$
where $\langle\cdot, \cdot\rangle$ denotes the Euclidean inner product. 
Under this setting, two network spaces are strongly isomorphic due to Example~\ref{ex:gauss}. 
We further specify $K_\cX = K_\cY = K_2$
where $K_2(x, y) = (\langle x, y\rangle + 1)^2$, a degree-2 polynomial kernel on $\mathbb{R}^2$, to ensure
$(\mathrm{Id}, F)_{\#}\mu \approx (B, \mathrm{Id})_{\#} \nu$ in the
empirical problem \eqref{eqn:1}. 
Now we expect that our model learns 
strongly isomorphic maps, e.g., ${F}(x) = \Sigma^{1/2} Qx$ and ${B}(y) = Q^{\top}\Sigma^{-1/2}y$ up to the orthogonal group $Q \in O(2)$.

\begin{figure}[ht]
    \centering
    \subfloat[\centering  ]{{\includegraphics[width=7cm]{img/gauss.pdf} }}%
    \qquad
    \subfloat[\centering ]{{\includegraphics[width=7cm]{img/gauss-loss.pdf} }}%
    \caption{Generated samples and training loss on 2D Gaussian data. Subfigure (a) compares 150 samples generated by applying $\widehat{F}$ on i.i.d.\ new samples from $\mu$ (our reversible Gromov-Monge sampling approach) and 150 new samples from the target $\nu$, and subfigure (b) shows the logarithmic loss for each component in the empirical loss \eqref{eqn:1} indexed by (gradient descent) iterations. }
    \label{fig:gauss}
\end{figure}

We set sample size $m = n = 1000$, tuning parameters $\lambda_1 = \lambda_2=\lambda_3 = 1$,  and use the gradient descent (GD) approach as mentioned in Section~\ref{sec:summary-of-results}. To be specific, we restrict $F$ and $B$ to be linear transformations, a function class rich enough to contain strong isomorphisms, and run gradient descent algorithm for $3000$ iterations. 
The learning rate at initial iteration is $0.05$, and will halve after every $500$ iterations. 
Figure~\ref{fig:gauss}(a) compares $150$ new samples from $\widehat{F}_\# \mu$ with $150$ new samples from $\nu$, which confirms that our model learns the distribution well.
The value of each component in \eqref{eqn:1}, which will be referred to as GM, $\mathrm{MMD}_{K_{\cX} \otimes K_{\cY}}^2$, $\mathrm{MMD}_{K_{\cX}}^2$ , and $\mathrm{MMD}_{K_{\cY}}^2$ in the order, are shown in Figure~\ref{fig:gauss}(b) on a logarithmic scale against the number of GD iterations.
For the estimated linear transformations $(\widehat{F}, \widehat{B})$, we have
\[
\widehat{F}\widehat{F}^\top \approx \begin{pmatrix}
    1.0202 & 0.6968 \\
    0.6968 & 0.9669	
\end{pmatrix}\;,
\qquad \widehat{B}\Sigma \widehat{B}^\top \approx
\begin{pmatrix}
	0.9615 & -0.0044 \\
	-0.0044 & 1.0776
\end{pmatrix} \;.
\]
Hence $\widehat{F}\widehat{F}^\top\approx \Sigma$, $\widehat{B}\Sigma \widehat{B}^\top \approx I_2$, implying that our model indeed approximately captures strong isomorphisms.


\subsection{MNIST}

Now we apply our method to generate new MNIST images (images unseen in the data set), whose distribution might be a high dimensional distribution confined to a low dimensional image manifold.
For simplicity, we focus on $4$ digits ($2, 4, 6, 7$) and choose $\nu$ to be the corresponding MNIST distribution supported on some manifold $\cY \subset \mathbb{R}^{784}$.
Since we lack additional knowledge on the existence of strong isomorphisms in this example,
we simply choose
$\cX = \mathbb{R}^2$, $\mu = N(0, I_2)$, $c_{\cX, 0} = K_\cX = K_2$,  $c_{\cY, 0} = K_\cY = K_{784}$,
where $K_d(x, y) = \exp(-\|x-y\|^2/d)$ for $d = 2$, $784$ and
$\|\cdot\|$ denotes the Euclidean distance. Finally, we compute $c_\cX, c_\cY$ by rescaling $c_{\cX, 0}, c_{\cY, 0}$. In words, we want to best match a two-dimensional space to the image manifold of digits ($2, 4, 6, 7$) in the MNIST data set.



\begin{figure}[ht]
    \centering
    \subfloat[\centering  ]{{\includegraphics[width=7cm]{img/2dim/mnist-gray-p2.pdf} }}%
    \qquad
    \subfloat[\centering ]{{\includegraphics[width=7cm]{img/2dim/mnist-loss-p2.pdf} }}%
    \caption{Generated images and training loss on MNIST data set for digits $2, 4, 6, 7$. Subfigure (a) visualizes our generated images by applying $\widehat{F}$ on i.i.d.\ new samples from $N(0, I_2)$: these are RGM simulated images that are distinct from the MNIST data.  Subfigure (b) shows the logarithmic training loss for each component in the empirical loss \eqref{eqn:1} over epochs. }
    \label{fig:mnist}
\end{figure}

We choose the sample size $m = 40000$,  $n = 8000$, tuning parameters $\lambda_1 = \lambda_2=\lambda_3 = 100$, and again use the (stochastic) gradient descent approach. In addition, we parameterize $F$ and $B$ by multi-layer perceptrons (MLP), a class of feedforward neural networks such that every two nearby layers are fully connected. 
Under this parameterization, both $F\colon \R^{2}\to \R^{784}$ and $B\colon \R^{784}\to \R^{2}$ have $3$ hidden layers, and each hidden layer consists of $50$ neurons.
We apply the rectified linear unit (ReLU) activation function,
\[
\sigma(x) = \max(x, 0)\;,
\]
to all hidden layers of $F$ and $B$. 
% and an additional sigmoid activation function,
% \[
% s(x) = \frac{1}{1+e^{-x}}\;,
% \]
% to the output layer of $F$.  % sigmoid removed for the moment
To put it explicitly, $F$ has the following form
\begin{align*}
    h_0 &= x, \quad x\in\R^2\\
    h_l &= \sigma(W_l h_{l-1}+b_l), \quad l = 1,2\\
    y &= W_3h_2 + b_3
\end{align*}
with the parameter space
$ \{(W_l, b_l, 1\le l\le 3)~|~W_1 \in \R^{50\times 2}, W_2\in \R^{50\times 50}, W_3\in\R^{784\times 50},
    b_1, b_2 \in\R^{50\times1}, b_3\in \R^{784\times 1}\}$.
Similarly, $B$ has the following form
\begin{align*}
    \widetilde{h}_0 &= y, \quad y\in\R^{784}\\
    \widetilde{h}_l &= \sigma(\widetilde{W}_l\widetilde{h}_{l-1}+\widetilde{b}_l), \quad l = 1,2\\
    x &= \widetilde{W}_3\widetilde{h}_2 + \widetilde{b}_3
\end{align*}
with $\widetilde{W}_1 \in \R^{50\times 784}, \widetilde{W}_2\in \R^{50\times 50}, \widetilde{W}_3\in\R^{2\times 50},
\widetilde{b}_1, \widetilde{b}_2 \in\R^{50\times1}, \widetilde{b}_3\in \R^{2\times 1}$.
We use Adam \citep{kingma2017}, a variant of stochastic gradient descent, to train the neural networks. 
The training set  is randomly divided into $20$ batches, each batch containing $2000$ samples from $\mu$ and $400$ samples from $\nu$. The learning rate is $0.01$ in the first $500$ iterations; it decreases to $0.001$ from iteration $501$ to iteration $1000$, and further reduces to $0.0001$ after $1000$ iterations.
Figure~\ref{fig:mnist} includes: (a) newly generated samples from $\widehat{F}_\# \mu$, namely generating a fresh two-dimensional Gaussian $x \sim \mu$ and push-forwarding it via the learned transformation $\widehat{F}$; (b) total loss and component-wise loss in \eqref{eqn:1} on a logarithmic scale  against the number of epochs. Here at each time stamp, each loss is computed as the average of the respective batch losses in a whole epoch. Subfigure (a) demonstrates the generative power of our method: these are new, unseen images different from the $60K$ images in the MNIST data set. Our RGM balances among-class expressivity (the newly generated images can express different digits), as well as in-class variability (the newly generated images with the same digit differ from each other).

\begin{figure}[ht]
    \centering
    {{\includegraphics[width =  8cm]{img/2dim/backward-p2.pdf} }}%
    \caption{
    Embedding MNIST images into $\R^2$.
    We generate these points by applying $\widehat{B}$ on $500$ MNIST test samples, where we pick up $125$ samples for each digit.}
    \label{fig:B}
\end{figure}

Next we examine the approximate isomorphism for $\widehat{B}$ in Figure~\ref{fig:B}, a scatter plot by applying $\widehat{B}$ on $500$ new samples from MNIST test set. In plain language, we would like to see how to best embed a $\R^{784}$ MNIST image to a $\R^{2}$ space. We note two observations. First, the distribution of images of $\widehat{B}$, as a whole, is similar to $N(0, I_2)$, which can be easily seen by overlooking the color of the data cloud. Second,  each digit forms a local cluster in $\mathbb{R}^2$ according to the angle.
In other words,  $\widehat{B}_\#\nu\approx N(0, I_2)$ and
$K_2(\widehat{B}(y), \widehat{B}(y^\prime))\approx K_{784}(y, y^\prime)$ hold for $y,  y^\prime\in\mathbb{R}^{784}$, hence indicating an approximate isomorphism. 


\section{Discussions}
In this work, we proposed a novel distance between network spaces, called the Reversible Gromov-Monge distance, inspired by the Gromov-Wasserstein distance between metric measure spaces. Based on this, we designed a transform sampler that can operate between distributions defined on heterogenous spaces. In addition, we introduced two concrete optimization methods for computing RGM given finite samples and proved their properties. Accordingly, our work not only provides a simple yet promising transform sampler, but also sheds light on tackling a notoriously difficult quadratic assignment problem.

Lastly, we mention a few directions that can lead to future research. First, it will be interesting to understand whether one can derive global convergence results for the gradient descent optimization method. Second, we should establish criteria that are theoretically justified for choosing the Lagrangian multipliers in \eqref{eqn:1}. More generally, how to specify and tune appropriate functions $c_{\cX}, c_{\cY}$ and kernels $K_{\cX}, K_{\cY}$ for better empirical results is still not fully elucidated. We leave such questions as potential future work. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Reference                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{ref}
\bibliographystyle{unsrt}
% \nocite{*}
\newpage
\appendix
\section{Remaining Proofs}
\label{sec:appendix}
\subsection{Proofs in Section~\ref{sec:analytic-properties}}

\begin{proof}[Proof of Proposition~\ref{prop:1}]
	Define
	\begin{equation*}
		Q(\gamma) = \int_{\cX \times \cY} \int_{\cX \times \cY} (c_{\cX}(x, x') - c_{\cY}(y, y'))^2 \dd{\gamma}(x, y) \dd{\gamma}(x', y')
	\end{equation*}
	for all $\gamma \in \Pi(\mu, \nu)$ so that
	\begin{equation*}
		\mathrm{GW}(\mu, \nu)^2 = \inf_{\gamma \in \Pi(\mu, \nu)} Q(\gamma) \;.
	\end{equation*}
	Recall that $\Pi_{\cT} = \{(\mathrm{Id}, T)_{\#} \mu : T \in \cT(\mu, \nu)\} \subset \Pi(\mu, \nu)$. Hence, as noted in Section \ref{sec:background}, 
	\begin{equation*}
		\mathrm{GM}(\mu, \nu)^2 = \inf_{\gamma \in \Pi_{\cT}} Q(\gamma) \;.
	\end{equation*}
	Define $\Pi' = \{\gamma \in \Pi(\mu, \nu) : \gamma = (\mathrm{Id}, F)_{\#} \mu = (B, \mathrm{Id})_{\#} \nu ~ \exists (F, B) \in \cI(\nu, \mu) \}$, then one can check
	\begin{equation*}
		\mathrm{RGM}(\mu, \nu)^2 = \inf_{\gamma \in \Pi'} Q(\gamma)
	\end{equation*}
	using change-of-variables. Note that $\Pi'$ may be rewritten as $\{\gamma \in \Pi_{\cT} : \gamma = (B, \mathrm{Id})_{\#} \nu ~ \exists B \in \cT(\nu, \mu) \}$. Hence, $\Pi' \subseteq \Pi_{\cT} \subseteq \Pi(\mu, \nu)$, thus we conclude $\mathrm{GW}(\mu, \nu) \le \mathrm{GM}(\mu, \nu) \le \mathrm{RGM}(\mu, \nu)$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:2}]
	Recall that
	\begin{equation*}
		\mathrm{RGM}(\mu_{\cX}, \mu_{\cZ}) = \inf_{(F, B) \in \cI(\mu_{\cX}, \mu_{\cZ})} C_{\cX \cZ}(F, B)\;,
		\end{equation*}
	where
	\begin{equation*}
		C_{\cX \cZ}(F, B)
		= \left(\int (c_{\cX}(x, B(z)) - c_{\cZ}(F(x), z))^2 \dd{\mu_{\cX} \otimes \mu_{\cZ}}(x, z)\right)^{1/2}\;.
	\end{equation*}
	First, $(F_{\cZ} \circ F_{\cY}, B_{\cX} \circ B_{\cY}) \in \cI(\mu_{\cX}, \mu_{\cZ})$ holds for $(F_{\cY}, B_{\cX}) \in \cI(\mu_{\cX}, \mu_{\cY})$ and $(F_{\cZ}, B_{\cY}) \in \cI(\mu_{\cY}, \mu_{\cZ})$\footnote{The subscript of a map denotes its range; for instance, $B_{\cX}$ maps to $\cX$.} since
	\begin{equation*}
		\begin{split}
			(\mathrm{Id}, F_{\cZ} \circ F_{\cY})_{\#}\mu_{\cX} &= (\mathrm{Id}, F_{\cZ})_{\#} (\mathrm{Id}, F_{\cY})_{\#} \mu_{\cX} \\
			&= (\mathrm{Id}, F_{\cZ})_{\#} (B_{\cX}, \mathrm{Id})_{\#} \mu_{\cY} \quad (\because (F_{\cY}, B_{\cX}) \in \cI(\mu_{\cX}, \mu_{\cY}))\\
			&= (B_{\cX}, \mathrm{Id})_{\#} (\mathrm{Id}, F_{\cZ})_{\#} \mu_{\cY} \\
			& = (B_{\cX}, \mathrm{Id})_{\#} (B_{\cY}, \mathrm{Id})_{\#} \mu_{\cZ} \quad (\because (F_{\cZ}, B_{\cY}) \in \cI(\mu_{\cY}, \mu_{\cZ})) \\
			& = (B_{\cX} \circ B_{\cY}, \mathrm{Id})_{\#} \mu_{\cZ}.
		\end{split}
	\end{equation*}
	\begin{equation*}
		\begin{tikzcd}[row sep = huge]
			\cX \arrow[rr, "F", shift left] \arrow[dr, "F_{\cY}", swap, shift right]
			&&
			\cZ	\arrow[ll, "B", shift left] \arrow[dl, "B_{\cY}", swap, shift right]\\
			&
			\cY \arrow[ur, "F_{\cZ}", swap, shift right] \arrow[ul, "B_{\cX}", swap, shift right]
		\end{tikzcd}
	\end{equation*}


	Moreover, in this case, we have
	\begin{equation*}
		C_{\cX \cZ}(F_{\cZ} \circ F_{\cY}, B_{\cX} \circ B_{\cY})
		\le
		C_{\cX \cY}(F_{\cY}, B_{\cX})
		+
		C_{\cY \cZ}(F_{\cZ}, B_{\cY})
	\end{equation*}
	since
	\begin{equation*}
		\begin{split}
			C_{\cX \cZ}(F_{\cZ} \circ F_{\cY}, B_{\cX} \circ B_{\cY})
			&= \left(\int \left[c_{\cX}(x, B_{\cX} \circ B_{\cY}(z)) - c_{\cZ}(F_{\cZ} \circ F_{\cY}(x), z)\right]^2 \dd{\mu_{\cX} \otimes \mu_{\cZ}}(x, z)\right)^{1/2} \\
			&\le
			\left(\int \int \left[c_{\cY}(x, B_{\cX} \circ B_{\cY}(z)) - c_{\cY}(F_{\cY}(x), B_{\cY}(z))\right]^2 \dd{\mu_{\cZ}}(z) \dd{\mu_{\cX}}(x)\right)^{1/2} \\
			&\quad  +
			\left(\int \int \left[c_{\cY}(F_{\cY}(x), B_{\cY}(z)) - c_{\cZ}(F_{\cZ} \circ F_{\cY}(x), z)\right]^2 \dd{\mu_{\cX}}(x) \dd{\mu_{\cZ}}(z)\right)^{1/2} \\
			& =
			\left(\int \int \left[c_{\cY}(x, B_{\cX}(y)) - c_{\cY}(F_{\cY}(x), y)\right]^2 \dd{\mu_{\cY}}(y) \dd{\mu_{\cX}}(x)\right)^{1/2} \, (\because (B_{\cY})_{\#} \mu_{\cZ} = \mu_{\cY})\\
			&\quad  +
			\left(\int \int \left[c_{\cY}(y, B_{\cY}(z)) - c_{\cZ}(F_{\cZ}(y), z)\right]^2 \dd{\mu_{\cY}}(y) \dd{\mu_{\cZ}}(z)\right)^{1/2} \, (\because (F_{\cY})_{\#}\mu_{\cX} = \mu_{\cY}) \\
			& =
			C_{\cX \cY}(F_{\cY}, B_{\cX})
			+
			C_{\cY \cZ}(F_{\cZ}, B_{\cY})\;.
		\end{split}
	\end{equation*}
	Hence,
	\begin{equation*}
		\begin{split}
			\mathrm{RGM}(\mu_{\cX}, \mu_{\cZ})
			&= \inf_{(F, B) \in \cI(\mu_{\cX}, \mu_{\cZ})} C_{\cX \cZ}(F, B) \\
			&\le \inf_{\substack{(F_{\cY}, B_{\cX}) \in \cI(\mu_{\cX}, \mu_{\cY}) \\ (F_{\cZ}, B_{\cY}) \in \cI(\mu_{\cY}, \mu_{\cZ})}} C_{\cX \cZ}(F_{\cZ} \circ F_{\cY}, B_{\cX} \circ B_{\cY}) \\
			&\le \inf_{(F_{\cY}, B_{\cX}) \in \cI(\mu_{\cX}, \mu_{\cY})} C_{\cX \cY}(F_{\cY}, B_{\cX}) + \inf_{(F_{\cZ}, B_{\cY}) \in \cI(\mu_{\cY}, \mu_{\cZ})} C_{\cY \cZ}(F_{\cZ}, B_{\cY}) \\
			& = \mathrm{RGM}(\mu_{\cX}, \mu_{\cY}) + \mathrm{RGM}(\mu_{\cY}, \mu_{\cZ})\;.
		\end{split}
	\end{equation*}
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:rgm}]
	Suppose $\mathrm{RGM}(\mu, \nu) = 0$. Due to the inequality $\mathrm{GW}(\mu, \nu) \le \mathrm{RGM}(\mu, \nu)$, we have $\mathrm{GW}(\mu, \nu) = 0$, that is, 
	\begin{equation*}
		\inf_{\gamma \in \Pi(\mu, \nu)} \left( \int_{\cX \times \cY} \int_{\cX \times \cY} (h(d_{\cX}(x, x')) - h(d_{\cY}(y, y')))^2 \dd{\gamma}(x, y) \dd{\gamma}(x', y') \right)^{1/2} = 0 \;.
	\end{equation*}
	Since there exists a coupling $\gamma^\star$ that achieves the minimum of GW due to Theorem 12 of \cite{chowdhury_memoli_2019}, we conclude
	\begin{equation*}
		h(d_{\cX}(x, x')) = h(d_{\cY}(y, y'))
	\end{equation*}
	holds $\gamma^\star \otimes \gamma^\star$ almost surely on $(\cX \times \cY)^2$. Since $h$ is strictly monotone, this means
	\begin{equation*}
		d_{\cX}(x, x') = d_{\cY}(y, y')
	\end{equation*}
	holds $\gamma^\star \otimes \gamma^\star$ almost surely on $(\cX \times \cY)^2$. Therefore, 
	\begin{equation*}
		\begin{split}
			& \inf_{\gamma \in \Pi(\mu, \nu)} \left( \int_{\cX \times \cY} \int_{\cX \times \cY} (d_{\cX}(x, x') - d_{\cY}(y, y'))^2 \dd{\gamma}(x, y) \dd{\gamma}(x', y') \right)^{1/2} \\
			& \ge \left( \int_{\cX \times \cY} \int_{\cX \times \cY} (d_{\cX}(x, x') - d_{\cY}(y, y'))^2 \dd{\gamma^\star}(x, y) \dd{\gamma^\star}(x', y') \right)^{1/2} \\
			& = 0 \;.
		\end{split}
	\end{equation*}
	Theorem \ref{thm:1} implies that metric measure spaces $(\cX, \mu, d_{\cX})$ and $(\cY, \nu, d_{\cY})$ are strongly isomorphic. Since $c_{\cX} = h(d_{\cX})$ and $c_{\cY} = h(d_{\cY})$, it follows easily that $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ are strongly isomorphic as well.

	To prove the if part, suppose $(\cX, \mu, c_{\cX})$ and $(\cY, \nu, c_{\cY})$ are strongly isomorphic and consider a strong isomorphism $T$. Then, $(T, T^{-1}) \in \cI(\mu, \nu)$ holds since $(\mathrm{Id}, T)_{\#} \mu = (T^{-1}, \mathrm{Id})_{\#} T_{\#} \mu = (T^{-1}, \mathrm{Id})_{\#} \nu$. Also, by definition of $T$, we have $c_{\cX}(x, T^{-1}(y)) = c_{\cY}(T(x), T \circ T^{-1}(y)) = c_{\cY}(T(x), y)$ for all $(x, y) \in \cX \times \cY$, thus
	\begin{equation*}
		\mathrm{RGM}(\mu, \nu) \le \int_{\cX \times \cY} (c_{\cX}(x, T^{-1}(y)) -  c_{\cY}(T(x), y))^2 \dd{\mu \otimes \nu} = 0\;.
	\end{equation*}
\end{proof}


\subsection{Proofs in Section~\ref{sec:statistical-theory}}

\begin{proof}[Proof of Proposition~\ref{prop:4}]
	Let $h_{F, B}(x, y) := (c_{\cX}(x, B(y)) - c_{\cY}(F(x), y))^2$, then
	\begin{equation*}
		\begin{split}
			\widehat{C}_0(F, B) - C_0(F, B)
			& = \frac{1}{m n} \sum_{i=1}^{m} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{(x, y) \sim \mu \otimes \nu} h_{F, B}(x, y) \\
			& = \frac{1}{m} \sum_{i=1}^{m} \left(\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right) \\
			& \quad + \frac{1}{m} \sum_{i=1}^{m} \E_{y \sim \nu} h_{F, B}(x_i, y) - \E_{(x, y) \sim \mu \otimes \nu} h_{F, B}(x, y)\;.
		\end{split}
	\end{equation*}
	Assumption \ref{a:bounded1} implies that a function $x \mapsto \E_{y \sim \nu} h_{F, B}(x, y)$ is bounded in $[0, H]$. Thus, by the McDiarmid's inequality,
	\begin{equation*}
		\frac{1}{m} \sum_{i=1}^{m} \E_{y \sim \nu} h_{F, B}(x_i, y) - \E_{(x, y) \sim \mu \otimes \nu} h_{F, B}(x, y) \le \sqrt{\frac{H^2 \log(1 / \delta)}{2 m}}
	\end{equation*}
	holds with probability at least $1 - \delta$. By the same logic, for fixed $x_i$,
	\begin{equation*}
		\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y) \le \sqrt{\frac{H^2 \log(1 / \delta)}{2 n}}
	\end{equation*}
	holds with probability at least $1 - \delta$, where the probability is the conditional probability of $y_1, \ldots, y_n$ given $x_1, \ldots, x_m$. Since this is true for all $x_i$, the union bound implies
	\begin{equation*}
		\frac{1}{m} \sum_{i=1}^{m} \left(\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right) \le \sqrt{\frac{H^2 \log(m / \delta)}{2 n}}
	\end{equation*}
	holds with probability at least $1 - \delta$. Hence,
	\begin{equation*}			
		\widehat{C}_0(F, B) - C_0(F, B)
		\precsim \sqrt{\frac{\log(m / \delta)}{m}} \le \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$. The same result holds for $C_0(F, B) - \widehat{C}_0(F, B)$, hence we complete the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:mmd_fixed}]
	By the triangle inequality, $|\widehat{M}(F, B) - M(F, B)|$ is bounded above by the sum of the following three terms:
	\begin{align*}
		& |\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)| \;, \\
		& |\mathrm{MMD}^2_{K_{\cX}}(\widehat{\mu}_m, B_{\#}\widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX}}(\mu, B_{\#} \nu)| \;, \\
		& |\mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)| \;.
	\end{align*}
	First, we give an upper bound on the first term. Boundedness of kernels (Assumption \ref{a:bounded_kernels}) implies
	\begin{equation*}
		\mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) \;, \; \mathrm{MMD}_{K_{\cY}}(F_{\#}\mu, \nu) \le 2 \sqrt{K} \;.
	\end{equation*}
	Hence,
	\begin{equation*}
		|\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)| \le 4 \sqrt{K} |\mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}_{K_{\cY}}(F_{\#}\mu, \nu)|\;.
	\end{equation*}
	Due to the triangle inequality of MMD, we have
	\begin{equation*}
		|\mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}_{K_{\cY}}(F_{\#}\mu, \nu)|
		\le \mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_{m}, F_{\#}\mu) + \mathrm{MMD}_{K_{\cY}}(\widehat{\nu}_n, \nu)\;.
	\end{equation*}
	By Theorem 3.4 of \cite{muandet_fukumizu_sriperumbudur_scholkopf_2017},
	\begin{equation*}
		\mathrm{MMD}_{K_{\cY}}(\widehat{\nu}_n, \nu) \le \sqrt{\frac{K}{n}} + \sqrt{\frac{2 K \log(1/\delta)}{n}}
	\end{equation*}
	holds with probability at least $1 - \delta$. Next, note that $F_{\#}\widehat{\mu}_m = \frac{1}{m} \sum_{i} \delta_{F(x_i)}$ is the empirical measure constructed from $\{F(x_i)\}_{i = 1}^{m}$. Since they are $m$ many i.i.d.\ samples from $F_{\#}\mu$, by the same theorem, 
	\begin{equation*}
		\mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_m, F_{\#}\mu) \le \sqrt{\frac{K}{m}} + \sqrt{\frac{2 K \log(1/\delta)}{m}}
	\end{equation*}
	holds with probability at least $1 - \delta$. Hence,
	\begin{equation*}
		|\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)| \precsim \sqrt{\frac{\log(1 / \delta)}{m}} + \sqrt{\frac{\log(1 / \delta)}{n}}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$. Similarly, we have
	\begin{align*}
		& |\mathrm{MMD}^2_{K_{\cX}}(\widehat{\mu}_m, B_{\#}\widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX}}(\mu, B_{\#} \nu)| \precsim \sqrt{\frac{\log(1 / \delta)}{m}} + \sqrt{\frac{\log(1 / \delta)}{n}}\;, \\
		& |\mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)| \precsim \sqrt{\frac{\log(1 / \delta)}{m}} + \sqrt{\frac{\log(1 / \delta)}{n}}\;,
	\end{align*}
	each of which holds with probability at least $1 - 2 \delta$. Combining these three probabilistic bounds, we obtain a bound for $|\widehat{M}(F, B) - M(F, B)|$.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:union-bound-GW-term}]
	Without loss of generality, assume $n \ge m$. From the proof of Proposition \ref{prop:4}, 
	\begin{equation*}
		\begin{split}
			\sup_{(F, B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)|
			& \le \frac{1}{m} \sum_{i=1}^{m} 
			\sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right| \\
			& \quad + 
			\sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \E_{y \sim \nu} h_{F, B}(x_i, y) - \E_{x \sim \mu} \E_{y \sim \nu} h_{F, B}(x, y)\right| \;.
		\end{split}
	\end{equation*}
	Since $x \mapsto \E_{y \sim \nu} h_{F, B}(x, y)$ is bounded in $[0, H]$, Lemma \ref{lem:bound} implies
	\begin{equation*}
		\begin{split}
			\sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \E_{y \sim \nu} h_{F, B}(x_i, y) - \E_{x \sim \mu} \E_{y \sim \nu} h_{F, B}(x, y)\right|
			& \precsim
			\sqrt{\frac{\log(1 / \delta)}{m}} \\
			& + 
			\E_{x_i} \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i \E_{y \sim \nu} h_{F, B}(x_i, y) \right|
		\end{split}
	\end{equation*}
	holds with probability at least $1 - \delta$. Since $n \ge m$, 
	\begin{equation*}
		\begin{split}
			\E_{x_i} \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i \E_{y \sim \nu} h_{F, B}(x_i, y) \right| 
			& = 
			\E_{x_i} \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\E_{y_1, \ldots, y_m} \frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right| \\
			& \le 
			\E_{x_i} \E_{\epsilon_i} \E_{y_1, \ldots, y_m} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right| \\
			& =
			\E_{x_i} \E_{y_i} \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right| \;.
		\end{split}
	\end{equation*}
	We first give an upper bound on 
	\begin{equation*}
		\E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \underbrace{\left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right|}_{=: X_{F, B}} \;.
	\end{equation*}
	First, observe that Assumption \ref{a:bounded1} and Assumption \ref{a:lip_of_C} imply 
	\begin{equation*}
		\begin{split}
			|h_{F, B}(x, y) - h_{F', B'}(x, y)|
			& \le
			\left| \sqrt{h_{F, B}(x, y)} + \sqrt{h_{F', B'}(x, y)} \right| \left| \sqrt{h_{F, B}(x, y)} - \sqrt{h_{F', B'}(x, y)} \right| \\
			& \le 
			2 \sqrt{H} \left( |c_{\cX}(x, B(y)) - c_{\cX}(x, B'(y))| + |c_{\cY}(F(x), y) - c_{\cY}(F'(x), y)| \right) \\
			& \le
			2 \sqrt{H} L \left( \|F(x) - F'(x)\| + \|B(y) - B'(y)\| \right) \\
			\\
			& =
			2 \sqrt{H} L \left[ \sqrt{\sum_{k = 1}^{\mathrm{dim}(\cY)} |F_k(x) - F_k'(x)|^2} + \sqrt{\sum_{\ell = 1}^{\mathrm{dim}(\cX)} |B_\ell(y) - B_\ell'(y)|^2} \right]\\
			& \le 
			2 \sqrt{H} L \left(\sum_{k = 1}^{\mathrm{dim}(\cY)} |F_k(x) - F_k'(x)| + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} |B_\ell(y) - B_\ell'(y)|\right) \;. 
		\end{split}
	\end{equation*}
	Therefore, 
	\begin{equation*}
		\begin{split}
			|X_{F, B} - X_{F', B'}| 
			& \le \frac{1}{m} \sum_{i = 1}^{m} |h_{F, B}(x_i, y_i) - h_{F', B'}(x_i, y_i)| \\
			& \le 
			2 \sqrt{H} L \left(\sum_{k = 1}^{\mathrm{dim}(\cY)} \frac{1}{m} \sum_{i = 1}^{m} |F_k(x_i) - F_k'(x_i)| + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \frac{1}{m} \sum_{i = 1}^{m} |B_\ell(y_i) - B_\ell'(y_i)|\right) \\
			& \le
			2 \sqrt{H} L \left(\sum_{k = 1}^{\mathrm{dim}(\cY)} \max_{i \in [m]} |F_k(x_i) - F_k'(x_i)| + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \max_{i \in [m]} |B_\ell(y_i) - B_\ell'(y_i)|\right) \\
			& =: \rho((F, B), (F', B')) \;.
		\end{split}
	\end{equation*}
	For $\epsilon > 0$, let $\cN_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m})$ be the minimal $\epsilon$-covering net of $\cF_k$ under the pseudometric $d$ induced by $x_1, \ldots, x_m$:
	\begin{equation*}
		d(F_k, F_k') := \max_{i \in [m]} | F_k(x_i) - F_k(x_i') |.
	\end{equation*}
	In other words, for any $F_k \in \cF_k$, we can find $F_k' \in \cN_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m})$ such that $d(F_k, F_k') \le \epsilon$. Also, $|\cN_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m})| = N_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m})$. We define $\cN_\infty(\epsilon, \cB_\ell, \{y_i\}_{i=1}^{m})$ in a similar fashion.
	
	Given $\epsilon > 0$, let $T_{\epsilon} = \otimes_{k = 1}^{\mathrm{dim}(\cY)} \cN_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m}) \times \otimes_{\ell = 1}^{\mathrm{dim}(\cX)} \cN_\infty(\epsilon, \cB_\ell, \{y_i\}_{i=1}^{m})$. Then, for any $(F, B) \in \cF \times \cB$, we can find $(F', B') \in T_{\epsilon}$ such that 
	\begin{equation*}
		\rho((F, B), (F', B')) \le \eta \epsilon \;,
	\end{equation*}
	where $\eta = 2 \sqrt{H} L (\mathrm{dim}(\cX) + \mathrm{dim}(\cY))$. As a result, one can easily check
	\begin{equation*}
		\begin{split}
			\sup_{(F, B) \in \cF \times \cB} X_{F, B}
			& \le \sup_{\rho((F, B), (F', B')) \le \eta \epsilon} |X_{F, B} - X_{F', B'}| + \sup_{(F, B) \in T_\epsilon} X_{F, B} \\
			& \le \eta \epsilon + \sup_{(F, B) \in T_\epsilon} X_{F, B}\;.
		\end{split}
	\end{equation*}
	Note that $X_{F, B}$ is a sub-Gaussian random variable with parameter $H^2 / (4 m)$. Hence, the maximal inequality yields
	\begin{equation*}
		\begin{split}
			\E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} X_{F, B}
			& \le \eta \epsilon + \E_{\epsilon_i} \sup_{(F, B) \in T_\epsilon} X_{F, B}
			\le \eta \epsilon + \sqrt{\frac{H^2 \log(|T_\epsilon|)}{2 m}}
			\;.
		\end{split}
	\end{equation*}
	Using $|T_\epsilon| = \prod_{k = 1}^{\mathrm{dim}(\cY)} N_{\infty}(\epsilon, \cF_k, \{x_i\}_{i=1}^{m}) \times \prod_{\ell = 1}^{\mathrm{dim}(\cX)} N_\infty(\epsilon, \cB_\ell, \{y_i\}_{i=1}^{m})$, we have
	\begin{equation*}
		\begin{split}
			& \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right| \\
			& \le 
			\eta \epsilon +  H \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, \{x_i\}_{i=1}^{m}) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, \{y_i\}_{i=1}^{m})}{2 m}} \\
			& \le \eta \epsilon +  H \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, m)}{2 m}} \;.
		\end{split}
	\end{equation*}
	The second inequality is obvious from the definition of the uniform covering number. Since the last equation is independent of $x_i$ and $y_i$, we have 
	\begin{equation*}
		\E_{x_i} \E_{y_i} \E_{\epsilon_i} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \epsilon_i h_{F, B}(x_i, y_i) \right| 
		\le 
		\eta \epsilon +  H \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, m)}{2 m}} \;.
	\end{equation*}
	As a result,
	\begin{equation}
		\label{eqn:b1}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{m} \sum_{i=1}^{m} \E_{y \sim \nu} h_{F, B}(x_i, y) - \E_{x \sim \mu} \E_{y \sim \nu} h_{F, B}(x, y)\right| \\
			& \precsim
			\sqrt{\frac{\log(1 / \delta)}{m}} + \epsilon +  \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, m)}{m}} \\
			& \le
			\sqrt{\frac{\log(1 / \delta)}{m}} + \epsilon +  \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{m}}
		\end{split}
	\end{equation}
	holds with probability at leat $1 - \delta$. Here, $\log N_{\infty}(\epsilon, \cB_\ell, m) \le \log N_{\infty}(\epsilon, \cB_\ell, n)$ holds since $n \ge m$, which is obvious from the definition of the uniform covering number. 

	Next, we give a bound on 
	\begin{equation*}
		\frac{1}{m} \sum_{i=1}^{m} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right| \;.
	\end{equation*}
	Considering $x_1, \ldots, x_m$ are fixed, Lemma \ref{lem:bound} implies
	\begin{equation*}
		\begin{split}
			\sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right|
			& \precsim \sqrt{\frac{\log(1 / \delta)}{n}} + \E_{y_j} \E_{\epsilon_j} \sup_{(F, B) \in \cF \times \cB} \underbrace{\left|\frac{1}{n} \sum_{j=1}^{n} \epsilon_j h_{F, B}(x_i, y_j) \right|}_{Y_{F, B}} \\
		\end{split}
	\end{equation*}
	holds with probability at least $1 - \delta$. Here, the probability should be understood as a conditional probability of $y_1, \ldots, y_n$ given $x_1, \ldots, x_m$. Again, we have
	\begin{equation*}
		\begin{split}
			|Y_{F, B} - Y_{F', B'}| 
			& \le
			2 \sqrt{H} L \left(\sum_{k = 1}^{\mathrm{dim}(\cY)} |F_k(x_i) - F_k'(x_i)| + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \frac{1}{n} \sum_{j = 1}^{n} |B_\ell(y_j) - B_\ell'(y_j)|\right) \\
			& \le
			2 \sqrt{H} L \left(\sum_{k = 1}^{\mathrm{dim}(\cY)} \max_{i \in [m]} |F_k(x_i) - F_k'(x_i)| + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \max_{j \in [n]} |B_\ell(y_j) - B_\ell'(y_j)|\right) \;.
		\end{split}
	\end{equation*}
	Also, $Y_{F, B}$ is a sub-Gaussian random variable with parameter $H^2 / (4 n)$. By the same argument as before, 
	\begin{equation*}
		\E_{y_j} \E_{\epsilon_j} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} \epsilon_j h_{F, B}(x_i, y_j) \right| \le \eta \epsilon +  H \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{2 n}} \;.
	\end{equation*}
	Hence, 
	\begin{equation*}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right| \\
			& \precsim \sqrt{\frac{\log(1 / \delta)}{n}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{n}} 
		\end{split}
	\end{equation*}
	holds with probability (conditional probability as explained earlier) at least $1 - \delta$. Since this holds for all $x_i$, the union bound implies
	\begin{equation}
		\label{eqn:b2}
		\begin{split}
			& \frac{1}{m} \sum_{i=1}^{m} \sup_{(F, B) \in \cF \times \cB} \left|\frac{1}{n} \sum_{j=1}^{n} h_{F, B}(x_i, y_j) - \E_{y \sim \nu} h_{F, B}(x_i, y)\right| \\
			& \precsim 
			\sqrt{\frac{\log(m / \delta)}{n}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{n}} \\
			& \le \sqrt{\frac{\log(m / \delta)}{m}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{m}} 
		\end{split}
	\end{equation}
	holds with probability at least $1 - \delta$. Combining \eqref{eqn:b1} and \eqref{eqn:b2}, for any $\epsilon > 0$, we have
	\begin{equation*}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)| \\
			\precsim
			& \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{m \wedge n}}
		\end{split}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$.
\end{proof}
\begin{proof}[Proof of Corollary~\ref{cor:union-bound-GW-term-pdim}]
	Combining Assumption \ref{a:uniform_boundedness} and Lemma \ref{lem:uniform_covering_to_pdim}, we have
	\begin{equation*}
		N_\infty(\epsilon, \cF_k, m) \le \left( \frac{2 e m b}{\epsilon \cdot {\rm Pdim}(\cF_{k})} \right)^{{\rm Pdim}(\cF_{k})} \;.
	\end{equation*}
	Hence,
	\begin{equation*}
		\begin{split}
			& \sup_{(F, B) \in \cF \times \cB} |\widehat{C}_0(F, B) - C_0(F, B)| \\
			\precsim
			& \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \epsilon + \sqrt{\frac{\sum_{k = 1}^{\mathrm{dim}(\cY)} \log N_{\infty}(\epsilon, \cF_k, m) + \sum_{\ell = 1}^{\mathrm{dim}(\cX)} \log N_{\infty}(\epsilon, \cB_\ell, n)}{m \wedge n}} \\
			& \le \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \epsilon + \sqrt{\frac{\log\left(\tfrac{2e b (m \vee n)}{\epsilon}\right)}{m \wedge n} \left( \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) + \sum_{\ell = 1}^{{\rm dim}(\cX)} {\rm Pdim}(\cB_{\ell})\right)} \\
			& \precsim \sqrt{\frac{\log(\tfrac{m \vee n}{\delta})}{m \wedge n}} + \sqrt{\frac{\log(m \vee n)}{m\wedge n} \left( \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) + \sum_{\ell = 1}^{{\rm dim}(\cX)} {\rm Pdim}(\cB_{\ell}) \right)} 
		\end{split}
	\end{equation*}
	holds with probability at least $1 - 2 \delta$, where the last bound comes from choosing $\epsilon = (m \wedge n)^{- 1 / 2}$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:uniform-deviation-mmd-term}]
	Using the triangle inequality, we bound $\sup_{(F,B) \in \cF \times \cB} |\widehat{M}(F, B) - M(F, B)|$ by the sum of the following three terms:
	\begin{align*}
		& \sup_{F \in \cF} |\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)| \;, \\
		& \sup_{B \in \cB} |\mathrm{MMD}^2_{K_{\cX}}(\widehat{\mu}_m, B_{\#}\widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX}}(\mu, B_{\#} \nu)| \;, \\
		& \sup_{(F,B) \in \cF \times \cB} |\mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)| \;.
	\end{align*}
	As in the proof of Proposition \ref{prop:mmd_fixed}, we have
	\begin{equation*}
		\sup_{F \in \cF} |\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)|
		\le 
		4 \sqrt{K} \left[\sup_{F \in \cF} \mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_{m}, F_{\#}\mu)  + \mathrm{MMD}_{K_{\cY}}(\widehat{\nu}_n, \nu) \right] \;.
	\end{equation*}
	$\mathrm{MMD}_{K_{\cY}}(\widehat{\nu}_n, \nu)$ has already been bounded in Proposition \ref{prop:mmd_fixed}. For the first term on the RHS, observe that
	\begin{equation*}
		\begin{split}
			\sup_{F \in \cF} \mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_{m}, F_{\#}\mu)
			& = \sup_{F \in \cF} \sup_{f \in \cH_{\cY}(1)} \left|\int f \dd{F_{\#} \widehat{\mu}_{m}} - \int f \dd{F_{\#} \mu} \right| \\
			& = \sup_{F \in \cF} \sup_{f \in \cH_{\cY}(1)} \left|\int f \circ F \dd{\widehat{\mu}_{m}} - \int f \circ F \dd{\mu} \right| \\
			& = \sup_{f \in \cH_{\cY}(1) \circ \cF} \left|\int f \dd{\widehat{\mu}_{m}} - \int f \dd{\mu} \right|\;,
		\end{split}
	\end{equation*}
	where the second equality follows from change-of-variables.

	First, we show $\cH_{\cY}(1)$ consists of $\sqrt{K}$-uniformly bounded functions. Let $\|\cdot\|_{\cH_{\cY}}$ be the norm of $\cH_{\cY}$ so that $f \in \cH_{\cY}(1)$ is equivalent to $\|f\|_{\cH_{\cY}} \le 1$. Then, the reproducing property implies
	\begin{equation*}
		|f(y)| \le \|f\|_{\cH_{\cY}} \sqrt{K_{\cY}(y, y)} \le \sqrt{K} 
	\end{equation*}
	for any $f \in \cH_{\cY}(1)$. Accordingly, $\cH_{\cY}(1) \circ \cF$ also consists of $\sqrt{K}$-uniformly bounded functions. Hence, Lemma \ref{lem:bound} implies that
	\begin{equation*}
		\begin{split}
			\sup_{F \in \cF} \mathrm{MMD}_{K_{\cY}}(F_{\#}\widehat{\mu}_{m}, F_{\#}\mu)
			&= \sup_{f \in \cH_{\cY}(1) \circ \cF} \left|\int f \dd{\widehat{\mu}_{m}} - \int f \dd{\mu} \right| \\
			&\le 2 R_m(\cH_{\cY}(1) \circ \cF, \mu) + \sqrt{\frac{2 K \log(1/\delta)}{m}}
		\end{split}
	\end{equation*}
	holds with probability at least $1 - \delta$. Therefore, combining this with the upper bound on $\mathrm{MMD}_{K_{\cY}}(\widehat{\nu}_n, \nu)$ derived in Proposition \ref{prop:mmd_fixed},
	\begin{equation}
		\label{eqn:m1}
		\sup_{F \in \cF} |\mathrm{MMD}^2_{K_{\cY}}(F_{\#}\widehat{\mu}_m, \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cY}}(F_{\#}\mu, \nu)|
		\precsim 
		R_m(\cH_{\cY}(1) \circ \cF, \mu) + \sqrt{\frac{\log(1/\delta)}{m}} + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	holds with probability at least $1 - 2 \delta$. Similarly, we can prove that
	\begin{equation}
		\label{eqn:m2} 
		\sup_{B \in \cB} |\mathrm{MMD}^2_{K_{\cX}}(\widehat{\mu}_m, B_{\#}\widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX}}(\mu, B_{\#} \nu)| 
		\precsim 
		R_n(\cH_{\cX}(1) \circ \cB, \nu) + \sqrt{\frac{\log(1/\delta)}{m}} + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	holds with probability at least $1 - 2 \delta$. 
	
	Lastly, since $K_{\cX} \otimes K_{\cY}$ is bounded by $K^2$, that is, 
	\begin{equation*}
		\sup_{(x, y), (x', y') \in \cX \times \cY} K_{\cX} \otimes K_{\cY}((x, y), (x', y')) \le K^2,
	\end{equation*}
	by the same argument, we have
	\begin{equation*}
		\begin{split}
			& \sup_{(F,B) \in \cF \times \cB} |\mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)| \\
			&\quad \le 4 K \left[\sup_{F \in \cF} \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (\mathrm{Id}, F)_{\#}\mu) + \sup_{B \in \cB} \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}} ((B, \mathrm{Id})_{\#} \widehat{\nu}_n, (B, \mathrm{Id})_{\#} \nu)\right] \;.
		\end{split}
	\end{equation*}
	Analogously, $\cH_{\cX \times \cY}(1)$ consists of $K$-uniformly bounded functions, hence
	\begin{align*}
		\sup_{F \in \cF} \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (\mathrm{Id}, F)_{\#}\mu)
		& = \sup_{f \in \cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF)} \left|\int f \dd{\widehat{\mu}_{m}} - \int f \dd{\mu} \right| \\
		& \le 2 R_m(\cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF), \mu) + \sqrt{\frac{2 K^2 \log(1/\delta)}{m}} \;, \\
		\sup_{B \in \cB} \mathrm{MMD}_{K_{\cX} \otimes K_{\cY}} ((B, \mathrm{Id})_{\#} \widehat{\nu}_n, (B, \mathrm{Id})_{\#} \nu)
		& = \sup_{f \in \cH_{\cX \times \cY}(1) \circ (\cB, \mathrm{Id})} \left|\int f \dd{\widehat{\nu}_{n}} - \int f \dd{\nu} \right| \\
		& \le 2 R_n(\cH_{\cX \times \cY}(1) \circ (\cB, \mathrm{Id}), \nu) + \sqrt{\frac{2 K^2 \log(1/\delta)}{n}} \;,
	\end{align*}	
	each of which holds with probability at least $1 - \delta$. Therefore,
	\begin{equation}
		\begin{split}		
			\label{eqn:m3}
			& \sup_{(F,B) \in \cF \times \cB} |\mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\widehat{\mu}_m, (B, \mathrm{Id})_{\#} \widehat{\nu}_n) - \mathrm{MMD}^2_{K_{\cX} \otimes K_{\cY}} ((\mathrm{Id}, F)_{\#}\mu, (B, \mathrm{Id})_{\#} \nu)| \\
			& \precsim R_m(\cH_{\cX \times \cY}(1) \circ (\mathrm{Id}, \cF), \mu) + R_n(\cH_{\cX \times \cY}(1) \circ (\cB, \mathrm{Id}), \nu) + \sqrt{\frac{\log(1/\delta)}{m}} + \sqrt{\frac{\log(1/\delta)}{n}}
		\end{split}
	\end{equation}
	holds with probability at least $1 - 2 \delta$. We complete the proof by combining \eqref{eqn:m1}, \eqref{eqn:m2}, \eqref{eqn:m3}.
\end{proof}


\begin{proof}[Proof of Lemma~\ref{lem:chaining}]
	For any integer $j \in \N \cup \{0\}$, define $\delta_j = 2^{-j} \Delta$, and let $\cT_{j} \subset \mathbb{S}_0^{m \times m}$ be a minimal $\delta_j$-covering net of $\cT$; clearly, $|\cT_j| = N(\delta_j, \cT)$. For each $j$, the covering set induces a mapping $\Pi_j : \cT \rightarrow \cT_j$ such that
	\begin{align*}
		\sup_{A \in \cT} d(A, \Pi_j(A)) \leq \delta_j \;.
	\end{align*}
	By definition of $\Delta$, we may assume $\cT_0 = \{A_0\}$ so that $\Pi_0(A) = A_0$ for all $A \in \cT$.

	Note that $\E g^\top A_0 g = 0$ by definition. Using this, we write $\E \sup_{A \in \cT} g^\top A g$ as a chaining sum:
	\begin{align*}
		\E \sup_{A \in \cT} g^\top A g
		& = \E_{g} \sup_{A \in \cT} g^\top A g - \E g^\top A_0 g \\ 
		& = \E \sup_{A \in \cT} \left(g^\top A g  - g^\top A_0 g  \right) \\
		& = \E \sup_{A \in \cT} \left( g^\top A g  - g^\top \Pi_J(A) g + \sum_{j=0}^{J-1} g^\top \Pi_{j+1}(A) g -  g^\top \Pi_{j}(A) g \right) \\
		& \leq \E \sup_{A \in \cT}  \left( g^\top A g  - g^\top \Pi_J(A) g \right) + \sum_{j=0}^{J-1} \E \sup_{A \in \cT} \left( g^\top \Pi_{j+1}(A) g -  g^\top \Pi_{j}(A) g \right).
	\end{align*}
	For the first term on RHS, using the Cauchy-Schwarz inequality and Jensen's inequality, we have
	\begin{align*}
		\E \sup_{A \in \cT} \left( g^\top A g  - g^\top \Pi_J(A) g \right) \leq \E \left[  \big(\sum_{i\neq j} g_i^2 g_j^2 \big)^{1/2} \cdot \delta_J \right]  \leq m \delta_J \;.
	\end{align*}
	For each summand in the second term on RHS, use Lemma~\ref{lem:maximal-ineq}. Note that for any $j$, the maximal cardinality of
	\begin{equation*}
		| \{ (\Pi_{j+1}(A), \Pi_{j}(A)) ~:~ A \in \cT \} | \leq N(\delta_{j+1}, \cT) \times N(\delta_{j}, \cT) \leq N(\delta_{j+1}, \cT)^2
	\end{equation*}
	and that
	\begin{equation*}
		d(\Pi_{j+1}(A), \Pi_{j}(A)) \leq  d(\Pi_{j+1}(A), A) + d(A, \Pi_{j}(A)) \leq 3 \delta_{j+1} \;.
	\end{equation*}
	Since $\Pi_{j + 1}(A) - \Pi_j(A) \in \mathbb{S}_0^{m \times m}$ and $\|\Pi_{j + 1}(A) - \Pi_j(A)\| \le 3 \delta_{j + 1}$, Lemma~\ref{lem:maximal-ineq} asserts that for any $j$ 
	\begin{align*}
		\E \sup_{A \in \cT}  \left( g^\top \Pi_{j+1}(A) g -  g^\top \Pi_{j}(A) g \right) \leq 6\delta_{j+1} \sqrt{2\log N(\delta_{j+1}, \cT)} + 12 \delta_{j+1} \log N(\delta_{j+1}, \cT) \;.
	\end{align*}
	Summing over $j$, we have for any $J$, the following inequality,
	\begin{align*}
		\E \sup_{A \in \cT} \left( g^\top A g  - g^\top A_0 g  \right) 
		\leq m \delta_J +  12\int_{\delta_J / 2}^{\Delta/2} \sqrt{2 \log N(\delta, \cT)} \dd{\delta} + 24 \int_{\delta_J / 2}^{\Delta/2} \log N(\delta, \cT) \dd{\delta} \;.
	\end{align*}
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:Rademacher-upper-bound-by-chaining}]
	To make use of the chaining inequality, we are only left to bound the covering number $N(\delta, \cT)$ with
	\begin{equation*}
		\cT := \{ A_F : F \in \cF \} \subset \mathbb{S}_0^{m \times m}\;.
	\end{equation*}
	Lipschitzness of $K_{\cY}$ (Assumption \ref{a:lip_kernels}) implies 
	\begin{equation*}
		\left|K_{\cY}(F(x_i), F(x_j)) - K_{\cY}(F'(x_i), F'(x_j))\right| \le \frac{L}{2} \|F(x_i) - F'(x_i)\| + \frac{L}{2} \|F(x_j) - F'(x_j)\|.
	\end{equation*}
	Hence, 
	\begin{equation*}
		\begin{split}
			d(A_F, A_{F'})
			& \leq m \max_{i\neq j} \left|K_{\cY}(F(x_i), F(x_j)) - K_{\cY}(F'(x_i), F'(x_j))\right| \\
			& \le \frac{m L}{2} \left( \max_{i \in [m]} \|F(x_i) - F'(x_i)\| + \max_{j \in [m]} \|F(x_j) - F'(x_j)\| \right)\\
			& = m L \max_{i \in [m]} \| F(x_i) - F'(x_i) \| \\
			& \le m L \max_{i \in [m]} \left(\sum_{k = 1}^{\mathrm{dim}(\cY)}  |F_k(x_i) - F_k'(x_i)|^2 \right)^{1/2} \\
			& \le m L \left[\sum_{k = 1}^{\mathrm{dim}(\cY)} \left(\max_{i \in [m]} |F_k(x_i) - F_k'(x_i)|\right)^2 \right]^{1/2} \;.	
		\end{split}
	\end{equation*}
	As in the proof of Proposition \ref{prop:union-bound-GW-term}, for $\epsilon > 0$, let $\cN_\infty(\epsilon, \cF_k, \{x_i\}_{i=1}^{m})$ be a minimal $\epsilon$-covering net of $\cF_k$. Then, one can easily see that 
	\begin{equation*}
		\left\{ A_F : F \in \otimes_{k = 1}^{\mathrm{dim}(\cY)}\cN_\infty \left( \tfrac{\delta}{m L \sqrt{\mathrm{dim}(\cY)}}, \cF_{k}, \{x_i\}_{i = 1}^{m}) \right) \right\}
	\end{equation*}
	is a $\delta$-covering of $\cT$. Therefore, we conclude
	\begin{equation*}
		N(\delta, \cT) 
		\le
		\prod_{k=1}^{{\rm dim}(\cY)} N_\infty \left(\tfrac{\delta}{m L \sqrt{\mathrm{dim}(\cY)}}, \cF_{k}, \{x_i\}_{i = 1}^{m} \right) 
		\le \prod_{k=1}^{{\rm dim}(\cY)} N_\infty \left(\tfrac{\delta}{m L \sqrt{\mathrm{dim}(\cY)}}, \cF_{k}, m \right) \;.
	\end{equation*}
	Lastly, we bound $N_\infty(\delta, \cF_k, m)$ by the pseudo-dimension of $\cF_k$ via Lemma \ref{lem:uniform_covering_to_pdim}. Combining Assumption \ref{a:uniform_boundedness} and Lemma \ref{lem:uniform_covering_to_pdim}, we have 
	\begin{align*}
		N(\delta, \cT) 
		\leq \prod_{k=1}^{{\rm dim}(\cY)} \left( \frac{2 e m b}{ \tfrac{\delta}{m L \sqrt{\mathrm{dim}(\cY)}} \cdot {\rm Pdim}(\cF_{k})} \right)^{{\rm Pdim}(\cF_{k})} \;.
	\end{align*}

	Now, to apply Lemma~\ref{lem:chaining}, fix $F_0 \in \cF$ and let $A_0 = A_{F_0}$. Then, 
	\begin{equation*}
		\int_{\delta_J / 2}^{\Delta/2} \log N(\delta, \cT) \dd{\delta} 
		\leq \Delta / 2 \cdot \sum_{k=1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) \cdot \log \left( \frac{2 e m b}{ \tfrac{1}{m L \sqrt{\mathrm{dim}(\cY)}} \Delta 2^{-J} / 2 \cdot {\rm Pdim}(\cF_{k})} \right) \;.
	\end{equation*}
	First, we obtain an upper bound on $\Delta$:
	\begin{equation*}
		\Delta = \sup_{F \in \cF} \|A_F - A_0\| \le m \max_{i\neq j} \left|K_{\cY}(F(x_i), F(x_j)) - K_{\cY}(F_0(x_i), F_0(x_j))\right| \le 2 m K \;.
	\end{equation*}
	Next, we claim that $\Delta$ is bounded below by a universal contant; this is to upper bound $\Delta$ in the denominator. Consider $y_0$ and $y_0'$ given in Assumption \ref{a:separation}. We may assume that $F_0$ is the constant map explained in Assumption \ref{a:separation}: $F_0(x) = y_0$ for all $x \in \cX$. Without loss of generality, we assume $x_1 \neq x_2$. Then, we can find $F \in \cF$ such that $F(x_1) = y_0$ and $F(x_2) = y_0'$ according to Assumption \ref{a:separation}. Hence, 
	\begin{equation*}
		\Delta \ge |K_{\cY}(F(x_1), F(x_2)) - K_{\cY}(F_0(x_1), F_0(x_2))| = |K_{\cY}(y_0, y_0') - K_{\cY}(y_0, y_0)| > 0 \;. 
	\end{equation*}
	Therefore, with the choice of $J$ such that $m 2^{-J} \asymp \sum_k {\rm Pdim}(\cF_{k})$, 
	\begin{equation*}
		\begin{split}
			\int_{\delta_J / 2}^{\Delta/2} \log N(\delta, \cT) \dd{\delta} 
			& \precsim m \left[\sum_{k=1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) \right] \cdot \log \left(\frac{m}{\min_{k \in [\mathrm{dim}(\cY)]} {\rm Pdim}(\cF_{k}) } \right) \\
			& \le m \log(m) \left[\sum_{k=1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) \right] \;.
		\end{split} 
	\end{equation*}
	Analogously, 
	\begin{equation*}
		\int_{\delta_J / 2}^{\Delta/2} \sqrt{2 \log N(\delta, \cT)} \dd{\delta} 
		\precsim m \log(m) \left[\sum_{k=1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) \right] \;.
	\end{equation*}
	Thus,
	\begin{align*}
		R_m(\cH_y(1)\circ \cF, \{x_i\}_{i=1}^{m}) \precsim \frac{1}{m} \left[ m K + \E_{g} \sup_{F \in \cF}  g^\top A_F g \right]^{1/2}
		\precsim \sqrt{\frac{\log m}{m} \sum_{k =1}^{{\rm dim}(\cY)} {\rm Pdim}(\cF_{k}) }
	\end{align*}
	
	The same argument can be applied to the other three Rademacher complexities. Hence, we have proved the proposition.
\end{proof}


\subsection{Auxiliary Lemmas}
\begin{lemma}[Theorem 4.10 of \cite{wainwright_2019}]
	\label{lem:bound}
	Let $(\cZ, \rho)$ be a probability space and $\cG$ be a class of $b$-uniformly bounded measurable functions defined on $\cZ$, that is, $\sup_{g \in \cG} \|g\|_\infty \le b$. Let $z_1, \ldots, z_m$ are i.i.d.\ samples from $\rho$ and let $\widehat{\rho}_m$ be the empirical measure constructed from them. Then, for any $\delta > 0$,
	\begin{equation*}
		\sup_{g \in \cG} \left|\int g \dd{\widehat{\rho}_m} - \int g \dd{\rho} \right| \le 2 R_m(\cG, \rho) + \sqrt{\frac{2 b^2 \log(1 / \delta)}{m}}
	\end{equation*}
	holds with probability at least $1 - \delta$.
\end{lemma}

\begin{lemma}[Theorem 12.2 of \cite{anthony_bartlett_1999}]
	\label{lem:uniform_covering_to_pdim}
	Let $\cG$ be a collection of real-valued functions defined on a set $\cZ$. Suppose $\sup_{g \in \cG} ||g||_\infty = b < \infty$. For $\epsilon > 0$ and $m \ge \mathrm{Pdim}(\cG)$,  
	\begin{equation*}
		N_\infty(\epsilon, \cG, m) \le \left( \frac{2 e m b}{\epsilon \cdot {\rm Pdim}(\cG)} \right)^{{\rm Pdim}(\cG)} \;.
	\end{equation*}
\end{lemma}


\begin{lemma}[Example 2.12 of \cite{boucheron_lugosi_massart_2013}]
	 For any $A \in \mathbb{S}_0^{m \times m}$ and $0 \leq \lambda < 1/(2\|A\|_{\mathrm{op}})$,
	\begin{align}
		\log \E_g e^{\lambda g^\top A g} \leq \frac{\lambda^2 \|A \|}{1- 2\lambda \| A \|_{\mathrm{op}}} \;,
	\end{align}
	where $g \sim N(0, I_m)$. Here, $\| \cdot \|_{\mathrm{op}}$ denotes the operator norm of $A$.
\end{lemma}
This lemma tells that $g^\top A g$ is a sub-Gamma random variable with variance factor $2 \|A\|^2$ and scale parameter $2 \|A\|_{\mathrm{op}}$ (see Chapter 2.4 of \cite{boucheron_lugosi_massart_2013} for the definition). Using Corollary 2.6 of the same text, we can derive the following maximal inequality.
\begin{lemma}[Maximal inequality]
	\label{lem:maximal-ineq}
	For $A_1, \ldots, A_N \in \mathbb{S}_0^{m \times m}$, suppose $\max_{i = 1, \ldots, N} \| A_i \| \leq \delta$. Then,
	\begin{align}
		\E_{g} \max_{i = 1, \ldots, N} g^\top A_i g \leq 2 \delta \left( \sqrt{\log N} + \log N \right) \;,
	\end{align}
	where $g \sim N(0, I_m)$.
\end{lemma}



\end{document}
