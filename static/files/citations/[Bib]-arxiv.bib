
@article{farrell2020DeepLearning,
  title = {Deep {{Learning}} for {{Individual Heterogeneity}}},
  author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
  year = {2020},
  month = oct,
  abstract = {We propose a methodology for effectively modeling individual heterogeneity using deep learning while still retaining the interpretability and economic discipline of classical models. We pair a transparent, interpretable modeling structure with rich data environments and machine learning methods to estimate heterogeneous parameters based on potentially high dimensional or complex observable characteristics. Our framework is widely-applicable, covering numerous settings of economic interest. We recover, as special cases, well-known examples such as average treatment effects and parametric components of partially linear models. However, we also seamlessly deliver new results for diverse examples such as price elasticities, willingness-to-pay, and surplus measures in choice models, average marginal and partial effects of continuous treatment variables, fractional outcome models, count data, heterogeneous production function components, and more. Deep neural networks are well-suited to structured modeling of heterogeneity: we show how the network architecture can be designed to match the global structure of the economic model, giving novel methodology for deep learning as well as, more formally, improved rates of convergence. Our results on deep learning have consequences for other structured modeling environments and applications, such as for additive models. Our inference results are based on an influence function we derive, which we show to be flexible enough to to encompass all settings with a single, unified calculation, removing any requirement for case-by-case derivations. The usefulness of the methodology in economics is shown in two empirical applications: the response of 410(k) participation rates to firm matching and the impact of prices on subscription choices for an online service. Extensions to instrumental variables and multinomial choices are shown.},
  file = {/Users/tengyuan/Zotero/storage/M56VYJBP/Farrell et al_2020_Deep Learning for Individual Heterogeneity.pdf;/Users/tengyuan/Zotero/storage/6ZQG9YJJ/2010.html},
  language = {en}
}

@article{liang2018HowWell,
  ids = {liang2018HowWellGenerative},
  title = {On How Well Generative Adversarial Networks Learn Distributions},
  author = {Liang, Tengyuan},
  year = {2018},
  month = nov,
  archivePrefix = {arXiv},
  date-added = {2020-07-24 09:11:18 -0500},
  date-modified = {2020-07-24 09:11:18 -0500},
  eprint = {1811.03179},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/KP5RM7IG/Liang_2018_On how well generative adversarial networks learn densities.pdf},
  journal = {arXiv preprint arXiv:1811.03179},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{liang2019EstimatingCertain,
  ids = {liang2019EstimatingCertainIntegrala},
  title = {Estimating Certain Integral Probability Metric ({{IPM}}) Is as Hard as Estimating under the {{IPM}}},
  author = {Liang, Tengyuan},
  year = {2019},
  month = nov,
  archivePrefix = {arXiv},
  date-added = {2020-07-24 09:09:58 -0500},
  date-modified = {2020-07-24 09:09:58 -0500},
  eprint = {1911.00730},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/FHRRQJCR/Liang_2019_Estimating certain integral probability metric (IPM) is as hard as estimating.pdf},
  journal = {arXiv preprint arXiv:1911.00730},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{liang2020PreciseHighdimensional,
  ids = {liang2020PreciseHighDimensionalAsymptotic,liang2020PreciseHighDimensionalAsymptotica},
  title = {A Precise High-Dimensional Asymptotic Theory for Boosting and Min-L1-Norm Interpolated Classifiers},
  author = {Liang, Tengyuan and Sur, Pragya},
  year = {2020},
  month = feb,
  archivePrefix = {arXiv},
  date-added = {2020-07-24 09:09:37 -0500},
  date-modified = {2020-07-24 09:09:37 -0500},
  eprint = {2002.01586},
  eprinttype = {arxiv},
  file = {/Users/tengyuan/Zotero/storage/WAWF6WEZ/Liang_Sur_2020_A precise high-dimensional asymptotic theory for boosting and min-l1-norm.pdf},
  journal = {arXiv preprint arXiv:2002.01586},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{liang2021InterpolatingClassifiers,
  title = {Interpolating {{Classifiers Make Few Mistakes}}},
  author = {Liang, Tengyuan and Recht, Benjamin},
  year = {2021},
  month = jan,
  abstract = {This paper provides elementary analyses of the regret and generalization of minimum-norm interpolating classifiers (MNIC). The MNIC is the function of smallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a label pattern on a finite data set. We derive a mistake bound for MNIC and a regularized variant that holds for all data sets. This bound follows from elementary properties of matrix inverses. Under the assumption that the data is independently and identically distributed, the mistake bound implies that MNIC generalizes at a rate proportional to the norm of the interpolating solution and inversely proportional to the number of data points. This rate matches similar rates derived for margin classifiers and perceptrons. We derive several plausible generative models where the norm of the interpolating classifier is bounded or grows at a rate sublinear in \$n\$. We also show that as long as the population class conditional distributions are sufficiently separable in total variation, then MNIC generalizes with a fast rate.},
  file = {/Users/tengyuan/Zotero/storage/8NZLWZHT/Liang_Recht_2021_Interpolating Classifiers Make Few Mistakes.pdf;/Users/tengyuan/Zotero/storage/JJ9BLCE8/2101.html},
  language = {en}
}


