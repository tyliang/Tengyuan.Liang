%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tengyuan Liang at 2020-07-23 11:14:10 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{pmlr-v125-liang20a,
	Abstract = { We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of $d = n^{\alpha}$, $\alpha\in(0,1)$, for the input dimension $d$ and sample size $n$. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore,  locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a  minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.  },
	Author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
	Booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	Date-Added = {2020-07-23 11:14:09 -0500},
	Date-Modified = {2020-07-23 11:14:09 -0500},
	Editor = {Abernethy, Jacob and Agarwal, Shivani},
	Month = {09--12 Jul},
	Pages = {2683--2711},
	Pdf = {http://proceedings.mlr.press/v125/liang20a/liang20a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry of Kernels},
	Url = {http://proceedings.mlr.press/v125/liang20a.html},
	Volume = {125},
	Year = {2020},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v125/liang20a.html}}

@inproceedings{pmlr-v40-Belloni15,
	Abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in \mathbbR^n  using only function evaluations. The problem is reduced to sampling from an \emphapproximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \mathcalO^* complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an ε-minimizer after \mathcalO^*(n^7.5ε^-2) noisy function evaluations  by inducing a \mathcalO(ε/n)-approximately log concave distribution. We also consider in detail the case when the ``amount of non-convexity'' decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
	Address = {Paris, France},
	Author = {Alexandre Belloni and Tengyuan Liang and Hariharan Narayanan and Alexander Rakhlin},
	Booktitle = {Proceedings of The 28th Conference on Learning Theory},
	Date-Added = {2020-07-22 20:53:46 -0500},
	Date-Modified = {2020-07-22 20:53:46 -0500},
	Editor = {Peter Gr{\"u}nwald and Elad Hazan and Satyen Kale},
	Month = {03--06 Jul},
	Pages = {240--265},
	Pdf = {http://proceedings.mlr.press/v40/Belloni15.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions},
	Url = {http://proceedings.mlr.press/v40/Belloni15.html},
	Volume = {40},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v40/Belloni15.html}}

@inproceedings{pmlr-v40-Liang15,
	Abstract = {We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of \citepRakSriTsy15 for the bounded case while also providing guarantees without the boundedness assumption.},
	Address = {Paris, France},
	Author = {Tengyuan Liang and Alexander Rakhlin and Karthik Sridharan},
	Booktitle = {Proceedings of The 28th Conference on Learning Theory},
	Date-Added = {2020-07-22 20:53:40 -0500},
	Date-Modified = {2020-07-22 20:53:40 -0500},
	Editor = {Peter Gr{\"u}nwald and Elad Hazan and Satyen Kale},
	Month = {03--06 Jul},
	Pages = {1260--1285},
	Pdf = {http://proceedings.mlr.press/v40/Liang15.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Learning with Square Loss: Localization through Offset Rademacher Complexity},
	Url = {http://proceedings.mlr.press/v40/Liang15.html},
	Volume = {40},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v40/Liang15.html}}

@inproceedings{pmlr-v70-kale17a,
	Abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Satyen Kale and Zohar Karnin and Tengyuan Liang and D{\'a}vid P{\'a}l},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Date-Added = {2020-07-22 20:53:32 -0500},
	Date-Modified = {2020-07-22 20:53:32 -0500},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {1780--1788},
	Pdf = {http://proceedings.mlr.press/v70/kale17a/kale17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under {RIP}},
	Url = {http://proceedings.mlr.press/v70/kale17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/kale17a.html}}

@inproceedings{pmlr-v75-tzen18a,
	Abstract = {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\varepsilon$-neighborhood of this particular optimum within a short \textit{recurrence time}; (2) it enters this $\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \textit{escape time}. We call this phenomenon \textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality. },
	Author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
	Booktitle = {Proceedings of the 31st Conference On Learning Theory},
	Date-Added = {2020-07-22 20:53:24 -0500},
	Date-Modified = {2020-07-22 20:53:24 -0500},
	Editor = {Bubeck, S\'ebastien and Perchet, Vianney and Rigollet, Philippe},
	Month = {06--09 Jul},
	Pages = {857--875},
	Pdf = {http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},
	Url = {http://proceedings.mlr.press/v75/tzen18a.html},
	Volume = {75},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v75/tzen18a.html}}

@inproceedings{pmlr-v89-liang19b,
	Abstract = {Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.},
	Author = {Liang, Tengyuan and Stokes, James},
	Booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2020-07-22 20:53:17 -0500},
	Date-Modified = {2020-07-22 20:59:36 -0500},
	Editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	Month = {16--18 Apr},
	Pages = {907--915},
	Pdf = {http://proceedings.mlr.press/v89/liang19b/liang19b.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks},
	Url = {http://proceedings.mlr.press/v89/liang19b.html},
	Volume = {89},
	Year = {2019},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v89/liang19b.html}}

@inproceedings{pmlr-v89-liang19a,
	Abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity --- the Fisher-Rao norm --- that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
	Author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
	Booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	Date-Modified = {2020-07-22 20:59:43 -0500},
	Editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	Month = {16--18 Apr},
	Pages = {888--896},
	Pdf = {http://proceedings.mlr.press/v89/liang19a/liang19a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks},
	Url = {http://proceedings.mlr.press/v89/liang19a.html},
	Volume = {89},
	Year = {2019},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v89/liang19a.html}}
