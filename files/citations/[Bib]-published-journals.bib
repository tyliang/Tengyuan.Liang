%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Tengyuan Liang at 2021-09-07 11:08:32 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{L2021JMLR,
	author = {Liang, Tengyuan},
	date-added = {2021-09-07 11:08:06 -0500},
	date-modified = {2021-09-07 11:08:18 -0500},
	journal = {Journal of Machine Learning Research, to appear},
	title = {How well generative adversarial networks learn distributions},
	year = {2021}}

@article{DL2021JASA,
	author = {Xialiang Dou and Tengyuan Liang},
	date-added = {2021-09-07 11:05:55 -0500},
	date-modified = {2021-09-07 11:06:36 -0500},
	doi = {10.1080/01621459.2020.1745812},
	eprint = {https://doi.org/10.1080/01621459.2020.1745812},
	journal = {Journal of the American Statistical Association},
	number = {535},
	pages = {1507-1520},
	publisher = {Taylor & Francis},
	title = {Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits},
	url = {https://doi.org/10.1080/01621459.2020.1745812},
	volume = {116},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2020.1745812}}

@article{LT2020JASA,
	abstract = { Abstract--We use a connection between compositional kernels and branching processes via Mehler's formula to study deep neural networks. This new probabilistic insight provides us a novel perspective on the mathematical role of activation functions in compositional neural networks. We study the unscaled and rescaled limits of the compositional kernels and explore the different phases of the limiting behavior, as the compositional depth increases. We investigate the memorization capacity of the compositional kernels and neural networks by characterizing the interplay among compositional depth, sample size, dimensionality, and nonlinearity of the activation. Explicit formulas on the eigenvalues of the compositional kernel are provided, which quantify the complexity of the corresponding reproducing kernel Hilbert space. On the methodological front, we propose a new random features algorithm, which compresses the compositional layers by devising a new activation function. Supplementary materials for this article are available online. },
	author = {Tengyuan Liang and Hai Tran-Bach},
	date-added = {2021-01-27 13:27:38 -0600},
	date-modified = {2021-01-27 13:27:56 -0600},
	doi = {10.1080/01621459.2020.1853547},
	eprint = {https://doi.org/10.1080/01621459.2020.1853547},
	journal = {Journal of the American Statistical Association},
	number = {0},
	pages = {1-14},
	publisher = {Taylor & Francis},
	title = {Mehler's Formula, Branching Process, and Compositional Kernels of Deep Neural Networks},
	url = {https://doi.org/10.1080/01621459.2020.1853547},
	volume = {0},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2020.1853547}}

@article{FLM2021ECMA,
	abstract = {We study deep neural networks and their use in semiparametric inference. We establish novel nonasymptotic high probability bounds for deep feedforward neural nets. These deliver rates of convergence that are sufficiently fast (in some cases minimax optimal) to allow us to establish valid second-step inference after first-step estimation with deep learning, a result also new to the literature. Our nonasymptotic high probability bounds, and the subsequent semiparametric inference, treat the current standard architecture: fully connected feedforward neural networks (multilayer perceptrons), with the now-common rectified linear unit activation function, unbounded weights, and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish the nonasymptotic bounds for these deep nets for a general class of nonparametric regression-type loss functions, which includes as special cases least squares, logistic regression, and other generalized linear models. We then apply our theory to develop semiparametric inference, focusing on causal parameters for concreteness, and demonstrate the effectiveness of deep learning with an empirical application to direct mail marketing.},
	author = {Farrell, Max H. and Liang, Tengyuan and Misra, Sanjog},
	date-added = {2021-01-25 10:40:09 -0600},
	date-modified = {2021-01-25 10:40:32 -0600},
	doi = {https://doi.org/10.3982/ECTA16901},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA16901},
	journal = {Econometrica},
	keywords = {Deep learning, neural networks, rectified linear unit, nonasymptotic bounds, convergence rates, semiparametric inference, treatment effects, program evaluation},
	number = {1},
	pages = {181-213},
	title = {Deep Neural Networks for Estimation and Inference},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901},
	volume = {89},
	year = {2021},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA16901},
	Bdsk-Url-2 = {https://doi.org/10.3982/ECTA16901}}

@article{CLZ2015JMVA,
	abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n→0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)>n is also discussed.},
	author = {T. Tony Cai and Tengyuan Liang and Harrison H. Zhou},
	date-added = {2020-07-22 21:21:30 -0500},
	date-modified = {2020-08-06 22:45:54 -0500},
	doi = {10.1016/j.jmva.2015.02.003},
	issn = {0047-259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Asymptotic optimality, Central limit theorem, Covariance matrix, Determinant, Differential entropy, Minimax lower bound, Sharp minimaxity},
	pages = {161 - 172},
	title = {Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
	volume = {137},
	year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jmva.2015.02.003}}

@article{CLR2016AOS,
	author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:20:52 -0500},
	date-modified = {2020-07-22 21:20:52 -0500},
	doi = {10.1214/15-AOS1426},
	fjournal = {Annals of Statistics},
	journal = {Ann. Statist.},
	month = {08},
	number = {4},
	pages = {1536--1563},
	publisher = {The Institute of Mathematical Statistics},
	title = {Geometric inference for general high-dimensional linear inverse problems},
	url = {https://doi.org/10.1214/15-AOS1426},
	volume = {44},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1214/15-AOS1426}}

@article{CLR2017TNSE,
	abstract = {In this paper, we study detection and fast reconstruction of the celebrated Watts-Strogatz (WS) small-world random graph model [29] which aims to describe real-world complex networks that exhibit both high clustering and short average length properties. The WS model with neighborhood size k and rewiring probability probability β can be viewed as a continuous interpolation between a deterministic ring lattice graph and the Erdos-Renyi random graph. We study the computational and statistical aspects of detection and recovery of the deterministic ring lattice structure (strong ties) in the presence of random connections (weak ties). The phase diagram in terms of (k, β) is shown to consist of several regions according to the difficulty of the problem. We propose distinct methods for these regions.},
	author = {T. {Cai} and T. {Liang} and A. {Rakhlin}},
	date-added = {2020-07-22 21:20:28 -0500},
	date-modified = {2020-07-22 21:20:28 -0500},
	doi = {10.1109/TNSE.2017.2703102},
	issn = {2327-4697},
	journal = {IEEE Transactions on Network Science and Engineering},
	keywords = {graph theory;interpolation;fast reconstruction;Watts-Strogatz small-world random graph model;real-world complex networks;WS model;Erdos-Renyi random graph;continuous interpolation;deterministic ring lattice graph;Lattices;Symmetric matrices;Structural rings;Computational modeling;Mathematical model;Complex networks;Testing;Small world networks;random graphs;spectral analysis;detection;reconstruction;computational boundary},
	month = {July},
	number = {3},
	pages = {165-176},
	title = {On Detection and Structural Reconstruction of Small-World Random Networks},
	volume = {4},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNSE.2017.2703102}}

@article{CLR2017AOS,
	author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:19:55 -0500},
	date-modified = {2020-07-22 21:19:55 -0500},
	doi = {10.1214/16-AOS1488},
	fjournal = {Annals of Statistics},
	journal = {Ann. Statist.},
	month = {08},
	number = {4},
	pages = {1403--1430},
	publisher = {The Institute of Mathematical Statistics},
	title = {Computational and statistical boundaries for submatrix localization in a large noisy matrix},
	url = {https://doi.org/10.1214/16-AOS1488},
	volume = {45},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1214/16-AOS1488}}

@article{LS2019JRSSB,
	abstract = {Summary Modern statistical inference tasks often require iterative optimization methods to compute the solution. Convergence analysis from an optimization viewpoint informs us only how well the solution is approximated numerically but overlooks the sampling nature of the data. In contrast, recognizing the randomness in the data, statisticians are keen to provide uncertainty quantification, or confidence, for the solution obtained by using iterative optimization methods. The paper makes progress along this direction by introducing moment-adjusted stochastic gradient descent: a new stochastic optimization method for statistical inference. We establish non-asymptotic theory that characterizes the statistical distribution for certain iterative methods with optimization guarantees. On the statistical front, the theory allows for model misspecification, with very mild conditions on the data. For optimization, the theory is flexible for both convex and non-convex cases. Remarkably, the moment adjusting idea motivated from `error standardization' in statistics achieves a similar effect to acceleration in first-order optimization methods that are used to fit generalized linear models. We also demonstrate this acceleration effect in the non-convex setting through numerical experiments.},
	author = {Liang, Tengyuan and Su, Weijie J.},
	date-added = {2020-07-22 21:17:08 -0500},
	date-modified = {2020-07-22 21:17:08 -0500},
	doi = {10.1111/rssb.12313},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12313},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Acceleration, Diffusion process, Discretized Langevin algorithm, Model misspecification, Non-asymptotic inference, Population landscape, Stochastic gradient methods},
	number = {2},
	pages = {431-456},
	title = {Statistical inference for the population landscape via moment-adjusted stochastic gradients},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12313},
	volume = {81},
	year = {2019},
	Bdsk-Url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12313},
	Bdsk-Url-2 = {https://doi.org/10.1111/rssb.12313}}

@article{CLR2020JMLR,
	author = {T. Tony Cai and Tengyuan Liang and Alexander Rakhlin},
	date-added = {2020-07-22 21:16:15 -0500},
	date-modified = {2020-07-22 21:16:15 -0500},
	journal = {Journal of Machine Learning Research},
	number = {11},
	pages = {1-34},
	title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
	url = {http://jmlr.org/papers/v21/18-573.html},
	volume = {21},
	year = {2020},
	Bdsk-Url-1 = {http://jmlr.org/papers/v21/18-573.html}}

@article{LR2020AOS,
	author = {Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:12:30 -0500},
	date-modified = {2020-07-22 21:12:30 -0500},
	doi = {10.1214/19-AOS1849},
	fjournal = {Annals of Statistics},
	journal = {Ann. Statist.},
	month = {06},
	number = {3},
	pages = {1329--1347},
	publisher = {The Institute of Mathematical Statistics},
	title = {Just interpolate: Kernel ``Ridgeless'' regression can generalize},
	url = {https://doi.org/10.1214/19-AOS1849},
	volume = {48},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1214/19-AOS1849}}
