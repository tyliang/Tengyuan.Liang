%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Liang, Tengyuan at 2019-04-12 11:19:03 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{kale2017adaptive,
	Abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
	Author = {Satyen Kale and Zohar Karnin and Tengyuan Liang and D{\'a}vid P{\'a}l},
	Booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Pages = {1780--1788},
	Publisher = {PMLR},
	Title = {Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under {RIP}},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/kale17a.html}}

@article{cai2017computational,
	Author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Fjournal = {The Annals of Statistics},
	Journal = {Ann. Statist.},
	Number = {4},
	Pages = {1403--1430},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Computational and statistical boundaries for submatrix localization in a large noisy matrix},
	Volume = {45},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1214/16-AOS1488}}

@article{farrell2018deep,
	Author = {Farrell, Max H and Liang, Tengyuan and Misra, Sanjog},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1809.09953},
	Title = {Deep neural networks for estimation and inference: Application to causal effects and other semiparametric estimands},
	Year = {2018}}

@inproceedings{belloni2015escape,
	Abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in \mathbbR^n  using only function evaluations. The problem is reduced to sampling from an \emphapproximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \mathcalO^* complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an ε-minimizer after \mathcalO^*(n^7.5ε^-2) noisy function evaluations  by inducing a \mathcalO(ε/n)-approximately log concave distribution. We also consider in detail the case when the ``amount of non-convexity'' decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
	Author = {Alexandre Belloni and Tengyuan Liang and Hariharan Narayanan and Alexander Rakhlin},
	Booktitle = {Proceedings of The Conference on Learning Theory (COLT)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Pages = {240--265},
	Publisher = {PMLR},
	Title = {Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions},
	Volume = {40},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v40/Belloni15.html}}

@inproceedings{liang2017fisher,
	Author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
	Booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:09:23 -0500},
	Publisher = {PMLR},
	Title = {Fisher-rao metric, geometry, and complexity of neural networks},
	Volume = {89},
	Year = {2019}}

@article{cai2016geometric,
	Author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Fjournal = {The Annals of Statistics},
	Journal = {Ann. Statist.},
	Number = {4},
	Pages = {1536--1563},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Geometric inference for general high-dimensional linear inverse problems},
	Volume = {44},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1214/15-AOS1426}}

@article{farrell2019growing,
	Author = {Farrell, Max H and Liang, Tengyuan and Shaikh, Azeem},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {working paper},
	Title = {Growing Random Forests as Learning Adaptive Kernels},
	Year = {2019}}

@article{liang2017well,
	Author = {Liang, Tengyuan},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1712.08244},
	Title = {How well can generative adversarial networks learn densities: A nonparametric view},
	Year = {2017}}

@article{cai2016inference,
	Author = {Cai, T Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1603.06923},
	Title = {Inference via message passing on partially labeled stochastic block models},
	Year = {2016}}

@inproceedings{liang2018interaction,
	Author = {Liang, Tengyuan and Stokes, James},
	Booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:09:41 -0500},
	Publisher = {PMLR},
	Title = {Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks},
	Volume = {89},
	Year = {2019}}

@article{liang2018just,
	Author = {Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {The Annals of Statistics, to appear},
	Title = {Just interpolate: Kernel" ridgeless" regression can generalize},
	Year = {2019}}

@article{cai2015law,
	Abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n→0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)>n is also discussed.},
	Author = {T. Tony Cai and Tengyuan Liang and Harrison H. Zhou},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {Journal of Multivariate Analysis},
	Keywords = {Asymptotic optimality, Central limit theorem, Covariance matrix, Determinant, Differential entropy, Minimax lower bound, Sharp minimaxity},
	Pages = {161-172},
	Title = {Law of log determinant of sample covariance matrix and optimal estimation of differential entropy for high-dimensional Gaussian distributions},
	Volume = {137},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0047259X1500038X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jmva.2015.02.003}}

@inproceedings{liang2015learn,
	Abstract = {We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of \citepRakSriTsy15 for the bounded case while also providing guarantees without the boundedness assumption.},
	Author = {Tengyuan Liang and Alexander Rakhlin and Karthik Sridharan},
	Booktitle = {Proceedings of The Conference on Learning Theory (COLT)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Pages = {1260--1285},
	Publisher = {PMLR},
	Title = {Learning with Square Loss: Localization through Offset Rademacher Complexity},
	Volume = {40},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v40/Liang15.html}}

@inproceedings{tzen2018local,
	Abstract = {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \textit{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the $\varepsilon$-neighborhood of this particular optimum within a short \textit{recurrence time}; (2) it enters this $\varepsilon$-neighborhood by the recurrence time and stays there until a potentially exponentially long \textit{escape time}. We call this phenomenon \textit{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality. },
	Author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
	Booktitle = {Proceedings of the Conference On Learning Theory (COLT)},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Pages = {857--875},
	Publisher = {PMLR},
	Title = {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},
	Volume = {75},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v75/tzen18a.html}}

@article{cai2017detection,
	Author = {T. Tony Cai and Tengyuan Liang and Alexander Rakhlin},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {IEEE Transactions on Network Science and Engineering},
	Keywords = {graph theory;interpolation;fast reconstruction;Watts-Strogatz small-world random graph model;real-world complex networks;WS model;Erdos-Renyi random graph;continuous interpolation;deterministic ring lattice graph;Lattices;Symmetric matrices;Structural rings;Computational modeling;Mathematical model;Complex networks;Testing;Small world networks;random graphs;spectral analysis;detection;reconstruction;computational boundary},
	Number = {3},
	Pages = {165-176},
	Title = {On Detection and Structural Reconstruction of Small-World Random Networks},
	Volume = {4},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNSE.2017.2703102}}

@article{liang2018well,
	Author = {Liang, Tengyuan},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1811.03179},
	Title = {On How Well Generative Adversarial Networks Learn Densities: Nonparametric and Parametric Results},
	Year = {2018}}

@article{liang2019statistical,
	Abstract = {Summary Modern statistical inference tasks often require iterative optimization methods to compute the solution. Convergence analysis from an optimization viewpoint informs us only how well the solution is approximated numerically but overlooks the sampling nature of the data. In contrast, recognizing the randomness in the data, statisticians are keen to provide uncertainty quantification, or confidence, for the solution obtained by using iterative optimization methods. The paper makes progress along this direction by introducing moment-adjusted stochastic gradient descent: a new stochastic optimization method for statistical inference. We establish non-asymptotic theory that characterizes the statistical distribution for certain iterative methods with optimization guarantees. On the statistical front, the theory allows for model misspecification, with very mild conditions on the data. For optimization, the theory is flexible for both convex and non-convex cases. Remarkably, the moment adjusting idea motivated from `error standardization' in statistics achieves a similar effect to acceleration in first-order optimization methods that are used to fit generalized linear models. We also demonstrate this acceleration effect in the non-convex setting through numerical experiments.},
	Author = {Liang, Tengyuan and Su, Weijie J.},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Keywords = {Acceleration, Diffusion process, Discretized Langevin algorithm, Model misspecification, Non-asymptotic inference, Population landscape, Stochastic gradient methods},
	Number = {2},
	Pages = {431-456},
	Title = {Statistical inference for the population landscape via moment-adjusted stochastic gradients},
	Volume = {81},
	Year = {2019},
	Bdsk-Url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12313},
	Bdsk-Url-2 = {https://doi.org/10.1111/rssb.12313}}

@article{cong2018textual,
	Author = {Cong, Lin William and Liang, Tengyuan and Zhang, Xiao},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {SSRN: https://ssrn.com/abstract=3307057},
	Title = {Textual Factors: A Scalable, Interpretable, and Data-driven Approach to Analyzing Unstructured Information},
	Year = {2018}}

@article{dou2019training,
	Author = {Dou, Xialiang and Liang, Tengyuan},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1901.07114},
	Title = {Training Neural Networks as Learning Data-adaptive Kernels: Provable Representation and Approximation Benefits},
	Year = {2019}}

@article{cai2017weighted,
	Author = {Cai, T Tony and Liang, Tengyuan and Rakhlin, Alexander},
	Date-Added = {2019-04-12 11:00:56 -0500},
	Date-Modified = {2019-04-12 11:00:56 -0500},
	Journal = {arXiv preprint arXiv:1709.03907},
	Title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
	Year = {2017}}
