%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for tengyuan at 2020-11-26 11:50:28 -0600 


%% Saved with string encoding Unicode (UTF-8) 



@article{farrell2018DeepNeural,
	author = {Farrell, Max H and Liang, Tengyuan and Misra, Sanjog},
	date-added = {2020-11-26 11:45:56 -0600},
	date-modified = {2020-11-26 11:50:28 -0600},
	doi = {10.3982/ECTA16901},
	journal = {Econometrica, forthcoming},
	month = {August},
	pubstate = {forthcoming},
	title = {Deep neural networks for estimation and inference},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.3982/ECTA16901}}

@article{liang2020MehlerFormula,
	author = {Liang, Tengyuan and Tran-Bach, Hai},
	date-added = {2020-11-26 11:45:42 -0600},
	date-modified = {2020-11-26 11:50:23 -0600},
	doi = {10.1080/01621459.2020.1853547},
	journal = {Journal of the American Statistical Association, forthcoming},
	month = {November},
	pubstate = {forthcoming},
	title = {Mehler's Formula, Branching Process, and Compositional Kernels of Deep Neural Networks},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2020.1853547}}

@inproceedings{belloni2015EscapingLocal,
	abstract = {We consider the problem of optimizing an approximately convex function over a bounded convex set in \textsuperscript{n} using only function evaluations. The problem is reduced to sampling from an log-concave distribution using the Hit-and-Run method, which is shown to have the same \textsuperscript{*} complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an {$\epsilon$}-minimizer after \textsuperscript{*}(n{$^7$}.5{$\epsilon^-$}2) noisy function evaluations by inducing a ({$\epsilon$}/n)-approximately log concave distribution. We also consider in detail the case when the ``amount of non-convexity'' decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.},
	address = {{Paris, France}},
	author = {Belloni, Alexandre and Liang, Tengyuan and Narayanan, Hariharan and Rakhlin, Alexander},
	booktitle = {Proceedings of the 28th Conference on Learning Theory},
	date-added = {2020-07-22 20:53:46 -0500},
	date-modified = {2020-07-22 20:53:46 -0500},
	editor = {Gr{\"u}nwald, Peter and Hazan, Elad and Kale, Satyen},
	file = {/Users/tliang/Zotero/storage/HQ8DB3SD/Belloni et al_2015_Escaping the local minima via simulated annealing.pdf},
	month = jul,
	pages = {240--265},
	pdf = {http://proceedings.mlr.press/v40/Belloni15.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Escaping the Local Minima via Simulated Annealing: {{Optimization}} of Approximately Convex Functions},
	volume = {40},
	year = {2015}}

@article{cai2015LawLog,
	abstract = {Differential entropy and log determinant of the covariance matrix of a multivariate Gaussian distribution have many applications in coding, communications, signal processing and statistical inference. In this paper we consider in the high-dimensional setting optimal estimation of the differential entropy and the log-determinant of the covariance matrix. We first establish a central limit theorem for the log determinant of the sample covariance matrix in the high-dimensional setting where the dimension p(n) can grow with the sample size n. An estimator of the differential entropy and the log determinant is then considered. Optimal rate of convergence is obtained. It is shown that in the case p(n)/n\textrightarrow 0 the estimator is asymptotically sharp minimax. The ultra-high-dimensional setting where p(n)\textquestiondown n is also discussed.},
	author = {Cai, T. Tony and Liang, Tengyuan and Zhou, Harrison H.},
	date-added = {2020-07-22 21:21:30 -0500},
	date-modified = {2020-07-22 21:21:30 -0500},
	doi = {10.1016/j.jmva.2015.02.003},
	file = {/Users/tliang/Zotero/storage/CEQIRPAX/Cai et al_2015_Law of log determinant of sample covariance matrix and optimal estimation of.pdf},
	ids = {cai2015LawLog},
	issn = {0047-259X},
	journal = {Journal of Multivariate Analysis},
	keywords = {Asymptotic optimality,Central limit theorem,Covariance matrix,Determinant,Differential entropy,Minimax lower bound,Sharp minimaxity},
	pages = {161--172},
	title = {Law of Log Determinant of Sample Covariance Matrix and Optimal Estimation of Differential Entropy for High-Dimensional {{Gaussian}} Distributions},
	volume = {137},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jmva.2015.02.003},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jmva.2015.02.003}}

@article{cai2016GeometricInference,
	author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:20:52 -0500},
	date-modified = {2020-07-22 21:20:52 -0500},
	doi = {10.1214/15-AOS1426},
	file = {/Users/tliang/Zotero/storage/37ZGDJYK/Cai et al_2016_Geometric inference for general high-dimensional linear inverse problems.pdf},
	fjournal = {Annals of Statistics},
	ids = {cai2016GeometricInference},
	journal = {The Annals of Statistics},
	month = aug,
	number = {4},
	pages = {1536--1563},
	publisher = {{The Institute of Mathematical Statistics}},
	title = {Geometric Inference for General High-Dimensional Linear Inverse Problems},
	volume = {44},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1214/15-AOS1426}}

@article{cai2017ComputationalStatistical,
	author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:19:55 -0500},
	date-modified = {2020-07-22 21:19:55 -0500},
	doi = {10.1214/16-AOS1488},
	file = {/Users/tliang/Zotero/storage/Z8UI2CYU/Cai et al_2017_Computational and statistical boundaries for submatrix localization in a large.pdf},
	fjournal = {Annals of Statistics},
	ids = {cai2017ComputationalStatistical},
	journal = {The Annals of Statistics},
	month = aug,
	number = {4},
	pages = {1403--1430},
	publisher = {{The Institute of Mathematical Statistics}},
	title = {Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix},
	volume = {45},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1214/16-AOS1488}}

@article{cai2017DetectionStructural,
	abstract = {In this paper, we study detection and fast reconstruction of the celebrated Watts-Strogatz (WS) small-world random graph model [29] which aims to describe real-world complex networks that exhibit both high clustering and short average length properties. The WS model with neighborhood size k and rewiring probability probability {$\beta$} can be viewed as a continuous interpolation between a deterministic ring lattice graph and the Erdos-Renyi random graph. We study the computational and statistical aspects of detection and recovery of the deterministic ring lattice structure (strong ties) in the presence of random connections (weak ties). The phase diagram in terms of (k, {$\beta$}) is shown to consist of several regions according to the difficulty of the problem. We propose distinct methods for these regions.},
	author = {Cai, T. and Liang, T. and Rakhlin, A.},
	date-added = {2020-07-22 21:20:28 -0500},
	date-modified = {2020-07-22 21:20:28 -0500},
	doi = {10.1109/TNSE.2017.2703102},
	file = {/Users/tliang/Zotero/storage/C2XDICXK/Cai et al_2017_On detection and structural reconstruction of small-world random networks.pdf},
	ids = {cai2017DetectionStructural},
	issn = {2327-4697},
	journal = {IEEE Transactions on Network Science and Engineering},
	keywords = {Complex networks,computational boundary,Computational modeling,continuous interpolation,detection,deterministic ring lattice graph,Erdos-Renyi random graph,fast reconstruction,graph theory,interpolation,Lattices,Mathematical model,random graphs,real-world complex networks,reconstruction,Small world networks,spectral analysis,Structural rings,Symmetric matrices,Testing,Watts-Strogatz small-world random graph model,WS model},
	month = jul,
	number = {3},
	pages = {165--176},
	title = {On Detection and Structural Reconstruction of Small-World Random Networks},
	volume = {4},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNSE.2017.2703102}}

@article{cai2020WeightedMessage,
	author = {Cai, T. Tony and Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:16:15 -0500},
	date-modified = {2020-07-22 21:16:15 -0500},
	file = {/Users/tliang/Zotero/storage/Y88CIPSX/Cai et al_2020_Weighted message passing and minimum energy flow for heterogeneous stochastic.pdf},
	ids = {cai2020WeightedMessage},
	journal = {Journal of Machine Learning Research},
	number = {11},
	pages = {1--34},
	title = {Weighted Message Passing and Minimum Energy Flow for Heterogeneous Stochastic Block Models with Side Information},
	volume = {21},
	year = {2020}}

@article{dou2020TrainingNeural,
	abstract = {AbstractConsider the problem: given the data pair (x,y) drawn from a population with f*(x)=E[y|x=x], specify a neural network model and run gradient flow on the weights over time until reaching any stationarity. How does ft, the function computed by the neural network at time t, relate to f*, in terms of approximation and representation? What are the provable benefits of the adaptive representation by neural networks compared to the prespecified fixed basis representation in the classical nonparametric literature? We answer the above questions via a dynamic reproducing kernel Hilbert space (RKHS) approach indexed by the training process of neural networks. First, we show that when reaching any local stationarity, gradient flow learns an adaptive RKHS representation and performs the global least-squares projection onto the adaptive RKHS, simultaneously. Second, we prove that as the RKHS is data-adaptive and task-specific, the residual for f* lies in a subspace that is potentially much smaller than the orthogonal complement of the RKHS. The result formalizes the representation and approximation benefits of neural networks. Finally, we show that the neural network function computed by gradient flow converges to the kernel ridgeless regression with an adaptive kernel, in the limit of vanishing regularization. The adaptive kernel viewpoint provides new angles of studying the approximation, representation, generalization, and optimization advantages of neural networks. Supplementary materials for this article are available online.},
	author = {Dou, Xialiang and Liang, Tengyuan},
	date-added = {2020-07-22 21:18:24 -0500},
	date-modified = {2020-11-26 11:48:52 -0600},
	doi = {10.1080/01621459.2020.1745812},
	eprint = {https://doi.org/10.1080/01621459.2020.1745812},
	file = {/Users/tliang/Zotero/storage/CECJIH3G/Dou_Liang_2020_Training neural networks as learning data-adaptive kernels.pdf},
	ids = {dou2020TrainingNeural},
	journal = {Journal of the American Statistical Association},
	number = {0},
	pages = {1--14},
	publisher = {{Taylor \& Francis}},
	title = {Training Neural Networks as Learning Data-Adaptive Kernels: {{Provable}} Representation and Approximation Benefits},
	volume = {0},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1080/01621459.2020.1745812}}

@inproceedings{kale2017AdaptiveFeature,
	abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
	address = {{International Convention Centre, Sydney, Australia}},
	author = {Kale, Satyen and Karnin, Zohar and Liang, Tengyuan and P{\'a}l, D{\'a}vid},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2020-07-22 20:53:32 -0500},
	date-modified = {2020-07-22 20:53:32 -0500},
	editor = {Precup, Doina and Teh, Yee Whye},
	file = {/Users/tliang/Zotero/storage/SQAJKQC6/Kale et al_2017_Adaptive feature selection.pdf},
	month = aug,
	pages = {1780--1788},
	pdf = {http://proceedings.mlr.press/v70/kale17a/kale17a.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Adaptive Feature Selection: {{Computationally}} Efficient Online Sparse Linear Regression under {{RIP}}},
	volume = {70},
	year = {2017}}

@inproceedings{liang2015LearningSquare,
	abstract = {We consider regression with square loss and general classes of functions without the boundedness assumption. We introduce a notion of offset Rademacher complexity that provides a transparent way to study localization both in expectation and in high probability. For any (possibly non-convex) class, the excess loss of a two-step estimator is shown to be upper bounded by this offset complexity through a novel geometric inequality. In the convex case, the estimator reduces to an empirical risk minimizer. The method recovers the results of 15 for the bounded case while also providing guarantees without the boundedness assumption.},
	address = {{Paris, France}},
	author = {Liang, Tengyuan and Rakhlin, Alexander and Sridharan, Karthik},
	booktitle = {Proceedings of the 28th Conference on Learning Theory},
	date-added = {2020-07-22 20:53:40 -0500},
	date-modified = {2020-07-22 20:53:40 -0500},
	editor = {Gr{\"u}nwald, Peter and Hazan, Elad and Kale, Satyen},
	file = {/Users/tliang/Zotero/storage/NXEKPVND/Liang et al_2015_Learning with square loss.pdf},
	month = jul,
	pages = {1260--1285},
	pdf = {http://proceedings.mlr.press/v40/Liang15.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Learning with Square Loss: {{Localization}} through Offset Rademacher Complexity},
	volume = {40},
	year = {2015}}

@article{liang2018HowWell,
	archiveprefix = {arXiv},
	author = {Liang, Tengyuan},
	date-added = {2020-07-24 09:11:18 -0500},
	date-modified = {2020-07-24 09:11:18 -0500},
	eprint = {1811.03179},
	eprinttype = {arxiv},
	file = {/Users/tliang/Zotero/storage/KP5RM7IG/Liang_2018_On how well generative adversarial networks learn densities.pdf},
	ids = {liang2018HowWell,liang2018HowWellGenerative},
	journal = {arXiv preprint arXiv:1811.03179},
	keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
	month = nov,
	title = {On How Well Generative Adversarial Networks Learn Densities: {{Nonparametric}} and Parametric Results},
	year = {2018}}

@article{liang2019EstimatingCertain,
	archiveprefix = {arXiv},
	author = {Liang, Tengyuan},
	date-added = {2020-07-24 09:09:58 -0500},
	date-modified = {2020-07-24 09:09:58 -0500},
	eprint = {1911.00730},
	eprinttype = {arxiv},
	file = {/Users/tliang/Zotero/storage/FHRRQJCR/Liang_2019_Estimating certain integral probability metric (IPM) is as hard as estimating.pdf},
	ids = {liang2019EstimatingCertain,liang2019EstimatingCertainIntegrala},
	journal = {arXiv preprint arXiv:1911.00730},
	keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
	month = nov,
	title = {Estimating Certain Integral Probability Metric ({{IPM}}) Is as Hard as Estimating under the {{IPM}}},
	year = {2019}}

@inproceedings{liang2019FisherraoMetric,
	abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity \textemdash{} the Fisher-Rao norm \textemdash{} that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks.},
	author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	chapter = {Machine Learning},
	date-added = {2020-04-10 12:58:44 -0500},
	date-modified = {2020-07-22 20:59:43 -0500},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	file = {/Users/tliang/Zotero/storage/7NHSTYWQ/Liang et al_2019_Fisher-rao metric, geometry, and complexity of neural networks.pdf},
	ids = {liang2019FisherRaoMetric},
	issn = {1938-7228},
	month = apr,
	pages = {888--896},
	pdf = {http://proceedings.mlr.press/v89/liang19a/liang19a.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Fisher-Rao Metric, Geometry, and Complexity of Neural Networks},
	volume = {89},
	year = {2019}}

@inproceedings{liang2019InteractionMatters,
	abstract = {Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.},
	author = {Liang, Tengyuan and Stokes, James},
	booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
	chapter = {Machine Learning},
	date-added = {2020-07-22 20:53:17 -0500},
	date-modified = {2020-07-22 20:59:36 -0500},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	file = {/Users/tliang/Zotero/storage/7JN7WBZT/Liang_Stokes_2019_Interaction matters.pdf},
	ids = {liang2019InteractionMatter,liang2019InteractionMattersa},
	issn = {1938-7228},
	month = apr,
	pages = {907--915},
	pdf = {http://proceedings.mlr.press/v89/liang19b/liang19b.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Interaction Matters: {{A}} Note on Non-Asymptotic Local Convergence of Generative Adversarial Networks},
	volume = {89},
	year = {2019}}

@article{liang2019StatisticalInference,
	abstract = {Summary Modern statistical inference tasks often require iterative optimization methods to compute the solution. Convergence analysis from an optimization viewpoint informs us only how well the solution is approximated numerically but overlooks the sampling nature of the data. In contrast, recognizing the randomness in the data, statisticians are keen to provide uncertainty quantification, or confidence, for the solution obtained by using iterative optimization methods. The paper makes progress along this direction by introducing moment-adjusted stochastic gradient descent: a new stochastic optimization method for statistical inference. We establish non-asymptotic theory that characterizes the statistical distribution for certain iterative methods with optimization guarantees. On the statistical front, the theory allows for model misspecification, with very mild conditions on the data. For optimization, the theory is flexible for both convex and non-convex cases. Remarkably, the moment adjusting idea motivated from `error standardization' in statistics achieves a similar effect to acceleration in first-order optimization methods that are used to fit generalized linear models. We also demonstrate this acceleration effect in the non-convex setting through numerical experiments.},
	author = {Liang, Tengyuan and Su, Weijie J.},
	date-added = {2020-07-22 21:17:08 -0500},
	date-modified = {2020-07-22 21:17:08 -0500},
	doi = {10.1111/rssb.12313},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12313},
	file = {/Users/tliang/Zotero/storage/WMQWR7DU/Liang_Su_2019_Statistical inference for the population landscape via moment-adjusted.pdf},
	ids = {liang2019StatisticalInference},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Acceleration,Diffusion process,Discretized Langevin algorithm,Model misspecification,Non-asymptotic inference,Population landscape,Stochastic gradient methods},
	number = {2},
	pages = {431--456},
	title = {Statistical Inference for the Population Landscape via Moment-Adjusted Stochastic Gradients},
	volume = {81},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1111/rssb.12313},
	Bdsk-Url-2 = {https://doi.org/10.1111/rssb.12313}}

@article{liang2020JustInterpolate,
	author = {Liang, Tengyuan and Rakhlin, Alexander},
	date-added = {2020-07-22 21:12:30 -0500},
	date-modified = {2020-07-22 21:12:30 -0500},
	doi = {10.1214/19-AOS1849},
	file = {/Users/tliang/Zotero/storage/RIZSR7QD/Liang_Rakhlin_2020_Just interpolate.pdf},
	fjournal = {Annals of Statistics},
	ids = {liang2020JustInterpolate},
	journal = {The Annals of Statistics},
	month = jun,
	number = {3},
	pages = {1329--1347},
	publisher = {{The Institute of Mathematical Statistics}},
	sici = {0090-5364(2020)48:3\textexclamdown 1329:JIKRRC\textquestiondown 2.0.CO;2-V},
	title = {Just Interpolate: {{Kernel}} ``{{Ridgeless}}'' Regression Can Generalize},
	volume = {48},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1214/19-AOS1849}}

@inproceedings{liang2020MultipleDescent,
	abstract = {We study the risk of minimum-norm interpolants of data in Reproducing Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape for the various scalings of d = n\textsuperscript{{$\alpha$}}, {$\alpha\in$}(0,1), for the input dimension d and sample size n. Empirical evidence supports our finding that minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample size; furthermore, locations of the peaks in our experiments match our theoretical predictions. Since gradient flow on appropriately initialized wide neural networks converges to a minimum-norm interpolant with respect to a certain kernel, our analysis also yields novel estimation and generalization guarantees for these over-parametrized models. At the heart of our analysis is a study of spectral properties of the random kernel matrix restricted to a filtration of eigen-spaces of the population covariance operator, and may be of independent interest.},
	author = {Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	chapter = {Machine Learning},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	file = {/Users/tliang/Zotero/storage/XC7NE2Y6/Liang et al_2020_On the multiple descent of minimum-norm interpolants and restricted lower.pdf},
	ids = {liang2020MultipleDescentMinimumNorma},
	issn = {1938-7228},
	month = jul,
	pages = {2683--2711},
	pdf = {http://proceedings.mlr.press/v125/liang20a/liang20a.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry of Kernels},
	volume = {125},
	year = {2020}}

@article{liang2020PreciseHighdimensional,
	archiveprefix = {arXiv},
	author = {Liang, Tengyuan and Sur, Pragya},
	date-added = {2020-07-24 09:09:37 -0500},
	date-modified = {2020-07-24 09:09:37 -0500},
	eprint = {2002.01586},
	eprinttype = {arxiv},
	file = {/Users/tliang/Zotero/storage/WAWF6WEZ/Liang_Sur_2020_A precise high-dimensional asymptotic theory for boosting and min-l1-norm.pdf},
	ids = {liang2020PreciseHighDimensionalAsymptotic,liang2020PreciseHighDimensionalAsymptotica},
	journal = {arXiv preprint arXiv:2002.01586},
	keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
	month = feb,
	title = {A Precise High-Dimensional Asymptotic Theory for Boosting and Min-L1-Norm Interpolated Classifiers},
	year = {2020}}

@inproceedings{tzen2018LocalOptimality,
	abstract = {We study the detailed path-wise behavior of the discrete-time Langevin algorithm for non-convex Empirical Risk Minimization (ERM) through the lens of metastability, adopting some techniques from Berglund and Gentz (2003). For a particular local optimum of the empirical risk, with an \emph{arbitrary initialization}, we show that, with high probability, at least one of the following two events will occur: (1) the Langevin trajectory ends up somewhere outside the {$\varepsilon$}-neighborhood of this particular optimum within a short \emph{recurrence time}; (2) it enters this {$\varepsilon$}-neighborhood by the recurrence time and stays there until a potentially exponentially long \emph{escape time}. We call this phenomenon \emph{empirical metastability}. This two-timescale characterization aligns nicely with the existing literature in the following two senses. First, the effective recurrence time (i.e., number of iterations multiplied by stepsize) is dimension-independent, and resembles the convergence time of continuous-time deterministic Gradient Descent (GD). However unlike GD, the Langevin algorithm does not require strong conditions on local initialization, and has the possibility of eventually visiting all optima. Second, the scaling of the escape time is consistent with the Eyring-Kramers law, which states that the Langevin scheme will eventually visit all local minima, but it will take an exponentially long time to transit among them. We apply this path-wise concentration result in the context of statistical learning to examine local notions of generalization and optimality.},
	author = {Tzen, Belinda and Liang, Tengyuan and Raginsky, Maxim},
	booktitle = {Proceedings of the 31st Conference on Learning Theory},
	date-added = {2020-07-22 20:53:24 -0500},
	date-modified = {2020-07-22 20:53:24 -0500},
	editor = {Bubeck, S{\'e}bastien and Perchet, Vianney and Rigollet, Philippe},
	file = {/Users/tliang/Zotero/storage/P5UFPYZY/Tzen et al_2018_Local optimality and generalization guarantees for the langevin algorithm via.pdf},
	month = jul,
	pages = {857--875},
	pdf = {http://proceedings.mlr.press/v75/tzen18a/tzen18a.pdf},
	publisher = {{PMLR}},
	series = {Proceedings of Machine Learning Research},
	title = {Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability},
	volume = {75},
	year = {2018}}
